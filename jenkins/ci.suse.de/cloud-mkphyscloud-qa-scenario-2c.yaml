- job:
    name: cloud-mkphyscloud-qa-scenario-2c
    node: cloud-mkphyscloud-gate-qa
    description: |
      <b>This job is managed by JJB! Changes must be done in
      <a href='https://github.com/SUSE-Cloud/automation/tree/master/jenkins/ci.suse.de/'>git</a>
      </b>

      This job will redeploy scenario-2c:
        - 7 nodes
        - HA with SBD (2 nodes)
        - DRBD
        - ceph (3 nodes)
        - 2 kvm compute nodes
        - Neutron: linuxbridge or OVS

      It will wipe all QA2 machines!

    wrappers:
        - build-name:
            name: '#${BUILD_NUMBER} ${ENV,var="cloudsource"} - qa${ENV,var="hw_number"} ${ENV,var="networkingplugin"}:${ENV,var="networkingmode"}'
    publishers:
      - mkphyscloud-qa-common-publishers

    logrotate:
      numToKeep: 15
      daysToKeep: -1

    parameters:
      - string:
          name: hw_number
          description: Mandatory, name of the QA cloud server as integer

      - string:
          name: admin_os
          default: sles12sp3
          description: Mandatory, admin node operating system version

      - string:
          name: sbd_ip
          default: 10.162.64.10
          description: Mandatory, IP of iscsi target for sbd

      - string:
          name: repo_owner
          default: SUSE-Cloud
          description: Mandatory, automation repo owner/organization

      - string:
          name: branch
          default: master
          description: Mandatory, automation repo branch

      - string:
          name: automation_repo
          default: github.com/$repo_owner/automation#$branch
          description: Mandatory, automation repo URL

      - string:
          name: netapp_server
          default: netapp-n1-e0m.cloud.suse.de
          description: Mandatory, the name of the NetApp Storage backend server 

      - string:
          name: netapp_vserver
          default: cloud-manila-svm
          description: Mandatory, the name of the NetApp Storage backend vserver 

      - string:
          name: tempest
          default: smoke
          description: Optional, specify what tempest test(s) to run, e.g. smoke, smoke|full or smoke|defcore

      - bool:
          name: rally-tests
          default: false
          description: Optional, Run rally  tests

      - string:
          name: ssl_type
          default: no-ssl
          description: "Mandatory, set the SSL configuration for the cloud, available options: no-ssl, ssl-insecure, ssl"

      - string:
          name: cloud_version
          default: "8"
          description: Mandatory, version of the cloud to be installed as integer

      - string:
          name: scenario_dir
          default: ""
          description: |
            Custom scenario directory to look for scenario file in. If
            unspecified this will be determined automatically based on cloud
            version and SSL settings.

      - string:
          name: scenario_file
          default: qa-scenario-2c.yaml
          description: Scenario YAML file name

      # Parameters for qa_crowbarsetup.sh
      - string:
          name: cloudsource
          default: develcloud$cloud_version

      - string:
          name: TESTHEAD
          default: "1"
          description: if non-empty, test latest version from Devel:Cloud:Staging

      - string:
          name: want_ceph
          default: "1"
          description: Install ceph barclamp

      - string:
          name: hacloud
          default: "1"
          description: By default we do not want HA configured and installed

      - string:
          name: clusterconfig
          default: services=2
          description: HA configuration for clusters. Make sense only if hacloud=1
      
      - string:
          name: networkingplugin
          default: linuxbridge
          description: |
              networking plugin to be used by Neutron. Available options are: openvswitch:gre, vlan, vxlan / linuxbridge:vlan

      - string:
          name: networkingmode
          default: vlan
          description: networking mode to be used by Neutron. Available options are gre, vlan, vxlan

      - string:
          name: nodenumber
          default: "7"
          description: Number of nodes to use; is scenario specific

      - string:
          name: commands
          default: addupdaterepo prepareinstallcrowbar runupdate bootstrapcrowbar installcrowbar allocate waitcloud setup_aliases
          description: All the steps that needs to be completed to have cloud installed

      - string:
          name: want_test_updates
          default: 0
          description: Use updates-test repos for all SUSE products

      - text:
          name: UPDATEREPOS
          default:
          description: Update repositories (one URL per line)

      - bool:
          name: UPDATEBEFOREINSTALL
          default: false
          description: add update repositories before crowbar install

    builders:
      - shell: |
          #!/bin/bash -x
          admin=crowbar$hw_number
          cloud=qa$hw_number
          netapp_server=$netapp_server
          netapp_password=`cat /home/jenkins/passwords/netapp_password`

          if [ ! -z "$UPDATEREPOS" ] ; then
            export UPDATEREPOS=${UPDATEREPOS//$'\n'/+}
          fi

          export artifacts_dir=$WORKSPACE/.artifacts
          rm -rf $artifacts_dir
          mkdir -p $artifacts_dir
          touch $artifacts_dir/.ignore

          # check that netapp backend server is available
          ping -c 3 $netapp_server || ret=$?

          if [ $ret != 0 ] ; then 
            echo "netapp server is unavailable!"
            exit 1
          fi

          # destroy the old admin VM if any and spawn a clean new admin VM afterwards
          # /usr/local/sbin/freshadminvm
          freshadminvm $admin $admin_os
          sleep 100 # time for the admin VM to boot

          # clone, fetch and update the automation repo
          # /usr/local/sbin/update_automation
          update_automation

          # put variables needed during the build process into mkcloud.config file
          env | grep -e networking -e libvirt -e cloud > mkcloud.config

          # copy scripts/ dir onto the admin node in /root/scripts
          scp -r ~/github.com/$repo_owner/automation/scripts mkcloud.config root@$admin:

          # Use custom scenario dir if specified, otherwise default to the one
          # based on $cloud_version and $ssl_type.

          if [ -z "$scenario_dir" ]; then
            scenario_dir=scripts/scenarios/cloud$cloud_version/qa/$ssl_type/$scenario_file
          fi

          # copy scenario file onto the admin node in /root/scenario.yml

          scp ~/github.com/$repo_owner/automation/${scenario_dir}/${scenario_file} \
              root@$admin:scenario.yml

          ret=0

          ssh root@$admin "
          export cloud=$cloud ;
          export hw_number=$hw_number ;
          export sbd_ip=$sbd_ip ;
          export UPDATEREPOS=$UPDATEREPOS ;
          export UPDATEBEFOREINSTALL=$UPDATEBEFOREINSTALL ;
          export want_test_updates=$want_test_updates ;
          export TESTHEAD=$TESTHEAD ;
          export cloudsource=$cloudsource ;
          export nodenumber=$nodenumber ;
          export hacloud=$hacloud ;
          export clusterconfig=$clusterconfig ;
          export want_node_aliases=controller=2,compute=2,ceph=3 ;
          export want_node_roles=controller=2,compute=2,storage=3 ;
          export want_ceph=1 ;
          export networkingplugin=$networkingplugin ;
          export networkingmode=$networkingmode ;
          export cephvolumenumber=1 ;
          export netapp_password=$netapp_password ;
          export netapp_server=$netapp_server ;
          export netapp_vserver=$netapp_vserver ;
          export scenario=\"/root/scenario.yml\" ;
          export commands=\"$commands\" "'

          sed -i -e "s,##networkingplugin##,$networkingplugin," scenario.yml
          sed -i -e "s,##networkingmode##,$networkingmode," scenario.yml
          sed -i -e "s,##netapp_password##,$netapp_password," scenario.yml
          sed -i -e "s,##netapp_server##,$netapp_server," scenario.yml
          sed -i -e "s,##netapp_vserver##,$netapp_vserver," scenario.yml

          [ $UPDATEBEFOREINSTALL == "true" ] && export updatesteps="addupdaterepo runupdate"

          timeout --signal=ALRM 240m bash -x -c "source scripts/qa_crowbarsetup.sh ; onadmin_runlist $commands"
          ' || ret=$?

          echo "mkphyscloud ret=$ret (before scenario)"

          if [ "$ret" = "0" ]; then
            # ----- Prepare the SBD setup:
            cat > /tmp/sbd_prepare_$admin <<EOSCRIPT
              # preparaton of iSCSI
              export ZYPP_LOCK_TIMEOUT=120
              zypper --gpg-auto-import-keys -p http://download.opensuse.org/repositories/devel:/languages:/python/SLE_12_SP2/ --non-interactive install python-sh

              chmod +x scripts/iscsictl.py

              for node in {controller1,controller2} ; do
                ./scripts/iscsictl.py --service initiator --target_host $sbd_ip --host \$node --no-key --id id-qa$hw_number
              done

              # preparation of SBD
              SBD_DEV=\$(ssh controller1 echo '/dev/disk/by-id/scsi-\$(lsscsi -i | grep LIO | tr -s " " |cut -d " " -f7)')
              ssh controller1 "zypper --non-interactive install sbd; sbd -d \$SBD_DEV create -4 20 -1 10"
              ssh controller2 "zypper --non-interactive install sbd"
              # watchdog configuration
              ssh controller1 "modprobe softdog; echo softdog > /etc/modules-load.d/watchdog.conf"
              ssh controller2 "modprobe softdog; echo softdog > /etc/modules-load.d/watchdog.conf"
              # take scenario yaml file and replace /dev/sda with the right thing:
              sed -i "s|/dev/sda|\${SBD_DEV}|g" scenario.yml
              # ----- End of SBD
          EOSCRIPT

            chmod +x /tmp/sbd_prepare_$admin
            scp /tmp/sbd_prepare_$admin root@$admin:sbd_prepare

            # Remove unused initiators from the iscsi target
            iscsi_target_cleanup qa$hw_number

            # Rerun chef-client to avoid frequent failures here and update bind records, see bsc#1021900
            ssh root@$admin 'chef-client'

            # Check if zypper is used by other application
            ssh root@$admin '
              source scripts/qa_crowbarsetup.sh;
              for node in $(crowbar machines aliases | grep "controller" | grep -oE "^.*[[:space:]]"); do
                wait_for 20 10 "ssh $node \"zypper refresh\"" "zypper to be available on $node"
              done
            '
          fi

          ssh root@$admin "
            export cloud=$cloud ;
            export TESTHEAD=$TESTHEAD ;
            export cloudsource=$cloudsource ;
            export nodenumber=$nodenumber ;
            export hacloud=$hacloud ;
            export clusterconfig=$clusterconfig ;
            export want_ceph=1 ;
            export cephvolumenumber=1;
            export scenario=scenario.yml "'
            echo "deployceph=1" >> mkcloud.config

            source scripts/qa_crowbarsetup.sh

            ./sbd_prepare

            crowbar batch --timeout 2400 build scenario.yml' || ret=$?

          if [ $ret != 0 ] ; then
            ssh root@$admin '
            set -x
            for node in $(crowbar machines list | grep ^d) ; do
              (
              echo "Collecting supportconfig from $node"
              timeout 400 ssh $node supportconfig | wc
              timeout 300 scp $node:/var/log/\*txz /var/log/
              )&
            done
            timeout 500 supportconfig | wc &
            wait
            '
          fi >&2

          scp root@$admin:/var/log/*txz $artifacts_dir/
          exit $ret

      - trigger-builds:
          - project: cloud-mkphyscloud-qa-tests-trigger
            condition: SUCCESS
            block: true
            predefined-parameters: |
              hw_number=$hw_number
              tempest=$tempest
              scenario_name=2c
              scenario_job_name=$JOB_NAME
              scenario_build_number=$BUILD_NUMBER
