#!/bin/bash
#
# mkcloud - Setup a virtual cloud on one system (physical or even virtual)
#
# Authors: J. Daniel Schmidt <jdsn@suse.de>
#          Bernhard M. Wiedemann <bwiedemann@suse.de>
#
# 2012, SUSE LINUX Products GmbH
#

# Quick introduction:
#
# This tool relies on the script qa_crowbarsetup.sh (in the same directory)
# Please 'export' environment variables according to your needs:
#
# Either 'cloudvg' or 'cloudpv' must be set.
# Another required parameter is 'cloudsource'.
# All other parameters have defaults unless defined otherwise.
#
# cloudvg=vg0
#       volume group name to use to create lvm volumes
#
# cloudpv=/dev/vdx  (default=/dev/vdb)
#       device where a LVM physical volume can be created
#       should be at least 80 GB
#       the volume group will be called "cloud"
#
# cloudsource=develcloud4|develcloud5|susecloud4|Beta?   (required, no default)
#       defines the source for the installation of the cloud product
#       develcloud2.0 : product from IBS Devel:Cloud:2.0
#       develcloud3   : product from IBS Devel:Cloud:3
#       develcloud4   : product from IBS Devel:Cloud:4
#       develcloud5   : product from IBS Devel:Cloud:5
#       susecloud4     : product from IBS SUSE:SLE....
#       Beta?         : uses official Beta? ISO image (? is a number)
#
# TESTHEAD=''|1   (default='')
#                will use latest published packages from Devel:Cloud
#                even if there is no new ISO created yet
#
# cephvolumenumber (default 0)
#                sets the number of 5GB ceph volumes that will be created per node
#                for ceph testing
#
# nodenumber     (default 2)
#                sets the number of nodes to be created
#
# vcpus          (default 1)
#                sets the number of CPU cores assigned to each compute node
# adminvcpus     (default $vcpus)
#                sets the number of CPU cores assigned to admin node


if [ -n "$CVOL" ] ; then
    cloudpv=$CVOL
    unset CVOL
    echo "------------------------------------------------------------------------"
    echo "The CVOL variable is deprecated."
    echo "Please either set 'cloudvg' or 'cloudpv'"
    echo " cloudvg will use an existing lvm and not destroy other volumes"
    echo " cloudpv will use an lvm pv device exclusively and destroy all volues"
    echo
    echo "Continuing in 20 seconds with fallback (cloudpv=\$CVOL):"
    echo " cloudpv=$CVOL"
    echo "Otherwise press Ctrl-C now!"
    echo "------------------------------------------------------------------------"
    sleep 20
fi
virtualcloud=${virtualcloud:-virtual}
cloudfqdn=${cloudfqdn:-$virtualcloud.cloud.suse.de}
forwardmode=${forwardmode:-nat}
net_fixed=${net_fixed:-192.168.123}
net_public=${net_public:-192.168.122}
net_admin=${net_admin:-192.168.124}
adminnetmask=${adminnetmask:-255.255.248.0}
adminip=$net_admin.10
admingw=$net_admin.1
nodenumber=${nodenumber:-2}
cephvolumenumber=${cephvolumenumber:-0}
allnodeids=`seq 1 $nodenumber`
vcpus=${vcpus:-1}
adminvcpus=${adminvcpus:-$vcpus}
cpuflags=''
working_dir_orig=`pwd`
artifacts_dir=${artifacts_dir:-$working_dir_orig/.artifacts}
start_time=`date`
cloud=${cloud:-cloud}
cloudvg=${cloudvg:-$cloud}
needcvol=1
admin_node_memory=${admin_node_memory:-2097152}
controller_node_memory=${compute_node_memory:-4194304}
compute_node_memory=${compute_node_memory:-2097152}
# pvlist is filled below
pvlist=
next_pv_device=
pv_cur_device_no=0
cloudbr=${cloudbr:-${cloud}br}
mnt=/tmp/cloudmnt/$$
sshopts="-oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null"
scp="scp $sshopts"
debug=${debug:-0}
#if localreposdir_src string is available, the local repositories are used for setup
localreposdir_src=${localreposdir_src:-""}
#localreposdir_target is the 9p target dir and also the mount target dir in the VM
localreposdir_target=${localreposdir_target:-"/repositories"}
[ -z "$localreposdir_src" ] && localreposdir_target=""

pidfile=mkcloud.pid

trap 'error_exit $? "error caught by trap"' TERM
exec </dev/null

function complain() # {{{
{
    local ex=$1; shift
    printf "Error: %s\n" "$@" >&2
    [[ $ex != - ]] && exit $ex
} # }}}

function show_environment()
{
    end_time=`date`
    echo "Environment Details"
    echo "-------------------------------"
    echo "    hostname: `hostname -f`"
    echo "     started: $start_time"
    echo "       ended: $end_time"
    echo "-------------------------------"
    echo " cloudsource: $cloudsource"
    echo "    TESTHEAD: $TESTHEAD"
    echo "  nodenumber: $nodenumber"
    echo "     cloudpv: $cloudpv"
    echo " UPDATEREPOS: $UPDATEREPOS"
    echo " cephvolumenumber: $cephvolumenumber"
    echo "-------------------------------"
    env | grep -i "_with_ssl"
    echo "-------------------------------"
}

function pre_exit_cleanup()
{
    rm $pidfile
}

function error_exit()
{
    exitcode=$1
    message=$2
    ssh $sshopts root@$adminip '
        set -x
        for node in $(crowbar machines list | grep ^d) ; do
            echo "Collecting supportconfig from $node"
            timeout 300 ssh $node supportconfig -x PROC | wc
            timeout 300 scp $node:/var/log/\*tbz /var/log/
        done
        timeout 300 supportconfig | wc
    '
    mkdir -p $artifacts_dir
    $scp root@$adminip:/var/log/*tbz $artifacts_dir/
    pre_exit_cleanup
    echo $message
    show_environment
    exit $exitcode
} >&2

function wait_for()
{
    timecount=${1:-300}
    timesleep=${2:-1}
    condition=${3:-'/bin/true'}
    waitfor=${4:-'unknown process'}

    echo "Waiting for: $waitfor"
    n=$timecount
    while test $n -gt 0 && ! eval $condition
    do
        echo -n .
        sleep $timesleep
        n=$(($n - 1))
    done
    echo

    if [ $n = 0 ] ; then
        complain 11 "Waiting for '$waitfor' timed out." \
            "This check was used: $condition"
    fi
}

function sshrun()
{
    mkcconf=mkcloud.config
    cat > $mkcconf <<EOF
        export debug=$debug ;
        export cloudfqdn=$cloudfqdn ;
        export cloudsource=$cloudsource ;
        export hacloud=$hacloud ;
        export libvirt_type=$libvirt_type ;
        export net_admin=$net_admin ;
        export net_public=$net_public ;
        export net_fixed=$net_fixed ;
        export networkingplugin=$networkingplugin ;
        export networkingmode=$networkingmode ;
        export nodenumber=$nodenumber ;
        export nosetestparameters=${nosetestparameters} ;
        export tempestoptions='${tempestoptions}' ;
        export cephvolumenumber=$cephvolumenumber ;
        export shell=$shell ;
        export want_sles12=$want_sles12 ;

        export all_with_ssl=$all_with_ssl ;
        export glance_with_ssl=$glance_with_ssl ;
        export keystone_with_ssl=$keystone_with_ssl ;
        export nova_with_ssl=$nova_with_ssl ;
        export novadashboard_with_ssl=$novadashboard_with_ssl ;

        export localreposdir_target=$localreposdir_target ;

        export cinder_conf_volume_type=$cinder_conf_volume_type;
        export cinder_conf_volume_params=$cinder_conf_volume_params;

        export TESTHEAD=$TESTHEAD ;
        export NOINSTALLCLOUDPATTERN=$NOINSTALLCLOUDPATTERN ;
EOF

    $scp qa_crowbarsetup.sh $mkcconf root@$adminip:
    ssh $sshopts root@$adminip "$@"
    return $?
}


function cleanup()
{
    # cleanup leftover from last run
    allnodenames=$(seq --format="node%.0f" 1 $(($nodenumber + 20)))
    for n in admin $allnodenames ; do
        virsh destroy $cloud-$n
        virsh undefine $cloud-$n
    done
    virsh net-destroy $cloud-admin
    virsh net-undefine $cloud-admin
    ip link set ${cloudbr}.300 down
    ip link set ${cloudbr} down
    ip link delete ${cloudbr} type bridge
    ip link delete ${cloudbr}-nic

    # remove all previous volumes for that cloud; this helps preventing
    # accidental booting and freeing space
    find -L /dev/$cloudvg/$cloud.* -type b | xargs lvremove --force

    for i in /dev/loop* ; do
        if losetup $i | grep -q $cloud-admin ; then
            losetup -d $i
        fi
    done
    rm -f /var/run/libvirt/qemu/$cloud-*.xml /var/lib/libvirt/network/$cloud-*.xml \
        /etc/sysconfig/network/ifcfg-$cloudbr.300
    if [ -n "$wipe" ] ; then
        vgchange -an $cloudvg
        dd if=/dev/zero of=$cloudpv count=1000
    fi
    return 0
}

function h_get_next_pv_device()
{
    if [ -z "$pvlist" ] ; then
        pvlist=`pvs --sort -Free | awk '$2~/'$cloudvg'/{print $1}'`
        pv_cur_device_no=0
    fi
    next_pv_device=`perl -e '$i=shift; $i=$i % @ARGV;  print $ARGV[$i]' $pv_cur_device_no $pvlist`
    pv_cur_device_no=$(( $pv_cur_device_no + 1 ))
}

function h_create_cloud_lvm()
{
    if [ -n "$needcvol" ] ; then
        pvcreate "$cloudpv"
        vgcreate "$cloudvg" "$cloudpv"
    fi
    vgchange -ay $cloudvg # for later boots

    # hdd size defaults (unless defined otherwise)
    adminnode_hdd_size=${adminnode_hdd_size:-15}
    controller_hdd_size=${controller_hdd_size:-20}
    computenode_hdd_size=${computenode_hdd_size:-15}
    cephvolume_hdd_size=${cephvolume_hdd_size:-11}
    controller_ceph_hdd_size=${controller_ceph_hdd_size:-25}
    local hdd_size

    lvrename /dev/$cloudvg/admin /dev/$cloudvg/$cloud.admin # transition until 2015
    h_get_next_pv_device
    lvcreate -n $cloud.admin -L ${adminnode_hdd_size}G $cloudvg $next_pv_device
    for i in $allnodeids ; do
        lvrename /dev/$cloudvg/node$i /dev/$cloudvg/$cloud.node$i # transition until 2015
        h_get_next_pv_device
        hdd_size=${computenode_hdd_size}
        test "$i" = "1" && hdd_size=${controller_hdd_size}
        lvcreate -n $cloud.node$i -L ${hdd_size}G $cloudvg $next_pv_device
    done

    if [ $cephvolumenumber -gt 0 ] ; then
        for i in $allnodeids ; do
            for n in $(seq 1 $cephvolumenumber) ; do
                lvrename /dev/$cloudvg/node$i-ceph$n /dev/$cloudvg/$cloud.node$i-ceph$n # transition until 2015
                h_get_next_pv_device
                hdd_size=${cephvolume_hdd_size}
                test "$i" = "1" -a "$n" = "1" && hdd_size=${controller_ceph_hdd_size}
                lvcreate -n $cloud.node$i-ceph$n -L ${hdd_size}G $cloudvg $next_pv_device
            done
        done
    fi
}

function h_deploy_admin_image()
{
    pushd /tmp
    wget --progress=dot:mega -nc http://clouddata.cloud.suse.de/images/SP3-64up.qcow2
    qemu-img convert -p SP3-64up.qcow2 /dev/$cloudvg/$cloud.admin
    popd

    if fdisk -l /dev/$cloudvg/$cloud.admin | grep -q 2056192 ; then
        # make a bigger partition 2
        echo -e "d\n2\nn\np\n2\n\n\na\n2\nw" | fdisk /dev/$cloudvg/$cloud.admin
        loopdevice=`losetup --find`
        losetup -o $(expr 2056192 \* 512) $loopdevice /dev/$cloudvg/$cloud.admin
        fsck -y -f $loopdevice
        resize2fs $loopdevice
        sync
        losetup -d $loopdevice
    fi
}

function h_add_etchosts_entries()
{
    grep -q crowbar /etc/hosts || echo "$adminip crowbar.$cloudfqdn crowbar" >> /etc/hosts
}

function h_enable_ksm
{
    # enable kernel-samepage-merging to save RAM
    echo 1 > /sys/kernel/mm/ksm/run
    echo 1000 > /sys/kernel/mm/ksm/pages_to_scan
}

function prepare()
{
    zypper --non-interactive in --no-recommends libvirt kvm lvm2 curl wget bridge-utils dnsmasq netcat-openbsd ebtables

    echo "Creating key for controlling our VMs..."
    [ -e ~/.ssh/id_dsa ] || ssh-keygen -t dsa -f ~/.ssh/id_dsa -N ""

    h_enable_ksm
    h_create_cloud_lvm
    h_deploy_admin_image
    h_add_etchosts_entries
}

function ssh_password()
{
    SSH_ASKPASS=/root/echolinux
    cat > $SSH_ASKPASS <<EOSSHASK
#!/bin/sh
echo linux
EOSSHASK
    chmod +x $SSH_ASKPASS
    DISPLAY=dummydisplay:0 SSH_ASKPASS=$SSH_ASKPASS setsid ssh $sshopts -oNumberOfPasswordPrompts=1 "$@"
}

function h_local_repository_mount()
{
    #add xml snippet to be able to mount a local dir via 9p in a VM
    if [ -n "${localreposdir_src}" ]; then
        local_repository_mount="<filesystem type='mount' accessmode='squash'>
            <source dir='$localreposdir_src'/>
            <target dir='$localreposdir_target'/>
            <readonly/>
            <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
            </filesystem>"
    else
        local_repository_mount=""
    fi
}

function h_cpuflags_settings()
{ # used for admin and compute nodes
    cpuflags="<cpu match='minimum'>
            <model>qemu64</model>
            <feature policy='require' name='fxsr_opt'/>
            <feature policy='require' name='mmxext'/>
            <feature policy='require' name='lahf_lm'/>
            <feature policy='require' name='sse4a'/>
            <feature policy='require' name='abm'/>
            <feature policy='require' name='cr8legacy'/>
            <feature policy='require' name='misalignsse'/>
            <feature policy='require' name='popcnt'/>
            <feature policy='require' name='pdpe1gb'/>
            <feature policy='require' name='cx16'/>
            <feature policy='require' name='3dnowprefetch'/>
            <feature policy='require' name='cmp_legacy'/>
            <feature policy='require' name='monitor'/>
        </cpu>"
    grep -q "flags.* npt" /proc/cpuinfo || cpuflags=""

    if grep -q "vendor_id.*GenuineIntel" /proc/cpuinfo; then
        cpuflags="<cpu mode='custom' match='exact'>
            <model fallback='allow'>core2duo</model>
            <feature policy='require' name='vmx'/>
        </cpu>"
    fi
}


function h_create_libvirt_adminnode_config()
{
    h_cpuflags_settings
    h_local_repository_mount

    cat > $1 <<EOLIBVIRT
  <domain type='kvm'>
    <name>$cloud-admin</name>
    <memory>$admin_node_memory</memory>
    <currentMemory>$admin_node_memory</currentMemory>
    <vcpu>$adminvcpus</vcpu>
    <os>
      <type arch='x86_64' machine='pc-0.14'>hvm</type>
      <boot dev='hd'/>
    </os>
    <features>
      <acpi/>
      <apic/>
      <pae/>
    </features>
    $cpuflags
    <clock offset='utc'/>
    <on_poweroff>preserve</on_poweroff>
    <on_reboot>restart</on_reboot>
    <on_crash>restart</on_crash>
    <devices>
      <emulator>/usr/bin/qemu-kvm</emulator>
      <disk type='block' device='disk'>
        <driver name='qemu' type='raw' cache='unsafe'/>
        <source dev='/dev/$cloudvg/$cloud.admin'/>
        <target dev='vda' bus='virtio'/>
        <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
      </disk>
      <interface type='network'>
        <mac address='52:54:00:77:77:70'/>
        <source network='$cloud-admin'/>
        <model type='virtio'/>
        <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
      </interface>
      <serial type='pty'>
        <target port='0'/>
      </serial>
      <console type='pty'>
        <target type='serial' port='0'/>
      </console>
      <input type='mouse' bus='ps2'/>
      <graphics type='vnc' port='-1' autoport='yes'/>
      <video>
        <model type='cirrus' vram='9216' heads='1'/>
        <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
      </video>
      <memballoon model='virtio'>
        <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
      </memballoon>
      $local_repository_mount
    </devices>
  </domain>
EOLIBVIRT
}

function h_create_libvirt_admin_network_config()
{
    # dont specify range
    # this allows to use the same network for cloud-nodes that get DHCP from crowbar
    # doc: http://libvirt.org/formatnetwork.html
    cat > $1 <<EOLIBVIRTNET
  <network>
    <name>$cloud-admin</name>
    <bridge name='$cloudbr' stp='off' delay='0' />
    <mac address='52:54:00:AB:B1:77'/>
    <ip address='$admingw' netmask='$adminnetmask'>
      <dhcp>
        <host mac="52:54:00:77:77:70" name="crowbar.$cloudfqdn" ip="$adminip"/>
      </dhcp>
    </ip>
    <forward mode='$forwardmode'>
    </forward>
  </network>
EOLIBVIRTNET
}


function setupadmin()
{
    echo "Injecting public key into image..."
    keyfile=~/.ssh/id_dsa.pub
    pubkey=`cut -d" " -f2 $keyfile`
    mkdir -p $mnt
    mount -o loop,offset=$(expr 2056192 \* 512) /dev/$cloudvg/$cloud.admin $mnt
    mkdir -p $mnt/root/.ssh
    grep -q "$pubkey" $mnt/root/.ssh/authorized_keys 2>/dev/null || cat $keyfile >> $mnt/root/.ssh/authorized_keys
    umount $mnt
    sync

    h_create_libvirt_adminnode_config /tmp/$cloud-admin.xml

    h_create_libvirt_admin_network_config /tmp/$cloud-admin.net.xml

    modprobe kvm-amd
    if [ ! -e /etc/modprobe.d/80-kvm-intel.conf ] ; then
        echo "options kvm-intel nested=1" > /etc/modprobe.d/80-kvm-intel.conf
        rmmod kvm-intel
    fi
    modprobe kvm-intel
    # needed for HA/STONITH via libvirtd:
    if ! grep -q ^listen_tcp /etc/libvirt/libvirtd.conf ; then
        service libvirtd stop
        echo 'auth_tcp = "none"
listen_tcp = 1
listen_addr = "0.0.0.0"' >> /etc/libvirt/libvirtd.conf
    fi
    chkconfig --add libvirtd
    service libvirtd start
    wait_for 300 1 '[ -S /var/run/libvirt/libvirt-sock ]' 'libvirt startup'

    if ! virsh net-dumpxml $cloud-admin > /dev/null 2>&1; then
        virsh net-define /tmp/$cloud-admin.net.xml
    fi
    virsh net-start $cloud-admin
    if ! virsh define /tmp/$cloud-admin.xml ; then
        echo "=====================================================>>"
        complain 76 "Could not define VM for: $cloud-admin"
    fi
    if ! virsh start $cloud-admin ; then
        echo "=====================================================>>"
        complain 76 "Could not start VM for: $cloud-admin"
    fi

    wait_for 300 1 "ping -q -c 1 -w 1 $adminip >/dev/null" 'crowbar admin VM'

    if ! grep -q "iptables -t nat -F PREROUTING" /etc/init.d/boot.local; then
        nodehostips=$(seq 81 $((80 + $nodenumber)))
        cat >> /etc/init.d/boot.local <<EOS
iptables -t nat -F PREROUTING
for i in 22 80 443 3000 4000 4040 ; do
    iptables -I FORWARD -p tcp --dport \$i -j ACCEPT
    for host in 10 $nodehostips ; do
        offset=80
        [ "\$host" = 10 ] && offset=10
        iptables -t nat -I PREROUTING -p tcp --dport \$((\$i + \$host - \$offset + 1100)) -j DNAT --to-destination $net_admin.\$host:\$i
    done
done
iptables -t nat -I PREROUTING -p tcp --dport 6080 -j DNAT --to-destination $net_public.2
echo 0 > /proc/sys/net/ipv4/conf/all/rp_filter
EOS
    fi
    /etc/init.d/boot.local

    wait_for 150 1 "nc -z $adminip 22" 'starting ssh daemon'

    echo "waiting some more for sshd+named to start"
    sleep 25
    ssh_password $adminip "mkdir ~/.ssh ; grep -q $pubkey ~/.ssh/authorized_keys 2>/dev/null || echo ssh-dss $pubkey injected-key-from-host >> ~/.ssh/authorized_keys"
    echo "you can now proceed with installing crowbar"
}

function prepareinstcrowbar()
{
    echo "connecting to crowbar admin server at $adminip"
    # qa_crowbarsetup uses static network which needs static resolv.conf
    # so we use the host's external IPv4 resolvers,
    # which must allow queries from routed IPs
    grep '^nameserver\s*[0-9]*\.' /etc/resolv.conf | sshrun dd of=/etc/resolv.conf
    sshrun "echo `hostname` > cloud ; prepareinstallcrowbar=1 bash -x qa_crowbarsetup.sh $virtualcloud"
    return $?
}

function instcrowbar()
{
    # copy local install-chef-suse.sh, if it exist it will override the original one:
    $scp -p install-chef-suse.sh root@$adminip:/tmp/
    sshrun "echo `hostname` > cloud ; installcrowbar=1 bash -x qa_crowbarsetup.sh $virtualcloud"
    local ret=$?
    $scp root@$adminip:screenlog.0 "$artifacts_dir/screenlog.0.install-suse-cloud"
    return $ret
}

function instcrowbarfromgit()
{
    # copy local install-chef-suse.sh, if it exist it will override the original one:
    $scp -p install-chef-suse.sh root@$adminip:/tmp/
    sshrun "echo `hostname` > cloud ; installcrowbarfromgit=1 bash -x qa_crowbarsetup.sh $virtualcloud"
    local ret=$?
    $scp root@$adminip:screenlog.0 "$artifacts_dir/screenlog.0.install-suse-cloud"
    return $ret
}

function mkvlan()
{
    local DEFVLAN=$1 ; shift
    local IP=$1 ; shift
    vconfig add $cloudbr $DEFVLAN
    ifconfig $cloudbr.$DEFVLAN $IP/24
    ethtool -K $cloudbr tx off
}


function h_create_libvirt_computenode_config()
{
    h_cpuflags_settings
    nodeconfigfile=$1
    nodecounter=$2
    macaddress=$3
    cephvolume="$4"
    nicmodel=virtio
    [ "$libvirt_type" = "xen" ] && nicmodel=e1000
    nodememory=$compute_node_memory
    [ "$nodecounter" = "1" ] && nodememory=$controller_node_memory

    cat > $nodeconfigfile <<EOLIBVIRT
<domain type='kvm'>
  <name>$cloud-node$nodecounter</name>
  <memory>$nodememory</memory>
  <currentMemory>$nodememory</currentMemory>
  <vcpu>$vcpus</vcpu>
  <os>
    <type arch='x86_64' machine='pc-0.14'>hvm</type>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  $cpuflags
  <clock offset='utc'/>
  <on_poweroff>preserve</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/bin/qemu-kvm</emulator>
    <disk type='block' device='disk'>
      <driver name='qemu' type='raw' cache='unsafe'/>
      <source dev='/dev/$cloudvg/$cloud.node$nodecounter'/>
      <target dev='vda' bus='virtio'/>
      <boot order='2'/>
    </disk>
    $cephvolume
    <interface type='network'>
      <mac address='$macaddress'/>
      <source network='$cloud-admin'/>
      <model type='$nicmodel'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
      <boot order='1'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target type='serial' port='0'/>
    </console>
    <input type='mouse' bus='ps2'/>
    <graphics type='vnc' port='-1' autoport='yes'/>
    <video>
      <model type='cirrus' vram='9216' heads='1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
  </devices>
</domain>
EOLIBVIRT

    if [ "$libvirt_type" = "xen" ] ; then
        sed -i -e "s/<target dev='vd\([^']*\)' bus='virtio'/<target dev='sd\1' bus='ide'/" $nodeconfigfile
    fi
}

function setuppublicnet()
{
    # workaround https://bugzilla.novell.com/show_bug.cgi?id=845496
    echo 0 > /proc/sys/net/bridge/bridge-nf-call-iptables
    # public = 300
    mkvlan 300 $net_public.1
}

function shutdowncloud()
{
    virsh shutdown $cloud-admin
    for i in $allnodeids ; do
        virsh shutdown $cloud-node$i
    done
}

function restartcloud()
{
    virsh net-start $cloud-admin
    virsh start $cloud-admin
    setuppublicnet
    for i in $allnodeids ; do
        virsh start $cloud-node$i
    done
}

function setupcompute()
{
    setuppublicnet
    alldevices=$(echo {b..z} {a..z}{a..z})
    for i in $allnodeids ; do
        c=1
        i2=$( printf %02x $i )
        macaddress="52:54:$i2:77:77:$i2"

        cephvolume=""
        if [ $cephvolumenumber -gt 0 ] ; then
            for n in $(seq 1 $cephvolumenumber) ; do
                dev=vd$(echo $alldevices | cut -d" " -f$c)
                c=$(($c + 1))
                cephvolume="$cephvolume
    <disk type='block' device='disk'>
        <serial>$cloud-node$i-ceph$n</serial>
        <driver name='qemu' type='raw' cache='unsafe'/>
        <source dev='/dev/$cloudvg/$cloud.node$i-ceph$n'/>
        <target dev='$dev' bus='virtio'/>
    </disk>"
            done
        fi

        h_create_libvirt_computenode_config /tmp/$cloud-node$i.xml $i $macaddress "$cephvolume"

        virsh destroy $cloud-node$i 2>/dev/null
        virsh undefine $cloud-node$i 2>/dev/null
        if ! virsh define /tmp/$cloud-node$i.xml ; then
            echo "====>>"
            complain 74 "Could not define VM for: node$i"
        fi
        if ! virsh start $cloud-node$i ; then
            echo "====>>"
            complain 74 "Could not start VM for: node$i"
        fi
    done
    return 0
}

function instcompute()
{
    sshrun "allocate=1 bash -x qa_crowbarsetup.sh $virtualcloud"
    ret=$?
    [ $ret != 0 ] && return $ret

    echo "Waiting for the installation of the nodes ..."
    sshrun "waitcompute=1 bash -x qa_crowbarsetup.sh $virtualcloud"
    return $?
}


function proposal()
{
    sshrun "proposal=1 bash -x qa_crowbarsetup.sh $virtualcloud"
    return $?
}

function testsetup()
{
    sshrun "testsetup=1 bash -x qa_crowbarsetup.sh $virtualcloud"
    local ret=$?
    $scp root@$adminip:tempest.log "$artifacts_dir/tempest.log"
    return $ret
}

function rebootcrowbar()
{
    # reboot the crowbar instance
    #  and test if everything is up and running afterwards
    sshrun "reboot"
    wait_for 500 3 "! nc -z $adminip 22" 'crowbar to go down'
    wait_for 500 3   "nc -z $adminip 22" 'crowbar to be back online'
    echo "waiting another 180 seconds for services"
    sleep 180
    sshrun "mount -a -t nfs" # FIXME workaround repos not mounted on reboot
    return $?
}

function rebootcompute()
{
    # reboot compute nodes
    #  and test if everthing is up and running afterwards
    sshrun "rebootcompute=1 bash -x qa_crowbarsetup.sh $virtualcloud"
    return $?
}

function rebootneutron()
{
    sshrun "rebootneutron=1 bash qa_crowbarsetup.sh $virtualcloud"
    return $?
}

function securitytests()
{
    # install and run security test suite owasp
    sshrun "securitytests=1 bash -x qa_crowbarsetup.sh $virtualcloud"
    return $?
}

function qa_test()
{
    local ghsc=github.com/SUSE-Cloud
    mkdir -p ~/$ghsc/
    pushd ~/$ghsc/
    if [ -e "qa-openstack-cli" ] ; then
        cd qa-openstack-cli/
        git pull
    else
        git clone https://$ghsc/qa-openstack-cli.git
    fi
    popd
    rsync -av ~/$ghsc/qa-openstack-cli root@$adminip:
    sshrun "qa_test=1 bash -x qa_crowbarsetup.sh $virtualcloud"
    ret=$?

    mkdir -p .artifacts
    $scp -r root@$adminip:qa_test.logs/ .artifacts/
    return $ret
}

function prepare_crowbar_backup_restore()
{
    curl -o crowbar-backup https://raw.githubusercontent.com/SUSE-Cloud/cloud-tools/master/backup/crowbar-backup
    $scp crowbar-backup root@$adminip:
}

function crowbarbackup()
{
    prepare_crowbar_backup_restore
    sshrun "AGREEUNSUPPORTED=1 bash -x crowbar-backup backup /tmp/backup-crowbar.tar.gz"
    ret=$?
    $scp root@$adminip:/tmp/backup-crowbar.tar.gz .
    [ -d "$artifacts_dir" ] && mv backup-crowbar.tar.gz "$artifacts_dir/"
    return $ret
}

function crowbarrestore()
{
    prepare_crowbar_backup_restore
    btarball=backup-crowbar.tar.gz
    [ -e "$artifacts_dir/$btarball" ] && btarball="$artifacts_dir/$btarball"
    if [ ! -e "$btarball" ] ; then
        echo "No crowbar backup tarball found."
        return 56
    fi
    $scp "$btarball" root@$adminip:/tmp/
    sshrun "AGREEUNSUPPORTED=1 bash -x crowbar-backup purge"
    sshrun "AGREEUNSUPPORTED=1 bash -x crowbar-backup restore /tmp/backup-crowbar.tar.gz"
    ret=$?
    return $ret
}

function prepare_cloudupgrade()
{
    sshrun "prepare_cloudupgrade=1 bash -x qa_crowbarsetup.sh $virtualcloud"
    return $?
}

function cloudupgrade_1st()
{
    sshrun "cloudupgrade_1st=1 bash -x qa_crowbarsetup.sh $virtualcloud"
    return $?
}

function cloudupgrade_2nd()
{
    sshrun "cloudupgrade_2nd=1 bash -x qa_crowbarsetup.sh $virtualcloud"
    return $?
}

function cloudupgrade_clients()
{
    sshrun "cloudupgrade_clients=1 bash -x qa_crowbarsetup.sh $virtualcloud"
    return $?
}

function cloudupgrade_reboot_and_redeploy_clients()
{
    sshrun "cloudupgrade_reboot_and_redeploy_clients=1 bash -x qa_crowbarsetup.sh $virtualcloud"
    return $?
}

function usage()
{
    echo "Usage:"
    echo "$0 <command> [<command>,...]"
    echo
    echo "  'command' is one of:"
    echo "   all instonly plain cleanup prepare setupadmin prepareinstcrowbar instcrowbar setupcompute"
    echo "   instcompute proposal addupdaterepo runupdate testsetup rebootcrowbar "
    echo "   rebootcompute help"
    echo
    echo "   all      -> expands to: cleanup prepare setupadmin addupdaterepo runupdate prepareinstcrowbar instcrowbar rebootcrowbar setupcompute instcompute proposal testsetup rebootcompute"
    echo "   all_noreboot -> exp to: cleanup prepare setupadmin addupdaterepo runupdate prepareinstcrowbar instcrowbar setupcompute instcompute proposal testsetup"
    echo "   plain    -> expands to: cleanup prepare setupadmin prepareinstcrowbar instcrowbar setupcompute instcompute proposal"
    echo "   plain_with_upgrade  -> expands to: cleanup prepare setupadmin prepareinstcrowbar instcrowbar setupcompute instcompute proposal prepare_cloudupgrade cloudupgrade_1st cloudupgrade_2nd cloudupcrade_clients cloudupgrade_reboot_and_redeploy_clients"
    echo "   instonly -> expands to: cleanup prepare setupadmin prepareinstcrowbar instcrowbar setupcompute instcompute"
    echo
    echo "   cleanup:       kill all running VMs, zero out boot sectors of all LVM volumes"
    echo "   prepare:       create LVM volumes, setup libvirt networks"
    echo "   setupadmin:    create the admin node and install the cloud product"
    echo "   prepareinstcrowbar: add repos and install crowbar packages"
    echo "   instcrowbar:   install crowbar and chef on the admin node"
    echo "   setupcompute:  create the compute nodes and let crowbar discover them"
    echo "   instcompute:   allocate and install compute nodes"
    echo "   proposal:      create and apply proposals for default setup"
    echo "   testsetup:     start a VM in the cloud"
    echo "   addupdaterepo: addupdate repos defined in UPDATEREPOS= (URLs separated by '+')"
    echo "   runupdate:     run zypper up on the crowbar node"
    echo "                  (compute nodes are automaticallyupdated via chef run)"
    echo "   rebootcrowbar: reboot the crowbar instance and wait for it being up"
    echo "   rebootcompute: reboot the compute nodes and wait for them being up"
    echo "   restartcloud:  start a pre-existing cloud again after host reboot"
    echo "   securitytests: install and run security test suite"
    echo "   qa_test:       run the qa test suite"
    echo "   help:          this usage"
    echo
    echo " Environment variables (need to be exported):"
    echo
    echo " Mandatory"
    echo "   cloudpv=/dev/vdx (default /dev/vdb)"
    echo "       :  LVM will be created on this device (at least 80GB)"
    echo "   cloudsource=develcloud2.0 | develcloud3 | develcloud4 | develcloud5 | susecloud4 | Beta?  (default '')"
    echo "       : defines the installation source"
    echo
    echo " Optional"
    echo "   hacloud='' | 1  (default='')"
    echo "       : setup a high availability cloud, requires at least 6 nodes"
    echo "   TESTHEAD='' | 1  (default='')"
    echo "       : use Media from Devel:Cloud:Staging and add test update repositories (except for GM* targets)"
    echo "   cephvolumenumber  (default=0)"
    echo "       : the number of 11GB ceph volumes that will be created per node"
    echo "         note: proposal step does NOT contain a ceph proposal, do it manually"
    echo "   nodenumber=2    (default 2)"
    echo "       : set the number of nodes to be created (excl. admin node)"
    echo "   vcpus=1         (default 1)"
    echo "       : set the number of CPU cores per compute node"
    echo "   adminvcpus=1    (default $vcpus)"
    echo "       : set the number of CPU cores for admin node"
    echo "   admin_node_memory (default 2097152)"
    echo "       : Set the memory in KB assigned to admin node"
    echo "   controller_node_memory (default 4194304)"
    echo "       : Set the memory in KB assigned to compute nodes"
    echo "   compute_node_memory (default 2097152)"
    echo "       : Set the memory in KB assigned to compute nodes"
    echo "   networkingplugin"
    echo "       : Set the networking plugin to be used by neutron (e.g. openvswitch),"
    echo "         if it isn't defined the barclamp-neutron's default is used."
    echo "   networkingmode"
    echo "       : Set the networking mode to be used by neutron (e.g. gre)"
    echo "         if it isn't defined the barclamp-neutron's default is used."
    echo
    exit 1
}


function addupdaterepo()
{
    sshrun "addupdaterepo=1 UPDATEREPOS=$UPDATEREPOS  bash -x qa_crowbarsetup.sh $virtualcloud"
    return $?
}

function runupdate()
{
    sshrun "runupdate=1 bash -x qa_crowbarsetup.sh $virtualcloud"
    return $?
}

function is_concurrent_run()
{
    [ -e $pidfile ] && kill -0 `cat $pidfile` 2>/dev/null && return 0
    echo $$ > $pidfile
    return 1
}

function sanity_checks()
{
    if test `id -u` != 0 ; then
        complain 1 "This script needs to be run as root" \
            "Please be aware that this script will create a LVM" \
            "and kill all current VMs on this host."
    fi

    if is_concurrent_run; then
        complain 33 "mkcloud was started twice from same working directory: `pwd`" \
            "Please always use a separate working directory for each (parallel) mkcloud run."
    fi

    # always fetch the latest version
    if [ -z "$NOQACROWBARDOWNLOAD" ] ; then
        #wget -O qa_crowbarsetup.sh "https://raw.githubusercontent.com/SUSE-Cloud/automation/master/scripts/qa_crowbarsetup.sh"
        # Switch to curl as wget has an issue with github: https://github.com/netz98/n98-magerun/issues/75
        curl -s -o qa_crowbarsetup.sh "https://raw.githubusercontent.com/SUSE-Cloud/automation/master/scripts/qa_crowbarsetup.sh"
    fi

    if [ ! -e qa_crowbarsetup.sh ] ; then
        complain 87 "qa_crowbarsetup.sh not found in same directory" \
            "Please put the latest version of this script in the same directory."
    fi

    if [ -z "$cloudsource" ] ; then
        echo "Please set the env variable:"
        echo "export cloudsource=Beta?|develcloud2.0|develcloud3|develcloud4|develcloud5|susecloud4"
        exit 1
    fi

    if [ -n "$hacloud" ] ; then
        if (( $nodenumber < 6 )) ; then
            complain 1 "A HA cloud setup requires at least 6 nodes, you configured only: $nodenumber" \
                "Please raise the number of nodes via e.g. export nodenumber=6"
        fi
    fi

    vgdisplay "$cloudvg" >/dev/null 2>&1 && needcvol=
    if [ -n "$needcvol" ] ; then
        cloudpv=${cloudpv:-/dev/vdb}
        if grep -q $cloudpv /proc/mounts ; then
            complain 92 "The device $cloudpv seems to be used. Exiting."
        fi
        if [ ! -e $cloudpv ] ; then
            complain 93 "$cloudpv does not exist." \
                "Please set the cloud volume group to an existing device: export cloudpv=/dev/sdx" \
                "Running 'partprobe' may help to let the device appear."
        fi
    fi

    if [ -e /etc/init.d/SuSEfirewall2_init ] && rcSuSEfirewall2 status ; then
        complain 91 "SuSEfirewall is running - it will interfere with the iptables rules done by libvirt" \
            "Please stop the SuSEfirewall completely and run mkcloud again" \
            "Run:  rcSuSEfirewall2 stop && insserv -r SuSEfirewall2_setup && insserv -r SuSEfirewall2_init"
    fi

    if grep "devpts.*[^x]mode=.00" /proc/mounts ; then
        complain 13 "/dev/pts is not accessible for libvirt, maybe you use autobuild on your system." \
            "Please remount it using the following command:" \
            " # mount -o remount,mode=620,gid=5 devpts -t devpts /dev/pts"
    fi
}


## MAIN ##
step_aliases="alias_new_admin alias_compute alias_upgrade alias_testupdate"

allcmds="$step_aliases all all_noreboot instonly plain plain_with_upgrade cleanup prepare setupadmin prepareinstcrowbar instcrowbar instcrowbarfromgit setupcompute instcompute proposal testsetup rebootcrowbar rebootcompute addupdaterepo runupdate testupdate securitytests crowbarbackup crowbarrestore shutdowncloud restartcloud qa_test help rebootneutron prepare_cloudupgrade cloudupgrade_1st cloudupgrade_2nd cloudupgrade_clients cloudupgrade_reboot_and_redeploy_clients"
wantedcmds=$@

if [ -z "$wantedcmds" ] ; then
    usage
fi

function expand_steps()
{
    # parse the commands and expand the aliases
    local runcmds=''
    local localwantedcmds=$@
    local cmd
    for cmd in $localwantedcmds ; do

        local found=0
        local onecmd
        for onecmd in $allcmds ; do
            if [ $onecmd = $cmd ] ; then
                found=1
                case "$cmd" in
                    alias_new_admin)
                        runcmds="$runcmds cleanup prepare setupadmin addupdaterepo runupdate prepareinstcrowbar instcrowbar"
                    ;;
                    alias_compute)
                        runcmds="$runcmds setupcompute instcompute proposal"
                    ;;
                    all)
                        runcmds="$runcmds `expand_steps alias_new_admin` rebootcrowbar `expand_steps alias_compute` testsetup rebootcompute"
                    ;;
                    all_noreboot)
                        runcmds="$runcmds`expand_steps alias_new_admin` `expand_steps alias_compute` testsetup"
                    ;;
                    alias_testupdate|testupdate)
                        runcmds="$runcmds addupdaterepo runupdate testsetup"
                    ;;
                    plain)
                        runcmds="$runcmds `expand_steps instonly` proposal"
                    ;;
                    instonly)
                        runcmds="$runcmds cleanup prepare setupadmin prepareinstcrowbar instcrowbar setupcompute instcompute"
                    ;;
                    alias_upgrade)
                        runcmds="$runcmds prepare_cloudupgrade cloudupgrade_1st cloudupgrade_2nd cloudupgrade_clients cloudupgrade_reboot_and_redeploy_clients"
                    ;;
                    plain_with_upgrade)
                        runcmds="$runcmds `expand_steps plain` addupdaterepo runupdate `expand_steps alias_upgrade`"
                    ;;
                    *)
                        runcmds="$runcmds $cmd"
                    ;;
                esac
            fi
        done
        [ $found == 0 ] && complain - "Step $cmd not found." && return

    done
    echo "$runcmds"
}

steplist=`expand_steps $wantedcmds`
[ -z "$steplist" ] && usage

sanity_checks

echo "You choose to run these mkcloud steps:"
echo "  $steplist"
echo
sleep 2

for cmd in `echo $steplist` ; do
    echo
    echo "============> MKCLOUD STEP START: $cmd <============"
    echo
    sleep 2
    $cmd
    ret=$?
    if [ $ret != 0 ] ; then
        echo
        echo '$h1!!'
        echo "Error detected. Stopping mkcloud."
        echo "The step '$cmd' returned with exit code $ret"
        echo "Please refer to the $cmd function in this script when debugging the issue."
        error_exit $ret ""
    fi >&2
    echo
    echo "^^^^^^^^^^^^= MKCLOUD STEP DONE: $cmd =^^^^^^^^^^^^"
    echo
done

pre_exit_cleanup
show_environment
