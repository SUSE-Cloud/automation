#!/bin/bash
#
# mkcloud - Setup a virtual cloud on one system (physical or even virtual)
#
# Authors: J. Daniel Schmidt <jdsn@suse.de>
#          Bernhard M. Wiedemann <bwiedemann@suse.de>
#
# 2012, SUSE LINUX Products GmbH
#

# Quick introduction:
#
# 1. read the usage: mkcloud help
#    or visit https://github.com/SUSE-Cloud/automation/blob/master/docs/mkcloud.md
# 2. This tool relies on the script qa_crowbarsetup.sh
# 3. Please 'export' environment variables according to your needs.

: ${cloud:=cloud}
: ${log_dir:=/var/log/mkcloud/$cloud}
mkdir -p "$log_dir"
log_file=$log_dir/`date -Iseconds`.log
exec >  >(tee -ia $log_file)
exec 2> >(tee -ia $log_file >&2)

if [[ $debug_mkcloud = 1 ]] ; then
    set -x
    PS4='+(${BASH_SOURCE##*/}:${LINENO}) ${FUNCNAME[0]:+${FUNCNAME[0]}(): }'
fi

# FIXME: separate user-tweakable parameters from script local variables.
# Currently there is no clearly defined interface point.  One of the
# causes of this is violation of the common shell coding standard which
# uses uppercase for environment variables and constants, and lowercase
# for local variables.
: ${mkcloud_dir:=$(dirname $(readlink -e $0))}
: ${qa_crowbarsetup:=${mkcloud_dir}/qa_crowbarsetup.sh}
: ${iscsictl:=${mkcloud_dir}/iscsictl.py}
mkcloud_lib_dir=${mkcloud_dir}/lib
# include separate bash libs
# NOTE that this is a temporary solution during refactoring of mkcloud
mkcloud_temporary_scripts="mkcloud-libvirt.sh"
for script in $mkcloud_temporary_scripts; do
    source ${mkcloud_lib_dir}/$script
done
mkcconf=mkcloud.config
rm -f $mkcconf # we dont want to source old info in the line below
source $qa_crowbarsetup

: ${virtualcloud:=virtual}
: ${net_admin:=192.168.124}
setcloudnetvars $virtualcloud
: ${forwardmode:=nat}
: ${adminnetmask:=255.255.248.0}
# the default nodenumber of compute nodes
nodenumbercomputedefault=2
[[ $hacloud ]] && nodenumbercomputedefault=1
: ${nodenumber:=$nodenumbercomputedefault}
: ${nodenumbercompute:=$nodenumbercomputedefault}
# expect to have this many physical machines attached via $mkch_physcloudif
# these replace virtual machines, so we start less VMs
: ${nodenumberphys:=0}
[[ $nodenumberphys = 0 ]] || [[ $mkch_physcloudif ]] || complain 100 "need to set mkch_physcloudif for physical nodes"
# configuration of clusters
: ${clusterconfig:=''}
: ${nodenumberlonelynode:=0}
export nodenumber nodenumbercompute nodenumberlonelynode clusterconfig
# '+'-separated list of MAC#serial_of_drbd_volume of the drbd cluster nodes
# (used only internally to transport this information to qa_crowbarsetup):
: ${drbdnode_mac_vol:=''}
: ${cephvolumenumber:=1}
: ${controller_raid_volumes:=0}
allnodeids_without_lonely=`seq 1 $((nodenumber-nodenumberphys))`
lonelynodeids=`seq $(( nodenumber + 1 )) $(( nodenumber + nodenumberlonelynode ))`
allnodeids="$allnodeids_without_lonely $lonelynodeids"
: ${vcpus:=2}
: ${adminvcpus:=1}
cpuflags=''
working_dir_orig=`pwd`
: ${artifacts_dir:=$working_dir_orig/.artifacts}
start_time=`date`
: ${cloudvg:=cloud}
needcvol=1
: ${vdisk_dir:=/dev/$cloudvg}
: ${admin_node_disk:=$vdisk_dir/$cloud.admin}
: ${admin_node_memory:=4194304}
: ${controller_node_memory:=6291456}
: ${compute_node_memory:=2621440}
: ${hyperv_node_memory:=3000000}
[[ "$libvirt_type" = "hyperv" && $compute_node_memory -lt $hyperv_node_memory ]] && compute_node_memory=$hyperv_node_memory
: ${xen_node_memory:=4000000}
[[ "$libvirt_type" = "xen" && $compute_node_memory -lt $xen_node_memory ]] && compute_node_memory=$xen_node_memory
# hdd size defaults (unless defined otherwise)
: ${adminnode_hdd_size:=15}
: ${controller_hdd_size:=20}
: ${computenode_hdd_size:=20}
: ${cephvolume_hdd_size:=21}
: ${controller_ceph_hdd_size:=25}
if [[ $hacloud ]]; then
    : ${drbd_hdd_size:=15}
else
    : ${drbd_hdd_size:=0}
fi
: ${drbd_database_size:=5}
: ${drbd_rabbitmq_size:=5}
# magnum sets up two nodes using docker/kubernetes on compute node
# each node requiring 2GB of RAM and 20GB of harddisk for the images
if [ -n "$want_magnum" ]; then
    : ${compute_node_memory:=6621440}
    : ${computenode_hdd_size:=80}
fi
# pvlist is filled below
pvlist=
next_pv_device=
pv_cur_device_no=0
: ${cloudbr:=${cloud}br}
sshopts="-oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null"
scp="scp $sshopts"
#if localreposdir_src string is available, the local repositories are used for setup
: ${localreposdir_src:=""}
#localreposdir_target is the 9p target dir and also the mount target dir in the VM
: ${localreposdir_target:="/repositories"}
[ -z "$localreposdir_src" ] && localreposdir_target=""
: ${install_chef_suse_override:=./install-chef-suse.sh}
: ${cct_tests:="features:base"}
: ${want_mtu_size:=1500}

emulator=/usr/bin/qemu-system-$arch
if [ -x /usr/bin/qemu-kvm ] && file /usr/bin/qemu-kvm | grep -q ELF; then
    # on SLE11, qemu-kvm is preferred, since qemu-system-x86_64 is
    # some rotten old stuff without KVM support
    emulator=/usr/bin/qemu-kvm
fi

pidfile=mkcloud.pid

trap 'error_exit $? "error caught by trap"' TERM
exec </dev/null

function is_suse()
{
    grep -qi suse /etc/*release
}

function show_environment()
{
    end_time=`date`
    echo "Environment Details"
    echo "-------------------------------"
    echo "    hostname: `hostname -f`"
    echo "     started: $start_time"
    echo "       ended: $end_time"
    echo "-------------------------------"
    echo " cloudsource: $cloudsource"
    echo "    TESTHEAD: $TESTHEAD"
    echo " want_test_updates: $want_test_updates"
    echo "    scenario: $scenario"
    echo "  nodenumber: $nodenumber"
    echo "     cloudpv: $cloudpv"
    echo " UPDATEREPOS: $UPDATEREPOS"
    echo "    cephvolumenumber: $cephvolumenumber"
    echo " upgrade_cloudsource: $upgrade_cloudsource"
    echo "-------------------------------"
    env | grep -i "^want_"
    echo "-------------------------------"
}

function pre_exit_cleanup()
{
    rm $pidfile
}

function error_exit()
{
    exitcode=$1
    message=$2
    if [ -z "$SKIPSUPPORTCONFIG" ] ; then
        ssh $sshopts root@$adminip '
            set -x
            for node in $(crowbar machines list | grep ^d) ; do
            (
                echo "Collecting supportconfig from $node"
                timeout 400 ssh $node supportconfig | wc
                timeout 300 scp $node:/var/log/\*tbz /var/log/
            )&
            done
            timeout 500 supportconfig | wc &
            wait
        '
        mkdir -p $artifacts_dir
        $scp root@$adminip:/var/log/*tbz $artifacts_dir/
    fi
    pre_exit_cleanup
    echo $message
    show_environment
    exit $exitcode
} >&2

function determine_host_mtu()
{
    host_mtu=$(LC_ALL=C sort -n /sys/class/net/*/mtu | head -n 1)
}

function sshrun()
{
    cat > $mkcconf <<EOF
        export drbdnode_mac_vol=$drbdnode_mac_vol ;
        export cloud=$virtualcloud ;
        # hostname of NFS server with repos and images:
        export clouddata=$clouddata ;
        export distsuse=$distsuse ;
        export susedownload=$susedownload ;
        export cloudfqdn=$cloudfqdn ;
        export cloudsource=$cloudsource ;
        export upgrade_cloudsource=$upgrade_cloudsource ;
        export adminip=$adminip ;
        export hacloud=$hacloud ;
        export libvirt_type=$libvirt_type ;
        export networkingplugin=$networkingplugin ;
        export networkingmode=$networkingmode ;
        export nosetestparameters=${nosetestparameters} ;
        export tempestoptions='${tempestoptions}' ;
        export ostestroptions='${ostestroptions}' ;
        export cephvolumenumber=$cephvolumenumber ;
        export controller_raid_volumes=$controller_raid_volumes ;
        export drbd_database_size=$drbd_database_size ;
        export drbd_rabbitmq_size=$drbd_rabbitmq_size ;
        export shell=$shell ;
        export keep_existing_hostname=$keep_existing_hostname ;
        export cct_tests=$cct_tests ;
        export scenario=$scenario ;
        export host_mtu=$host_mtu ;

        export nova_shared_instance_storage=$nova_shared_instance_storage ;

        export localreposdir_target=$localreposdir_target ;

        export cinder_backend=$cinder_backend;
        export cinder_netapp_storage_protocol=$cinder_netapp_storage_protocol;
        export cinder_netapp_login=$cinder_netapp_login;
        export cinder_netapp_password=$cinder_netapp_password;

        export TESTHEAD=$TESTHEAD ;
        export NOINSTALLCLOUDPATTERN=$NOINSTALLCLOUDPATTERN ;

        export clouddescription=$clouddescription;
        export JENKINS_BUILD_URL=$BUILD_URL;
        export JENKINS_NODE_NAME=$NODE_NAME;
        export JENKINS_EXECUTOR_NUMBER=$EXECUTOR_NUMBER;
        export JENKINS_WORKSPACE=$WORKSPACE;
EOF
    env|grep -e ^debug_ -e ^pre_ -e ^vlan_ -e ^want_ -e ^net_ -e ^nodenumber -e ^clusterconfig | sort >> $mkcconf

    $scp $qa_crowbarsetup $mkcconf $iscsictl root@$adminip:
    [[ $need_scenario = 1 ]] && $scp $scenario_path root@$adminip:
    ssh $sshopts root@$adminip "echo `hostname` > cloud ; . qa_crowbarsetup.sh ; $@"
    return $?
}

function onadmin()
{
    # functions can have parameters, so pass on all except $1
    local cmd=$1
    shift
    sshrun onadmin_$cmd "$@"
}

function onhost()
{
    # functions can have parameters, so pass on all except $1
    local cmd=$1
    shift
    onhost_$cmd "$@"
}

function cleanup()
{
    # cleanup leftover from last run
    ${mkcloud_lib_dir}/libvirt/cleanup $cloud $nodenumber $cloudbr $vlan_public

    if ip link show ${cloudbr}.$vlan_public >/dev/null 2>&1; then
        ip link set ${cloudbr}.$vlan_public down
    fi
    if ip link show ${cloudbr} >/dev/null 2>&1; then
        ip link set ${cloudbr} down
        ip link delete ${cloudbr} type bridge
        ip link delete ${cloudbr}-nic
    fi
    # 1. remove leftover partition mappings that are still open for this cloud
    local vol
    dmsetup ls | awk "/^$cloudvg-${cloud}\./ {print \$1}" | while read vol ; do
        kpartx -dsv /dev/mapper/$vol
    done

    # workaround host grabbing guest devices
    for vol in postgresql rabbitmq ; do
        dmsetup remove drbd-$vol
    done
    # 2. remove all previous volumes for that cloud; this helps preventing
    # accidental booting and freeing space
    if [ -d $vdisk_dir ]; then
        find -L $vdisk_dir -name "$cloud.*" -type b | \
            xargs --no-run-if-empty lvremove --force || complain 104 "lvremove failure"
    fi
    rm -f /etc/lvm/archive/*

    if [[ $wipe = 1 ]] ; then
        vgchange -an $cloudvg
        dd if=/dev/zero of=$cloudpv count=1000
    fi
    return 0
}


function onhost_cleanup_admin_node()
{
    # this function is meant to only clean the admin node
    # in order to deploy a new one, while keeping all cloud nodes

    ${mkcloud_lib_dir}/libvirt/cleanup_one_node ${cloud}-admin
}

function onhost_get_next_pv_device()
{
    if [ -z "$pvlist" ] ; then
        pvlist=`pvs --sort -Free | awk '$2~/'$cloudvg'/{print $1}'`
        pv_cur_device_no=0
    fi
    next_pv_device=`perl -e '$i=shift; $i=$i % @ARGV;  print $ARGV[$i]' $pv_cur_device_no $pvlist`
    pv_cur_device_no=$(( $pv_cur_device_no + 1 ))
}

# create lv device wrapper
function _lvcreate()
{
    lv_name=$1
    lv_size=$2
    lv_vg=$3
    lv_pv=$4

    # first: create on the PV device (spread IO)
    # fallback: create in VG (if PVs with different size exist)
    lvcreate -n $lv_name -L ${lv_size}G $lv_vg $lv_pv || \
        safely lvcreate -n $lv_name -L ${lv_size}G $lv_vg
}

# spread block devices over a LVM's PVs so that different VMs
# are likely to use different PVs to optimize concurrent IO throughput
function onhost_create_cloud_lvm()
{
    safely vgchange -ay $cloudvg # for later boots

    local hdd_size

    onhost_get_next_pv_device
    _lvcreate $cloud.admin $adminnode_hdd_size $cloudvg $next_pv_device
    for i in $allnodeids ; do
        onhost_get_next_pv_device
        hdd_size=${computenode_hdd_size}
        test "$i" = "1" && hdd_size=${controller_hdd_size}
        _lvcreate $cloud.node$i $hdd_size $cloudvg $next_pv_device
    done
    if [ $controller_raid_volumes -gt 1 ] ; then
        # total wipeout of the disks used for RAID, to prevent bsc#966685
        volume="/dev/$cloudvg/$cloud.node1"
        dd if=/dev/zero of=$volume bs=1M count=$(($controller_hdd_size * 1024))
        for n in $(seq 1 $(($controller_raid_volumes-1))) ; do
            onhost_get_next_pv_device
            hdd_size=${controller_hdd_size}
            _lvcreate $cloud.node1-raid$n $hdd_size $cloudvg $next_pv_device
            volume="/dev/$cloudvg/$cloud.node1-raid$n"
            dd if=/dev/zero of=$volume bs=1M count=$(($hdd_size * 1024))
        done
    fi

    if [ $cephvolumenumber -gt 0 ] ; then
        for i in $allnodeids ; do
            for n in $(seq 1 $cephvolumenumber) ; do
                onhost_get_next_pv_device
                hdd_size=${cephvolume_hdd_size}
                test "$i" = "1" -a "$n" = "1" && hdd_size=${controller_ceph_hdd_size}
                _lvcreate $cloud.node$i-ceph$n $hdd_size $cloudvg $next_pv_device
            done
        done
    fi

    # create volumes for drbd
    if [ $drbd_hdd_size != 0 ] ; then
        for i in `seq 1 2`; do
            onhost_get_next_pv_device
            _lvcreate $cloud.node$i-drbd $drbd_hdd_size $cloudvg $next_pv_device
        done
    fi

    echo "Checking for LVs treated by LVM as valid PV devices ..."
    if [[ $SHAREDVG != 1 ]] && lvmdiskscan | egrep "/dev/($cloudvg/|mapper/$cloudvg-)"; then
        error=$(cat <<EOF
Error: your lvm.conf is not filtering out mkcloud LVs.
Please fix by adding the following regular expressions
to the filter value in the devices { } block within your
/etc/lvm/lvm.conf file (Be sure to place them before "a/.*/"):

    "r|/dev/mapper/$cloudvg-|", "r|/dev/$cloudvg/|", "r|/dev/disk/by-id/|"

The filter should also include something like "r|/dev/dm-|" or "r|/dev/dm-1[56]|", but
the exact values depend on your local system setup and could change
over time or have side-effects (on lvm in dm-crypt or lvm in lvm),
so please add/modify it manually.
EOF
)
        complain 94 "$error"
    fi
}

function onhost_deploy_image()
{
    local role=$1
    local image=$(dist_to_image_name $2)
    local disk=$3

    [[ $clouddata ]] || complain 108 "clouddata IP not set - is DNS broken?"
    pushd /tmp
    safely wget --progress=dot:mega -N \
        http://$clouddata/images/$arch/$image

    echo "Cloning $role node vdisk from $image ..."
    safely qemu-img convert -t none -O raw -S 0 -p $image $disk
    popd

    resize_partition $disk
}

function onhost_add_etchosts_entries()
{
    grep -q crowbar /etc/hosts || echo "$adminip crowbar.$cloudfqdn crowbar" >> /etc/hosts
}

function onhost_enable_ksm
{
    # enable kernel-samepage-merging to save RAM
    [[ -w /sys/kernel/mm/ksm/merge_across_nodes ]] && echo 0 > /sys/kernel/mm/ksm/merge_across_nodes
    [[ -w /sys/kernel/mm/ksm/run ]] && echo 1 > /sys/kernel/mm/ksm/run
    # Don't waste a complete CPU core on low-core count machines
    local ppcpu=64
    # aarch64 machines have high core count but low single-core performance
    [ $(uname -m) = aarch64 ] && ppcpu=4
    local pts=$(($(lscpu -p | grep -vc '^#')*$ppcpu))
    [[ -w /sys/kernel/mm/ksm/pages_to_scan ]] && echo $pts > /sys/kernel/mm/ksm/pages_to_scan

    # huge pages can not be shared or swapped, so do not use them
    [[ -w /sys/kernel/mm/transparent_hugepage/enabled ]] && echo never > /sys/kernel/mm/transparent_hugepage/enabled
}

function setuphost()
{
    if is_suse ; then
        export ZYPP_LOCK_TIMEOUT=60
        kvmpkg=kvm
        osloader=
        ipxe=
        [[ $arch = aarch64 ]] && {
            kvmpkg=qemu-arm
            osloader=qemu-uefi-aarch64
            ipxe=qemu-ipxe
        }
        [[ $arch = s390x ]] && kvmpkg=qemu-s390
        zypper --non-interactive in --no-recommends \
            libvirt $kvmpkg $osloader $ipxe lvm2 curl wget bridge-utils \
            dnsmasq netcat-openbsd ebtables libvirt-python
        [ "$?" == 0 -o "$?" == 4 ] || complain 10 "setuphost failed to install required packages"

        # enable KVM
        [[ $arch = s390x ]] && {
            echo 1 > /proc/sys/vm/allocate_pgste
        }
    fi

    sed -i 's/net.ipv4.ip_forward = 0/net.ipv4.ip_forward = 1/' /etc/sysctl.conf
    echo "net.ipv4.conf.all.rp_filter = 0" > /etc/sysctl.d/90-cloudrpfilter.conf
    echo 0 > /proc/sys/net/ipv4/conf/all/rp_filter
    if [ -n "$needcvol" ] ; then
        safely pvcreate "$cloudpv"
        safely vgcreate "$cloudvg" "$cloudpv"
    fi
}

function prepare()
{
    if ! [ -e ~/.ssh/id_rsa ]; then
        echo "Creating key for controlling our VMs..."
        ssh-keygen -t rsa -f ~/.ssh/id_rsa -N ""
    fi

    onhost_enable_ksm
    onhost_create_cloud_lvm
    onhost_add_etchosts_entries
    libvirt_prepare
    onhost_prepareadmin
}

function onhost_prepareadmin()
{
    onhost_deploy_image "admin" $(get_admin_node_dist) "$admin_node_disk"
}

function ssh_password()
{
    SSH_ASKPASS=/root/echolinux
    cat > $SSH_ASKPASS <<EOSSHASK
#!/bin/sh
echo linux
EOSSHASK
    chmod +x $SSH_ASKPASS
    DISPLAY=dummydisplay:0 SSH_ASKPASS=$SSH_ASKPASS setsid ssh $sshopts -oNumberOfPasswordPrompts=1 "$@"
}

function wait_for_crowbar_ssh()
{
    wait_for 150 1 "nc -z $adminip 22" 'admin node to start ssh daemon'
}

function wait_for_crowbar_ntpd()
{
    wait_for 200 10 "ntpdate -d $adminip" "admin node ntpd service"
}

# bring up the VM for crowbar
function setupadmin()
{
    local ofs=$IFS nfs=$'\n' mss

    libvirt_setupadmin

    wait_for 300 1 "ping -q -c 1 -w 1 $adminip >/dev/null" 'crowbar admin VM'
    wait_for_crowbar_ssh

    wait_for 20 2 "nc -z $adminip 22" 'sshd to start'
    echo "Injecting public key into admin node..."
    local keyfile
    for keyfile in ~/.ssh/*.pub ; do
        local pubkey=`cat $keyfile`
        ssh_password $adminip "
            mkdir -p ~/.ssh;
            grep -q '$pubkey' ~/.ssh/authorized_keys 2>/dev/null ||
                echo '$pubkey injected-from-host' >> ~/.ssh/authorized_keys
        "
    done
    if [[ $user_keyfile ]]; then
        cat $user_keyfile | sshrun "cat >> ~/.ssh/authorized_keys"
    fi
    echo "you can now proceed with installing crowbar"
    # prevent jumbo frames from going out
    if [[ $want_mtu_size -gt 1500 ]] || [[ $host_mtu -lt 1500 ]]; then
        # we subtract 40 to account for the IP + TCP headers.
        let mss=host_mtu-40
        # Remove all previous TCPMSS rules
        IFS=$nfs
        for x in $(iptables -S FORWARD | grep -o --color=never FORWARD.*TCPMSS.*); do
            IFS=$ofs
            iptables -D ${x} 2>/dev/null
            IFS=$nfs
        done
        IFS=$ofs
        iptables -I FORWARD -p tcp --tcp-flags SYN,RST SYN -j TCPMSS --set-mss $mss
    fi
}

function wait_for_node_shutdown()
{
    wait_for 150 1 "virsh domstate $1 | grep shut.off" "$1 node to shut down"
}

function remove_snapshot_volume()
{
    if $(lvs "$1.snap" > /dev/null 2>&1) ; then
        safely lvremove -f "$1.snap"
    fi
}

function merge_snapshot_volume()
{
    if $(lvs "$1.snap" > /dev/null 2>&1) ; then
        safely lvconvert --merge "$1.snap"
    fi
}

function create_snapshot_volume()
{
    safely lvcreate -l100%ORIGIN -s -n "$1.snap" "$1"
}

function createadminsnapshot()
{
    virsh shutdown $cloud-admin
    wait_for_node_shutdown $cloud-admin
    remove_snapshot_volume $admin_node_disk
    create_snapshot_volume $admin_node_disk
    virsh start $cloud-admin
    wait_for_crowbar_ssh
}

function restoreadminfromsnapshot()
{
    virsh destroy $cloud-admin
    merge_snapshot_volume $admin_node_disk
    createadminsnapshot
}

function createcloudsnapshot()
{
    shutdowncloud
    wait_for_node_shutdown $cloud-admin
    for i in $allnodeids ; do
        wait_for_node_shutdown $cloud-node$i
    done
    for d in $(lvs -o lv_name ${cloudvg} | grep "${cloud}\..*snap") ; do
        remove_snapshot_volume "${cloudvg}/${d%.snap}"
    done
    for d in $(lvs -o lv_name ${cloudvg} | grep ${cloud}\.) ; do
        create_snapshot_volume "${cloudvg}/${d}"
    done
    restartcloud
}

function restorecloudfromsnapshot()
{
    local can_restore=1
    lvs -o lv_name ${cloudvg} | grep "${cloud}.admin.snap" || {
        echo "Missing snapshot for ${cloud}.admin"
        can_restore=
    }
    for i in $allnodeids; do
        if [[ ! ${can_restore} ]]; then
            break
        fi
        lvs -o lv_name ${cloudvg} | grep "${cloud}.node$i.snap" || {
            echo "Missing snapshot for ${cloud}.node$i"
            can_restore=
        }
    done

    if [[ ! ${can_restore} ]]; then
        complain 88 "Cannot restore cloud, missing snapshot(s)"
        return
    fi

    virsh destroy $cloud-admin
    for i in $allnodeids ; do
        virsh destroy $cloud-node$i
    done
    for d in $(lvs -o lv_name ${cloudvg} | grep "${cloud}\..*snap") ; do
        merge_snapshot_volume "${cloudvg}/${d%.snap}"
    done
    createcloudsnapshot
}

# Returns success if a change was made
function confset()
{
    file="$1" key="$2" value="$3"
    if grep -q "^$key *= *$value" "$file"; then
        return 1 # already set correctly
    fi

    new_line="$key = $value"
    if grep -q "^$key[ =]" "$file"; then
        # change existing value
        sed -i "s/^$key *=.*/$new_line/" "$file"
    elif grep -q "^# *$key[ =]" "$file"; then
        # uncomment existing setting
        sed -i "s/^# *$key *=.*/$new_line/" "$file"
    else
        # add new setting
        echo "$new_line" >> "$file"
    fi

    return 0
}

function macfunc()
{
    printf "52:54:77:77:77:%02x" $1
}

function get_admin_node_dist()
{
    # echo the name of the current dist for the admin node
    local dist=
    case $(getcloudver) in
        7)
            dist=SLE12SP2
            [[ $want_sles12sp1_admin ]] && dist=SLE12SP1
            ;;
        6)  dist=SLE12SP1 ;;
        5)  dist=SLE11    ;;
        *)  dist=SLE11    ;;
    esac
    echo "$dist"
}

function get_lonely_node_dist()
{
    local dist=$(get_admin_node_dist)
    iscloudver 5 && [[ $want_sles12 ]] && dist=SLE12
    echo $dist
}

function dist_to_image_name()
{
    # get the name of the image to deploy the admin node
    local dist=$1
    case $dist in
        SLE12SP2) image=SLES12-SP2 ;;
        SLE12SP1) image=SLES12-SP1 ;;
        SLE12)    image=SLES12     ;;
        SLE11)    image=SP3-64up   ;;
        *)
            complain 71 "No admin node image defined for this distribution: $dist"
        ;;
    esac
    echo "$image.qcow2"
}

# bring up lonely_node(s) in the admin network
function setuplonelynodes()
{
    local i
    for i in $lonelynodeids; do
        local mac=$(macfunc $i)
        local lonely_node
        lonely_node=$cloud-node$i
        safely ${mkcloud_lib_dir}/libvirt/compute-config $cloud $i $mac 0\
            "$cephvolumenumber" "$drbdvolume" $compute_node_memory\
            $controller_node_memory $libvirt_type $vcpus $emulator $vdisk_dir\
            1 1 > /tmp/$cloud-node$i.xml

        local lonely_disk
        lonely_disk="$vdisk_dir/${cloud}.node$i"

        onhost_deploy_image "lonely" $(get_lonely_node_dist) $lonely_disk
        ${mkcloud_lib_dir}/libvirt/vm-start /tmp/${lonely_node}.xml
    done
}

# register lonely_node against crowbar
function crowbar_register()
{
    for i in $lonelynodeids; do
        local mac=$(macfunc $i)
        sshrun "lonelymac=$mac adminip=$adminip onadmin_crowbar_register"
    done
}

function prepareinstcrowbar()
{
    echo "connecting to crowbar admin server at $adminip"
    onadmin prepareinstallcrowbar
    return $?
}

function bootstrapcrowbar()
{
    echo "bootstrapping crowbar"
    onadmin bootstrapcrowbar
    return $?
}

function scp_install_chef_suse_override()
{
    if [ -e "$install_chef_suse_override" ]; then
        ssh root@$adminip "mv /opt/dell/bin/install-chef-suse.sh{,.orig}"
        $scp -p "$install_chef_suse_override" \
            root@$adminip:/opt/dell/bin/install-chef-suse.sh
        ssh root@$adminip "chmod +x /opt/dell/bin/install-chef-suse.sh"
    fi
}

function instcrowbar()
{
    scp_install_chef_suse_override
    onadmin installcrowbar
    local ret=$?
    $scp root@$adminip:$crowbar_install_log "$artifacts_dir/"
    return $ret
}

function instcrowbarfromgit()
{
    scp_install_chef_suse_override
    safely rsync -av --exclude ".git" --exclude ".ci-tracking" ./crowbar root@$adminip:/root/
    onadmin installcrowbarfromgit
    local ret=$?
    $scp root@$adminip:$crowbar_install_log "$artifacts_dir/"
    return $ret
}

function mkvlan()
{
    local DEFVLAN=$1 ; shift
    local IP=$1 ; shift
    vconfig set_name_type DEV_PLUS_VID_NO_PAD
    vconfig add $cloudbr $DEFVLAN
    ifconfig $cloudbr.$DEFVLAN $IP/24
    ethtool -K $cloudbr tx off
}

function hypervisor_has_virtio()
{
    local llibvirt_type=${1:-$libvirt_type}
    [[ "$llibvirt_type" = "xen" || "$llibvirt_type" = "hyperv" ]] && return 1
    return 0
}

function setuppublicnet()
{
    # workaround https://bugzilla.novell.com/show_bug.cgi?id=845496
    echo 0 > /proc/sys/net/bridge/bridge-nf-call-iptables
    mkvlan $vlan_public $net_public.1
    if [[ $mkch_physcloudif ]] ; then
        brctl addif $cloudbr $mkch_physcloudif
        ip link set dev $mkch_physcloudif up
    fi
}

function shutdowncloud()
{
    virsh shutdown $cloud-admin
    for i in $allnodeids ; do
        virsh shutdown $cloud-node$i
    done
}

function restartcloud()
{
    libvirt_net_start
    virsh start $cloud-admin
    setuppublicnet
    wait_for_crowbar_ntpd
    for i in $allnodeids ; do
        virsh start $cloud-node$i
    done
    wait_for_crowbar_ssh
}

# bring up VMs that will take cloud controller/compute/storage roles
function setupnodes()
{
    onadmin wait_tftpd || return $?

    local nodenumbercontroller=1
    if [[ $clusterconfig == *services* ]]; then
        nodenumbercontroller=`echo ${clusterconfig}\
            | sed -e "s/^.*services[^:]*=\([[:digit:]]\+\).*/\1/"`
    fi

    setuppublicnet
    for i in $allnodeids_without_lonely ; do
        local macaddress=$(macfunc $i)

        # transport drdb volume information to admin node (needed for proposal of data cluster)
        drbd_serial=""
        if [ $drbd_hdd_size != 0 ]; then
            if [ $i -le 2 ] ; then
                drbd_serial="$cloud-node$i-drbd"
                # libvirt does not accept anything other than [:alnum:]_-
                # for serial strings:
                drbd_serial=${drbd_serial//[^A-Za-z0-9-_]/_}
                drbdnode_mac_vol="${drbdnode_mac_vol}+${macaddress}#${drbd_serial}"
                drbdnode_mac_vol="${drbdnode_mac_vol#+}"
            fi
        fi

        ${mkcloud_lib_dir}/libvirt/compute-config $cloud $i $macaddress\
            $controller_raid_volumes $cephvolumenumber "$drbd_serial"\
            $compute_node_memory $controller_node_memory $libvirt_type $vcpus\
            $emulator $vdisk_dir 3 $nodenumbercontroller > /tmp/$cloud-node$i.xml
        ${mkcloud_lib_dir}/libvirt/vm-start /tmp/$cloud-node$i.xml
    done

    echo "========================================================"
    echo " Note: If you interrupt mkcloud now and want to proceed"
    echo "       later, make sure to run the following command:"
    echo
    echo " export drbdnode_mac_vol=\"${drbdnode_mac_vol}\""
    echo
    echo "========================================================"
    sleep 10

    # mkch_physwol can be set by the user to a space-separated list of MAC-addrs
    # of pysical nodes to wake up using WoL
    for i in $mkch_physwol ; do
        ether-wake -i $cloudbr $i
    done
    return 0
}

# allocate cloud nodes with an operating system
# and wait for nodes to reach the ready state
function instnodes()
{
    safely onadmin allocate

    echo "Waiting for the installation of the nodes ..."
    safely onadmin waitcloud
    safely onadmin post_allocate
}


function proposal()
{
    onadmin proposal
    return $?
}

function testsetup()
{
    onadmin testsetup
    local ret=$?
    mkdir -p "$artifacts_dir"
    $scp root@$adminip:tempest*.log "$artifacts_dir/"

    # Register the cloud in Rally and import the results
    if [[ $rally_server && -f $artifacts_dir/tempest.subunit.log ]] ; then
        local title=$(testname)
        local type=$(testtype)

        $scp root@$adminip:.openrc "$artifacts_dir/"
        local tmpdir=`ssh $sshopts rally@$rally_server "mktemp -d rally-XXXXXX"`
        $scp "$artifacts_dir/tempest.subunit.log" "$artifacts_dir/.openrc" rally@$rally_server:$tmpdir
        ssh $sshopts rally@$rally_server "
            source rally/bin/activate
            source $tmpdir/.openrc
            deployment=\$(rally deployment create --fromenv --name=\"$title\" | awk '/Using deployment:/{print \$3}')
            rally verify import --set-name=\"$type\" --file=$tmpdir/tempest.subunit.log --deployment=\$deployment
            rm -fr $tmpdir
        "
    fi

    return $ret
}

function testname()
{
    local -a var_names=(ha cluster upgrade ceph cinder drbd net
        netmode sles12 dvr docker rootfs)
    local -a var_values=("$hacloud" "$clusterconfig"
        "$upgrade_cloudsource" "$cephvolumenumber" "$cinder_backend"
        "$drbd_hdd_size" "$networkingplugin" "$networkingmode"
        "$want_sles12" "$want_dvr" "$want_docker" "$want_rootfs")

    if [[ ${#var_names[@]} != ${#var_values[@]} ]] ; then
        complain 123 "testname error: arrays of different length, please check the code"
    fi

    local name="$cloudsource"
    local i=0
    for n in ${var_names[@]} ; do
        if [[ ${var_values[$i]} ]] ; then
            name="$name/$n:${var_values[$i]}"
        fi
        i=$(($i+1))
    done

    # Check if this instance of mkcloud is running inside Jenkins
    if [[ $JENKINS_URL ]] ; then
        name="$BUILD_NUMBER - $name - $BUILD_URL"
    fi

    echo $name
}

function testtype()
{
    # If `tempestoptions` contains '-s' or '--smoke' parameter, is a
    # smoke test.  If there is '--load-list 2015.03.required.txt' is a
    # defcore test.  A full test is supossed by default.
    local type="full"
    if [[ $tempestoptions ]] ; then
        if [[ $tempestoptions =~ -s ]] ; then
            type="smoke"
        elif [[ $tempestoptions =~ --load-list.*.required.txt ]] ; then
            type="defcore"
        elif [[ $tempestoptions =~ --load-list ]] ; then
            type="ad-hoc"
        fi
    fi

    echo $type
}

function rebootcrowbar()
{
    # reboot the crowbar instance
    #  and test if everything is up and running afterwards
    sshrun "reboot"
    wait_for 50 3 "! nc -z $adminip 22" 'crowbar to go down'
    wait_for_crowbar_ssh
    echo "waiting another 180 seconds for services"
    sleep 180
    sshrun "mount -a -t nfs" # workaround repos not mounted on reboot because NFS needs bind, but bind says it requires NFS
    return $?
}

function rebootcloud()
{
    # reboot compute nodes
    #  and test if everthing is up and running afterwards
    onadmin rebootcloud
    return $?
}

function rebootcompute()
{
    echo "WARNING: called deprecated rebootcompute step" >&2
    rebootcloud
    return $?
}

function rebootneutron()
{
    onadmin rebootneutron
    return $?
}

function qa_test()
{
    local ghsc=github.com/SUSE-Cloud
    mkdir -p ~/$ghsc/
    pushd ~/$ghsc/
    if [ -e "qa-openstack-cli" ] ; then
        cd qa-openstack-cli/
        git pull
    else
        git clone https://$ghsc/qa-openstack-cli.git
    fi
    popd
    rsync -av ~/$ghsc/qa-openstack-cli root@$adminip:
    onadmin qa_test
    ret=$?

    mkdir -p .artifacts
    $scp -r root@$adminip:qa_test.logs/ .artifacts/
    return $ret
}

function crowbarbackup()
{
    safely onadmin crowbarbackup "$@"
    local btargetdir=${artifacts_dir:-.}
    mkdir -p "$btargetdir"
    safely $scp root@$adminip:/tmp/backup-crowbar.tar.gz "$btargetdir"
}

function crowbarrestore()
{
    local btarball=backup-crowbar.tar.gz
    [ -e "$artifacts_dir/$btarball" ] && btarball="$artifacts_dir/$btarball"
    if [ ! -e "$btarball" ] ; then
        complain 56 "No crowbar backup tarball found."
    fi
    if iscloudver 5minus ; then
        safely onadmin crowbarpurge
    else
        onhost_reset_admin
    fi
    safely $scp "$btarball" root@$adminip:/tmp/
    safely onadmin crowbarrestore "$@"
}


function onhost_reset_admin()
{
    # deploy and setup a fresh admin node
    safely onhost_cleanup_admin_node
    safely onhost_prepareadmin
    safely setupadmin
    safely prepareinstcrowbar
}

function crowbarupgrade_5plus()
{
    onadmin prepare_crowbar_upgrade
    if iscloudver 6plus ; then
        onadmin prepare_cloudupgrade_repos_6_to_7
        onadmin upgrade_admin_server
        wait_for 200 3 "! nc -z $adminip 22" 'crowbar to go down after upgrade'
        wait_for_crowbar_ssh
        onadmin check_admin_server_upgraded
        # use crowbar-init to bootstrap crowbar
        want_postgresql=1 onadmin bootstrapcrowbar 1
        onadmin activate_repositories
    else
        crowbarbackup "with_upgrade"
        export cloudsource=$upgrade_cloudsource
        crowbarrestore "with_upgrade"
    fi
}

function cloudupgrade()
{
    if iscloudver 6plus ; then
        crowbarupgrade_5plus
        # upgrade and reapply all barclamps

        # commented out while developing the 6->7 upgrade process
        #onadmin runlist crowbar_nodeupgrade \
        #                proposal

    elif iscloudver 5plus; then
        crowbarupgrade_5plus
        # upgrade and reapply all barclamps
        onadmin runlist crowbar_nodeupgrade \
                        proposal

    else
        onadmin runlist prepare_cloudupgrade \
                        cloudupgrade_1st \
                        cloudupgrade_2nd \
                        cloudupgrade_clients \
                        cloudupgrade_reboot_and_redeploy_clients
    fi
}

function cct()
{
    onadmin run_cct "$@"
    return $?
}

function devsetup()
{
    onadmin devsetup
    return $?
}

function setup_aliases()
{
    onadmin setup_aliases
    return $?
}


function batch()
{
    onadmin batch $scenario
    return $?
}

function show_steps()
{
    cat <<EOSTEPS
Usage:
$0 <step> [<step>,...]

'step' is one of:
    $allcmds

These steps are expanding to the following steps:
    all
        -> $(expand_steps all)
    all_noreboot
        -> $(expand_steps all_noreboot)
    all_batch
        -> $(expand_steps all_batch)
    all_batch_noreboot
        -> $(expand_steps all_batch_noreboot)
    plain
        -> $(expand_steps plain)
    plain_with_upgrade
        -> $(expand_steps plain_with_upgrade)
    instonly
        -> $(expand_steps instonly)

Steps:
    cleanup:        kill all running VMs, zero out boot sectors of all LVM volumes
    prepare:        create LVM volumes, setup libvirt networks
    setupadmin:     create the admin node and install the cloud product
    prepareinstcrowbar: add repos and install crowbar packages
    bootstrapcrowbar: setup the postgres database and start crowbar
    instcrowbar:    install crowbar and chef on the admin node
    setupnodes:     create the nodes and let crowbar discover them
    instnodes:      allocate and install compute nodes
    onadmin:        run a step on the admin node - use as onadmin+func+param1
    proposal:       create and apply proposals for default setup
    setup_aliases:  set aliases for the nodes (usually needed for batch step)
    batch:          build proposals from exported crowbar_batch YAML file
    setuplonelynodes: boot a number (defined by nodenumberlonelynode) of non-crowbar registered nodes in the admin network
    crowbar_register: register a number (defined by nodenumberlonelynode) of non-crowbar nodes with crowbar (setuplonelynodes needs to have run before)
    testsetup:      start a VM in the cloud
    addupdaterepo:  addupdate repos defined in UPDATEREPOS= (URLs separated by '+')
    runupdate:      run zypper up on the crowbar node
                    (compute nodes are automaticallyupdated via chef run)
    rebootcrowbar:  reboot the crowbar instance and wait for it being up
    rebootcloud:    reboot the cloud nodes and wait for them being up
    restartcloud:   start a pre-existing cloud again after host reboot
    createadminsnapshot: create snapshot of admin node disk
    restoreadminfromsnapshot: restore admin node disk from snapshot
    createcloudsnapshot:
                    create a snapshot of all nodes
    restorecloudfromsnapshot:
                    restore all nodes from snapshot
    devsetup:       installs the crowbar development environment in /opt/crowbar
    qa_test:        run the qa test suite
    help:           usage of steps and environment variables
    steps:          usage of steps only

EOSTEPS
}

function steps() {
    show_steps
    exit 1
}

function usage()
{
    show_steps
    cat <<EOUSAGE


Environment variables (need to be exported):

Mandatory
    cloudvg=vg0
        set the volume group name to create lvm volumes in
        Cloud volumes will be prefixed with the cloud name.
        The cleanup function will only cleanup volumes with this prefix.
    cloudpv=/dev/vdx (default /dev/vdb)
        Device where a LVM physical volume will be created, all data lost on this device
        Should be at least 80 GB. The volume group will be called 'cloud'.
    cloudsource=develcloud4/5/6/7 | susecloud7 | GM4+up | GM5 | GM5+up | GM6 | GM6+up | M?  (default '')
        NOTE: The latest version always is in development. So do NOT expect it to work out of the box.
        NOTE: If you need a stable/working version use <latest-version>-1.
        defines the installation source of the product
        develcloud4   : product from IBS Devel:Cloud:4
        develcloud5   : product from IBS Devel:Cloud:5
        develcloud6   : product from IBS Devel:Cloud:6
        develcloud7   : product from IBS Devel:Cloud:7
        susecloud7    : product from IBS SUSE:SLE....
        GM5           : SUSE Cloud Goldmaster 5 without updates
        GM6           : SUSE Cloud Goldmaster 6 without updates
        GM4+up        : SUSE Cloud Goldmaster 4 with released maintenance updates
        GM5+up        : SUSE Cloud Goldmaster 5 with released maintenance updates
        GM6+up        : SUSE Cloud Goldmaster 6 with released maintenance updates
        M?            : uses official Milestone? ISO image (? is a number)

Optional
    qa_crowbarsetup='path/to/script' (default: same directory as mkcloud is located in)
        set an optional path to qa_crowbarsetup.sh
    hacloud='' | 1  (default='')
        Set up a highly available cloud; requires to configure the clusters via clusterconfig='...'
    clusterconfig
        A string with a cluster configuration. The services for data, network and services cluster can
        be deployed in 1, 2 or 3 clusters. The configuration string looks like this:
        The string is: '<group1>:<group2>' (a ':' separated list of groups).
        A group is:    'clustername1+clustername2=clusternodecount'
        The first clustername of a group defines the name of the cluster.
        Where 'clusternodecount' is the number of nodes in the given cluster.
        Examples:
            3 clusters: clusterconfig='data=2:network=3:services=5'
            2 clusters: clusterconfig='services+data=2:network=3'
            1 cluster:  clusterconfig='data+network+services=2'
    upgrade_cloudsource='' (default='')
        set new cloudsource for upgrade process
    TESTHEAD='' | 1  (default='')
        use Media from Devel:Cloud:Staging (except for GM* targets)
    controller_raid_volumes (default=0)
        The number of disks to join into the software RAID for the controller node.
        Mimimal number to setup RAID is 2.
    cephvolumenumber  (default=1)
        the number of ceph volumes that will be created per node
        note: proposal step does contain a ceph proposal only in SUSE Cloud 4, in
        SUSE OpenStack Cloud 5 do it manually
    cephvolume_hdd_size (default 21)
        Set the size in GB of data disk attached in compute nodes with enabled 'cephvolumenumber'
    controller_ceph_hdd_size (default 25)
        Set the size in GB of data disk attached in controller node  with enabled 'cephvolumenumber'
    nodenumber=2    (default 2)
        set the number of nodes to be created for the cloud (excluding admin node)
        In HA mode (hacloud=1) the nodes needed for clusters are subtracted from nodenumber; the
        remaining nodes are compute nodes.
    nodenumberlonelynode=2    (default 0)
        set the number of non-crowbar registered nodes to be created in the admin network
    nodenumbercompute=1  (default $nodenumber resp. 1 in case of hacloud=1)'
        set the number of compute nodes in a HA setup, the remaining nodes
        of $nodenumber - $nodenumbercompute are used as HA cluster nodes
    vcpus=1         (default $vcpus)
        set the number of CPU cores per compute node
    adminvcpus=1    (default $adminvcpus)
        set the number of CPU cores for admin node
    admin_node_memory (default 4194304)
        Set the memory in KB assigned to admin node
    controller_node_memory (default 5242880)
        Set the memory in KB assigned to controller nodes
    compute_node_memory (default 2097152)
        Set the memory in KB assigned to compute nodes
    drbd_hdd_size  (default 0, or 15 if hacloud is set)
        Set the size in GB of the DRBD data disks attached to the
        nodes in the cluster hosting the database and rabbitmq.
    drbd_database_size (default 5)
        Set the size in GB of the DRBD LV to request the database barclamp
        to set up within the DRBD data disks attached to the nodes in the
        cluster hosting the database.
    drbd_rabbitmq_size (default 5)
        Set the size in GB of the DRBD LV to request the rabbitmq barclamp
        to set up within the DRBD data disks attached to the nodes in the
        cluster hosting RabbitMQ.
    networkingplugin
        Set the networking plugin to be used by neutron (e.g. openvswitch),
        if it isn't defined the barclamp-neutron's default is used.
    networkingmode
        Set the networking mode to be used by neutron (e.g. gre)
        if it isn't defined the barclamp-neutron's default is used.
    keep_existing_hostname=1    (default='')
        If this option is enabled crowbar_register keeps the existing
        hostname. When the crowbar_register option is enabled too then
        it generates and sets random hostnames.
    debug_mkcloud=1  (default 0)
        enable debug mode for mkcloud via 'set -x'
    debug_qa_crowbarsetup=1 (default 0)
        enable debug mode for qa_crowbarsetup.sh via 'set -x'
    debug_openstack=1 (default 0)
        enable debug mode for the openstack components
        sets debug true in the openstack proposals
    user_keyfile='path/to/file'
        path to optional user public ssh keyfile to inject into crowbar to
        /root/.ssh/authorized_keys file and to provisioner barclamp
    want_sles12=1 (default 0)
        setup SLE12 compute nodes
    want_sles12sp1_admin=1 (default 0)
        setup admin server with SLE12SP1 (default is SP2)
    want_dvr=1 (default='')
        if DVR support should be turned on by neutron barclamp. Only works with openvswitch and vxlan.
    want_docker=1 (default='')
        Deploy docker image (upload image to glance, create instance based on it etc.).
        Only possible with SLE12 node.
    want_rootfs=btrfs (default='')
        Deploy all discovered/pxe booted nodes with BTRFS as root filesystem. if empty,
        an intrinsic default is chosen. Only possible with SLE12 node.
    want_all_debug=1 (default='')
        Enable debug level logging in barclamp proposals for all proposals.
    want_magnum=1 (default='')
        Increase RAM and HDD size on compute node for container image deployment by magnum.
    want_barbican=0 (default=1)
        Deployed by default. Set to 0 to prevent barbican from being deployed.
    want_sahara=0 (default=0)
        Set to 1 to deploy sahara.
    want_murano=0 (default=0)
            Set to 1 to deploy murano.
    want_postgresql=0 (default=1)
        Use PostgreSQL instead of SQLite as a Crowbar database
    install_chef_suse_override='path/to/script'
        Optional path to an alternate version of install-chef-suse.sh on the mkcloud hostto use
        instead of the one from the packages.  This will be scp'd to the admin node before use.
    vlan_public=<id> (default 300)
        VLAN id for public network
    tempestoptions (default='-t -s')
        parameters passed to run_tempest.sh script
    ostestroptions (default='')
        If set, ostestr is installed and executed in the testsetup step.
        This is useful when tempest just executes smoke tests but you want to
        run extra tests on top of that (without running the full testsuite)
        Example: export ostestroptions=" --regex '^manila_tempest_tests.tests.api'"
    rally_server (default='')
        rally server address
    cct_tests='test1+test2' (default='features')
        Defines which cct_tests should be run while the cct step.
    scenario='scenario.yaml' (default='')
        Defines which scenario file should be used. Only works with cloud5 or newer.
        Currently only the step 'batch' uses such a file.
    scenario_dir='/tmp' (default=${mkcloud_dir}/scenarios/cloud\$(getcloudver)/)
        Full path to directory containing scenario file.
EOUSAGE
    onadmin_help
    exit 1

# UNDOCUMENTED OPTIONS:
#
# virtualcloud
# cloudfqdn
# forwardmode
# net_public
# net_admin
# adminnetmask
# adminip
# admingw
# cpuflags
# admin_node_memory
# controller_node_memory
# compute_node_memory
# hyperv_node_memory
# adminnode_hdd_size
# controller_hdd_size
# computenode_hdd_size
# cloudbr
# libvirt_type
# localreposdir_src
# localreposdir_target
# wipe

}


function addupdaterepo()
{
    sshrun "UPDATEREPOS=$UPDATEREPOS onadmin_addupdaterepo"
    return $?
}

function runupdate()
{
    onadmin runupdate
    return $?
}

function is_concurrent_run()
{
    [ -e $pidfile ] && kill -0 `cat $pidfile` 2>/dev/null && return 0
    echo $$ > $pidfile
    return 1
}

function sanity_checks()
{
    if test `id -u` != 0 ; then
        complain 1 "This script needs to be run as root" \
            "Please be aware that this script will create a LVM" \
            "and kill all current VMs on this host."
    fi

    if is_concurrent_run; then
        complain 33 "mkcloud was started twice from same working directory: `pwd`" \
            "Please always use a separate working directory for each (parallel) mkcloud run."
    fi

    # test for existence of qa_crowbarsetup.sh
    if [ ! -e $qa_crowbarsetup ] ; then
        complain 87 "$qa_crowbarsetup not found
            Please define the path to it by setting qa_crowbarsetup=/path/to/file
            or call mkcloud from your git clone."
    fi

    if [ -z "$cloudsource" ] ; then
        echo "Please set the env variable:"
        echo "export cloudsource=M?|develcloud4|develcloud5|develcloud6|develcloud7|susecloud7|GM4+up|GM5|GM5+up|GM6|GM6+up"
        exit 1
    fi

    # checking clusterconfig
    if [[ $hacloud && ! $clusterconfig ]] ; then
        echo "Examples for clusterconfig:"
        echo '3 clusters: clusterconfig="data=2:network=3:services=3"'
        echo '2 clusters: clusterconfig="services+data=2:network=3"'
        echo '1 cluster:  clusterconfig="data+network+services=2"'
        complain 70 "No cluster config provided for HA setup."
    fi

    vgdisplay "$cloudvg" >/dev/null 2>&1 && needcvol=
    if [ -n "$needcvol" ] ; then
        : ${cloudpv:=/dev/vdb}
        if grep -q $cloudpv /proc/mounts ; then
            complain 92 "The device $cloudpv seems to be used. Exiting."
        fi
        if [ ! -e $cloudpv ] ; then
            complain 93 "$cloudpv does not exist." \
                "Please set the cloud volume group to an existing device: export cloudpv=/dev/sdx" \
                "Running 'partprobe' may help to let the device appear."
        fi
    fi

    if [ -e /etc/init.d/SuSEfirewall2_init ] && rcSuSEfirewall2 status ; then
        complain 91 "SuSEfirewall is running - it will interfere with the iptables rules done by libvirt" \
            "Please stop the SuSEfirewall completely and run mkcloud again" \
            "Run:  rcSuSEfirewall2 stop && insserv -r SuSEfirewall2_setup && insserv -r SuSEfirewall2_init"
    fi

    if grep "devpts.*[^x]mode=.00" /proc/mounts ; then
        complain 13 "/dev/pts is not accessible for libvirt, maybe you use autobuild on your system." \
            "Please remount it using the following command:" \
            " # mount -o remount,mode=620,gid=5 devpts -t devpts /dev/pts"
    fi

    if [[ $wantedcmds =~ "setuplonelynodes" || $wantedcmds =~ "crowbar_register" ]] ; then
        if [[ -z "$nodenumberlonelynode" || $nodenumberlonelynode -lt 1 ]] ; then
            complain 80 "Please set nodenumberlonelynode."
        fi
    fi

    case $cinder_backend in
        ''|netapp|local|raw|rbd)
            ;;
        *)
            complain 101 "$cinder_backend as the cinder backend is currently not supported."
            ;;
    esac

    if [[ $want_sles12 = 1 ]] && [[ $want_ceph = 1 ]] && [[ $nodenumber -lt 3 ]] ; then
        complain 113 "Please increase number of nodes for this setup, minimal nodenumber=3"
    fi

    if iscloudver 6plus && [[ $want_ceph = 1 ]] && [[ $nodenumber -lt 3 ]] ; then
        complain 113 "Please increase number of nodes for this setup, minimal nodenumber=3"
    fi

    if [[ " ${steplist[*]} " == *" batch "* ]] ; then
        : ${scenario_dir:="${mkcloud_dir}/scenarios/cloud$(getcloudver)"}
        need_scenario=1
        if [[ ! $scenario ]] ; then
            complain 114 "scenario parameter for batch step must point to a file in $scenario_dir"
        fi
        scenario_path="$scenario_dir/$scenario"
        if [[ ! -f $scenario_path ]]; then
            complain 115 "Scenario file not found at $scenario_path"
        fi
    fi
}


## MAIN ##
step_aliases="_new_admin _compute _upgrade _testupdate"

allcmds="$step_aliases all all_noreboot all_batch all_batch_noreboot instonly \
    plain plain_with_upgrade cleanup setuphost prepare setupadmin \
    prepareinstcrowbar bootstrapcrowbar instcrowbar instcrowbarfromgit setupnodes \
    setupcompute instnodes instcompute proposal testsetup rebootcrowbar \
    rebootcloud addupdaterepo runupdate testupdate \
    crowbarbackup crowbarrestore shutdowncloud restartcloud qa_test help \
    rebootneutron cloudupgrade \
    setuplonelynodes crowbar_register createadminsnapshot \
    restoreadminfromsnapshot createcloudsnapshot restorecloudfromsnapshot \
    cct steps batch setup_aliases onadmin onhost devsetup"
wantedcmds=$@

function expand_steps()
{
    # parse the commands and expand the aliases
    local runcmds=''
    local localwantedcmds=$@
    local cmd
    for cmd in $localwantedcmds ; do

        local found=0
        local onecmd
        for onecmd in $allcmds ; do
            if [[ $cmd =~ ^$onecmd(\+.+)?$ ]] ; then
                found=1
                case "$cmd" in
                    setupcompute|instcompute)
                        corrected=${cmd/compute/nodes}
                        echo "WARNING: the '$cmd' step is deprecated; use '$corrected' instead." >&2
                        cmd=$corrected
                        ;;
                esac

                case "$cmd" in
                    _new_admin)
                        runcmds="$runcmds cleanup prepare setupadmin addupdaterepo runupdate prepareinstcrowbar bootstrapcrowbar instcrowbar"
                    ;;
                    _compute)
                        runcmds="$runcmds setupnodes instnodes setup_aliases proposal"
                    ;;
                    all)
                        runcmds="$runcmds `expand_steps _new_admin` rebootcrowbar `expand_steps _compute` testsetup cct rebootcloud"
                    ;;
                    all_noreboot)
                        runcmds="$runcmds `expand_steps _new_admin` `expand_steps _compute` testsetup cct"
                    ;;
                    all_batch)
                        runcmds="$runcmds `expand_steps _new_admin` rebootcrowbar setupnodes instnodes setup_aliases batch testsetup cct rebootcloud"
                    ;;
                    all_batch_noreboot)
                        runcmds="$runcmds `expand_steps _new_admin` setupnodes instnodes setup_aliases batch testsetup cct"
                    ;;
                    _testupdate|testupdate)
                        runcmds="$runcmds addupdaterepo runupdate testsetup cct"
                    ;;
                    plain)
                        runcmds="$runcmds `expand_steps instonly` proposal"
                    ;;
                    instonly)
                        runcmds="$runcmds cleanup prepare setupadmin prepareinstcrowbar bootstrapcrowbar instcrowbar setupnodes instnodes setup_aliases"
                    ;;
                    _upgrade)
                        runcmds="$runcmds cloudupgrade"
                    ;;
                    plain_with_upgrade)
                        runcmds="$runcmds `expand_steps plain` addupdaterepo runupdate `expand_steps _upgrade`"
                    ;;
                    *)
                        runcmds="$runcmds $cmd"
                    ;;
                esac
            fi
        done
        [ $found == 0 ] && complain - "Step $cmd not found." && return

    done
    runcmds=${runcmds## }
    runcmds=${runcmds%% }
    echo "${runcmds//  / }"
}

steplist=`expand_steps $wantedcmds`
[[ ! $steplist ]] || [[ "$steplist" =~ "help" ]] && usage
[[ "$steplist" =~ "steps" ]] && steps

sanity_checks

determine_host_mtu

echo "You choose to run these mkcloud steps:"
echo "  $steplist"
echo
sleep 2

for cmd in `echo $steplist` ; do
    echo
    echo "============> MKCLOUD STEP START: $cmd <============"
    echo
    sleep 2
    echo $cmd >> mkcloud.steps.log
    # support calls to functions with parameters
    # this is currently used for the cct function
    cmd_parameters="${cmd#*+}"
    cmd=${cmd%%+*}
    $cmd "${cmd_parameters//+/ }"
    ret=$?
    if [ $ret != 0 ] ; then
        set +x
        echo
        echo '$h1!!'
        echo "Error detected. Stopping mkcloud."
        echo "The step '$cmd' returned with exit code $ret"
        echo "Please refer to the $cmd function in this script when debugging the issue."
        error_exit $ret ""
    fi >&2
    echo
    echo "^^^^^^^^^^^^= MKCLOUD STEP DONE: $cmd =^^^^^^^^^^^^"
    echo
done

pre_exit_cleanup
show_environment
