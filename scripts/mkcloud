#!/bin/bash
#
# mkcloud - Setup a virtual cloud on one system (physical or even virtual)
#
# Authors: J. Daniel Schmidt <jdsn@suse.de>
#          Bernhard M. Wiedemann <bwiedemann@suse.de>
#
# 2012, SUSE LINUX Products GmbH
#

# Quick introduction:
#
# 1. read the usage: mkcloud help
# 2. This tool relies on the script qa_crowbarsetup.sh
# 3. Please 'export' environment variables according to your needs.
# 4. read the usage: mkcloud help


if [[ $debug_mkcloud = 1 ]] ; then
    set -x
fi

if [ -n "$CVOL" ] ; then
    cloudpv=$CVOL
    unset CVOL
    echo "------------------------------------------------------------------------"
    echo "The CVOL variable is deprecated."
    echo "Please either set 'cloudvg' or 'cloudpv'"
    echo " cloudvg will use an existing lvm and not destroy other volumes"
    echo " cloudpv will use an lvm pv device exclusively and destroy all volumes"
    echo
    echo "Continuing in 20 seconds with fallback (cloudpv=\$CVOL):"
    echo " cloudpv=$CVOL"
    echo "Otherwise press Ctrl-C now!"
    echo "------------------------------------------------------------------------"
    sleep 20
fi

# FIXME: separate user-tweakable parameters from script local variables.
# Currently there is no clearly defined interface point.  One of the
# causes of this is violation of the common shell coding standard which
# uses uppercase for environment variables and constants, and lowercase
# for local variables.
: ${qa_crowbarsetup:=$(dirname $0)/qa_crowbarsetup.sh}
: ${virtualcloud:=virtual}
: ${cloudfqdn:=$virtualcloud.cloud.suse.de}
: ${forwardmode:=nat}
: ${net_fixed:=192.168.123}
: ${net_public:=192.168.122}
: ${net_admin:=192.168.124}
: ${public_vlan:=300}
: ${adminnetmask:=255.255.248.0}
: ${adminip:=$net_admin.10}
: ${admingw:=$net_admin.1}
# the default nodenumber of compute nodes
nodenumbercomputedefault=2
[ -n "$hacloud" ] && nodenumbercomputedefault=1
: ${nodenumber:=$nodenumbercomputedefault}
: ${nodenumbercompute:=$nodenumbercomputedefault}
# configuration of clusters
: ${clusterconfig:=''}
: ${nodenumberlonelynode:=0}
export nodenumber nodenumbercompute nodenumberlonelynode clusterconfig
# '+'-separated list of MAC#serial_of_drbd_volume of the drbd cluster nodes
# (used only internally to transport this information to qa_crowbarsetup):
: ${drbdnode_mac_vol:=''}
: ${cephvolumenumber:=0}
allnodeids=`seq 1 $(( nodenumber + nodenumberlonelynode ))`
allnodeids_without_lonely=`seq 1 $nodenumber`
lonelynodeids=`seq $(( nodenumber + 1 )) $(( nodenumber + nodenumberlonelynode ))`
: ${vcpus:=1}
: ${adminvcpus:=$vcpus}
cpuflags=''
working_dir_orig=`pwd`
: ${artifacts_dir:=$working_dir_orig/.artifacts}
start_time=`date`
: ${cloud:=cloud}
: ${cloudvg:=$cloud}
needcvol=1
: ${vdisk_dir:=/dev/$cloudvg}
: ${admin_node_disk:=$vdisk_dir/$cloud.admin}
: ${admin_node_memory:=2097152}
: ${controller_node_memory:=4194304}
: ${compute_node_memory:=2097152}
: ${hyperv_node_memory:=3000000}
[[ "$libvirt_type" = "hyperv" && $compute_node_memory -lt $hyperv_node_memory ]] && compute_node_memory=$hyperv_node_memory
# hdd size defaults (unless defined otherwise)
: ${adminnode_hdd_size:=15}
: ${controller_hdd_size:=20}
: ${computenode_hdd_size:=15}
: ${cephvolume_hdd_size:=21}
: ${controller_ceph_hdd_size:=25}
if [ -n "$hacloud" ]; then
    : ${drbd_hdd_size:=50}
else
    : ${drbd_hdd_size:=0}
fi
# pvlist is filled below
pvlist=
next_pv_device=
pv_cur_device_no=0
: ${cloudbr:=${cloud}br}
mnt=/tmp/cloudmnt/$$
sshopts="-oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null"
scp="scp $sshopts"
#if localreposdir_src string is available, the local repositories are used for setup
: ${localreposdir_src:=""}
#localreposdir_target is the 9p target dir and also the mount target dir in the VM
: ${localreposdir_target:="/repositories"}
[ -z "$localreposdir_src" ] && localreposdir_target=""
: ${install_chef_suse_override:=./install-chef-suse.sh}

emulator=/usr/bin/qemu-system-$(uname -m)
if [ -x /usr/bin/qemu-kvm ] && file /usr/bin/qemu-kvm | grep -q ELF; then
    # on SLE11, qemu-kvm is preferred, since qemu-system-x86_64 is
    # some rotten old stuff without KVM support
    emulator=/usr/bin/qemu-kvm
fi

pidfile=mkcloud.pid

trap 'error_exit $? "error caught by trap"' TERM
exec </dev/null

function is_suse()
{
    grep -qi suse /etc/*release
}

function complain() # {{{
{
    local ex=$1; shift
    printf "Error: %s\n" "$@" >&2
    [[ $ex != - ]] && exit $ex
} # }}}

safely () {
    if ! "$@"; then
        complain 30 "$* failed! Aborting."
    fi
}

function show_environment()
{
    end_time=`date`
    echo "Environment Details"
    echo "-------------------------------"
    echo "    hostname: `hostname -f`"
    echo "     started: $start_time"
    echo "       ended: $end_time"
    echo "-------------------------------"
    echo " cloudsource: $cloudsource"
    echo "    TESTHEAD: $TESTHEAD"
    echo "  nodenumber: $nodenumber"
    echo "     cloudpv: $cloudpv"
    echo " UPDATEREPOS: $UPDATEREPOS"
    echo "    cephvolumenumber: $cephvolumenumber"
    echo " upgrade_cloudsource: $upgrade_cloudsource"
    echo "-------------------------------"
    env | grep -i "_with_ssl"
    echo "-------------------------------"
}

function pre_exit_cleanup()
{
    rm $pidfile
}

function error_exit()
{
    exitcode=$1
    message=$2
    if [ -z "$SKIPSUPPORTCONFIG" ] ; then
        ssh $sshopts root@$adminip '
            set -x
            for node in $(crowbar machines list | grep ^d) ; do
                echo "Collecting supportconfig from $node"
                timeout 300 ssh $node supportconfig | wc
                timeout 300 scp $node:/var/log/\*tbz /var/log/
            done
            timeout 300 supportconfig | wc
        '
        mkdir -p $artifacts_dir
        $scp root@$adminip:/var/log/*tbz $artifacts_dir/
    fi
    pre_exit_cleanup
    echo $message
    show_environment
    exit $exitcode
} >&2

function wait_for()
{
    timecount=${1:-300}
    timesleep=${2:-1}
    condition=${3:-'/bin/true'}
    waitfor=${4:-'unknown process'}

    echo "Waiting for: $waitfor"
    n=$timecount
    while test $n -gt 0 && ! eval $condition
    do
        echo -n .
        sleep $timesleep
        n=$(($n - 1))
    done
    echo

    if [ $n = 0 ] ; then
        complain 11 "Waiting for '$waitfor' timed out." \
            "This check was used: $condition"
    fi
}

function wait_for_if_running()
{
    local procname=${1}
    local timecount=${2:-300}

    wait_for $timecount 5 "! pidofproc ${procname} >/dev/null" "process '${procname}' to terminate"
}

function sshrun()
{
    mkcconf=mkcloud.config
    cat > $mkcconf <<EOF
        export drbdnode_mac_vol=$drbdnode_mac_vol ;
        export cloud=$virtualcloud ;
        export cloudfqdn=$cloudfqdn ;
        export cloudsource=$cloudsource ;
        export upgrade_cloudsource=$upgrade_cloudsource ;
        export adminip=$adminip ;
        export hacloud=$hacloud ;
        export libvirt_type=$libvirt_type ;
        export networkingplugin=$networkingplugin ;
        export networkingmode=$networkingmode ;
        export nosetestparameters=${nosetestparameters} ;
        export tempestoptions='${tempestoptions}' ;
        export cephvolumenumber=$cephvolumenumber ;
        export shell=$shell ;
        export keep_existing_hostname=$keep_existing_hostname ;

        export all_with_ssl=$all_with_ssl ;
        export glance_with_ssl=$glance_with_ssl ;
        export keystone_with_ssl=$keystone_with_ssl ;
        export nova_with_ssl=$nova_with_ssl ;
        export nova_shared_instance_storage=$nova_shared_instance_storage ;
        export novadashboard_with_ssl=$novadashboard_with_ssl ;

        export localreposdir_target=$localreposdir_target ;

        export cinder_conf_volume_type=$cinder_conf_volume_type;
        export cinder_conf_volume_params=$cinder_conf_volume_params;

        export TESTHEAD=$TESTHEAD ;
        export NOINSTALLCLOUDPATTERN=$NOINSTALLCLOUDPATTERN ;
EOF
    env|grep -e ^debug_ -e ^pre_ -e ^want_ -e ^net_ -e ^nodenumber -e ^clusterconfig | sort >> $mkcconf

    $scp $qa_crowbarsetup $mkcconf root@$adminip:
    ssh $sshopts root@$adminip "echo `hostname` > cloud ; . qa_crowbarsetup.sh ; $@"
    return $?
}

function onadmin()
{
    sshrun onadmin_$1
}

function cleanup()
{
    # cleanup leftover from last run
    allnodenames=$(seq --format="node%.0f" 1 $(($nodenumber + 20)))
    for name in admin $allnodenames ; do
        local vm=$cloud-$name
        if LANG=C virsh domstate $vm 2>/dev/null | grep -q running ; then
            safely virsh destroy $vm
        fi
        if virsh domid $vm >/dev/null 2>&1; then
            safely virsh undefine $vm
        fi
        local machine=qemu-$vm
        if test -x /usr/bin/machinectl && machinectl status $machine 2>/dev/null ; then
            safely machinectl terminate $machine # workaround bnc#916518
        fi
    done

    local net=$cloud-admin
    if virsh net-uuid $net >/dev/null 2>&1; then
        virsh net-destroy $net
        safely virsh net-undefine $net
    fi

    if ip link show ${cloudbr}.$public_vlan >/dev/null 2>&1; then
        ip link set ${cloudbr}.$public_vlan down
    fi
    if ip link show ${cloudbr} >/dev/null 2>&1; then
        ip link set ${cloudbr} down
        ip link delete ${cloudbr} type bridge
        ip link delete ${cloudbr}-nic
    fi

    # 1. remove all loop devices that are still open for this cloud
    local vol
    dmsetup ls | awk "/^$cloudvg-${cloud}\./ {print \$1}" | while read vol ; do
        local link=`readlink /dev/mapper/$vol`
        local dev=`cd /dev/mapper/ ; readlink -f $link`
        local lodev=`losetup --associated $dev | cut -d":" -f1`
        [ -z "$lodev" ] && continue

        if mount | grep "$lodev " ; then
            umount $lodev || complain 41 "Error: Can not umount $lodev to cleanup $dev ($vol)."
        fi
        losetup -d $lodev || complain 41 "Error: Can not remove loopdevice $lodev to cleanup $dev ($vol)."
    done

    # 2. remove all previous volumes for that cloud; this helps preventing
    # accidental booting and freeing space
    if [ -d $vdisk_dir ]; then
        find -L $vdisk_dir -name "$cloud.*" -type b | \
            xargs -r lvremove --force
    fi

    rm -f /var/run/libvirt/qemu/$cloud-*.xml /var/lib/libvirt/network/$cloud-*.xml \
        /etc/sysconfig/network/ifcfg-$cloudbr.$public_vlan
    if [ -n "$wipe" ] ; then
        vgchange -an $cloudvg
        dd if=/dev/zero of=$cloudpv count=1000
    fi
    return 0
}

function onhost_get_next_pv_device()
{
    if [ -z "$pvlist" ] ; then
        pvlist=`pvs --sort -Free | awk '$2~/'$cloudvg'/{print $1}'`
        pv_cur_device_no=0
    fi
    next_pv_device=`perl -e '$i=shift; $i=$i % @ARGV;  print $ARGV[$i]' $pv_cur_device_no $pvlist`
    pv_cur_device_no=$(( $pv_cur_device_no + 1 ))
}

# spread block devices over a LVM's PVs so that different VMs
# are likely to use different PVs to optimize concurrent IO throughput
function onhost_create_cloud_lvm()
{
    if [ -n "$needcvol" ] ; then
        safely pvcreate "$cloudpv"
        safely vgcreate "$cloudvg" "$cloudpv"
    fi
    safely vgchange -ay $cloudvg # for later boots

    local hdd_size

    lvrename $vdisk_dir/admin $admin_node_disk # transition until 2015
    onhost_get_next_pv_device
    safely lvcreate -n $cloud.admin -L ${adminnode_hdd_size}G $cloudvg $next_pv_device
    for i in $allnodeids ; do
        lvrename $vdisk_dir/node$i $vdisk_dir/$cloud.node$i # transition until 2015
        onhost_get_next_pv_device
        hdd_size=${computenode_hdd_size}
        test "$i" = "1" && hdd_size=${controller_hdd_size}
        safely lvcreate -n $cloud.node$i -L ${hdd_size}G $cloudvg $next_pv_device
    done

    if [ $cephvolumenumber -gt 0 ] ; then
        for i in $allnodeids ; do
            for n in $(seq 1 $cephvolumenumber) ; do
                lvrename $vdisk_dir/node$i-ceph$n $vdisk_dir/$cloud.node$i-ceph$n # transition until 2015
                onhost_get_next_pv_device
                hdd_size=${cephvolume_hdd_size}
                test "$i" = "1" -a "$n" = "1" && hdd_size=${controller_ceph_hdd_size}
                safely lvcreate -n $cloud.node$i-ceph$n -L ${hdd_size}G $cloudvg $next_pv_device
            done
        done
    fi

    # create volumes for drbd
    if [ $drbd_hdd_size != 0 ] ; then
        for i in `seq 1 2`; do
            onhost_get_next_pv_device
            safely lvcreate -n $cloud.node$i-drbd -L ${drbd_hdd_size=}G $cloudvg $next_pv_device
        done
    fi
}

function onhost_deploy_image()
{
    local image
    local offset
    local role=$1
    local dist=${2:-SLE11}
    local disk=${3:-$admin_node_disk}

    case $dist in
    SLE12)
        image=SLES12.qcow2
        offset=2039808
        ;;
    SLE11)
        image=SP3-64up.qcow2
        offset=2056192
        ;;
    *)
        complain 71 "Error: onhost_deploy_image has not enough information about how to deploy. (distribution: $dist)"
        ;;
    esac

    pushd /tmp
    safely wget --progress=dot:mega -nc \
        http://clouddata.cloud.suse.de/images/$image

    echo "Cloning $role node vdisk from $image ..."
    safely qemu-img convert -p $image $disk
    popd

    if fdisk -l $disk | grep -q $offset ; then
        # make a bigger partition 2
        echo -e "d\n2\nn\np\n2\n\n\na\n2\nw" | fdisk $disk
        loopdevice=`losetup --find`
        losetup -o $(( $offset * 512 )) $loopdevice $disk
        safely fsck -y -f $loopdevice
        safely resize2fs $loopdevice
        losetup -d $loopdevice
    fi
}

function onhost_add_etchosts_entries()
{
    grep -q crowbar /etc/hosts || echo "$adminip crowbar.$cloudfqdn crowbar" >> /etc/hosts
}

function onhost_enable_ksm
{
    # enable kernel-samepage-merging to save RAM
    echo 1 > /sys/kernel/mm/ksm/run
    echo 1000 > /sys/kernel/mm/ksm/pages_to_scan
}

function setuphost()
{
    if is_suse ; then
        export ZYPP_LOCK_TIMEOUT=60
        zypper --non-interactive in --no-recommends \
            libvirt kvm lvm2 curl wget bridge-utils \
            dnsmasq netcat-openbsd ebtables
        [ "$?" == 0 -o "$?" == 4 ] || exit 10
    fi
}

function prepare()
{
    if ! [ -e ~/.ssh/id_dsa ]; then
        echo "Creating key for controlling our VMs..."
        ssh-keygen -t dsa -f ~/.ssh/id_dsa -N ""
    fi

    onhost_enable_ksm
    onhost_create_cloud_lvm
    onhost_deploy_image "admin" "SLE11" "$admin_node_disk"
    onhost_add_etchosts_entries
}

function ssh_password()
{
    SSH_ASKPASS=/root/echolinux
    cat > $SSH_ASKPASS <<EOSSHASK
#!/bin/sh
echo linux
EOSSHASK
    chmod +x $SSH_ASKPASS
    DISPLAY=dummydisplay:0 SSH_ASKPASS=$SSH_ASKPASS setsid ssh $sshopts -oNumberOfPasswordPrompts=1 "$@"
}

function onhost_local_repository_mount()
{
    #add xml snippet to be able to mount a local dir via 9p in a VM
    if [ -n "${localreposdir_src}" ]; then
        local_repository_mount="<filesystem type='mount' accessmode='squash'>
            <source dir='$localreposdir_src'/>
            <target dir='$localreposdir_target'/>
            <readonly/>
            <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
            </filesystem>"
    else
        local_repository_mount=""
    fi
}

function onhost_cpuflags_settings()
{ # used for admin and compute nodes
    cpuflags="<cpu match='minimum'>
            <model>qemu64</model>
            <feature policy='require' name='fxsr_opt'/>
            <feature policy='require' name='mmxext'/>
            <feature policy='require' name='lahf_lm'/>
            <feature policy='require' name='sse4a'/>
            <feature policy='require' name='abm'/>
            <feature policy='require' name='cr8legacy'/>
            <feature policy='require' name='misalignsse'/>
            <feature policy='require' name='popcnt'/>
            <feature policy='require' name='pdpe1gb'/>
            <feature policy='require' name='cx16'/>
            <feature policy='require' name='3dnowprefetch'/>
            <feature policy='require' name='cmp_legacy'/>
            <feature policy='require' name='monitor'/>
        </cpu>"
    grep -q "flags.* npt" /proc/cpuinfo || cpuflags=""

    if grep -q "vendor_id.*GenuineIntel" /proc/cpuinfo; then
        cpuflags="<cpu mode='custom' match='exact'>
            <model fallback='allow'>core2duo</model>
            <feature policy='require' name='vmx'/>
        </cpu>"
    fi
}


function onhost_create_libvirt_adminnode_config()
{
    onhost_cpuflags_settings
    onhost_local_repository_mount

    cat > $1 <<EOLIBVIRT
  <domain type='kvm'>
    <name>$cloud-admin</name>
    <memory>$admin_node_memory</memory>
    <currentMemory>$admin_node_memory</currentMemory>
    <vcpu>$adminvcpus</vcpu>
    <os>
      <type arch='x86_64' machine='pc-0.14'>hvm</type>
      <boot dev='hd'/>
    </os>
    <features>
      <acpi/>
      <apic/>
      <pae/>
    </features>
    $cpuflags
    <clock offset='utc'/>
    <on_poweroff>preserve</on_poweroff>
    <on_reboot>restart</on_reboot>
    <on_crash>restart</on_crash>
    <devices>
      <emulator>$emulator</emulator>
      <disk type='block' device='disk'>
        <driver name='qemu' type='raw' cache='unsafe'/>
        <source dev='$admin_node_disk'/>
        <target dev='vda' bus='virtio'/>
        <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
      </disk>
      <interface type='network'>
        <mac address='52:54:00:77:77:70'/>
        <source network='$cloud-admin'/>
        <model type='virtio'/>
        <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
      </interface>
      <serial type='pty'>
        <target port='0'/>
      </serial>
      <console type='pty'>
        <target type='serial' port='0'/>
      </console>
      <input type='mouse' bus='ps2'/>
      <graphics type='vnc' port='-1' autoport='yes'/>
      <video>
        <model type='cirrus' vram='9216' heads='1'/>
        <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
      </video>
      <memballoon model='virtio'>
        <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
      </memballoon>
      $local_repository_mount
    </devices>
  </domain>
EOLIBVIRT
}

function onhost_create_libvirt_admin_network_config()
{
    # dont specify range
    # this allows to use the same network for cloud-nodes that get DHCP from crowbar
    # doc: http://libvirt.org/formatnetwork.html
    cat > $1 <<EOLIBVIRTNET
  <network>
    <name>$cloud-admin</name>
    <bridge name='$cloudbr' stp='off' delay='0' />
    <mac address='52:54:00:AB:B1:77'/>
    <ip address='$admingw' netmask='$adminnetmask'>
      <dhcp>
        <host mac="52:54:00:77:77:70" name="crowbar.$cloudfqdn" ip="$adminip"/>
      </dhcp>
    </ip>
    <forward mode='$forwardmode'>
    </forward>
  </network>
EOLIBVIRTNET
}


# bring up the VM for crowbar
function setupadmin()
{
    echo "Injecting public key into image..."
    keyfile=~/.ssh/id_dsa.pub
    pubkey=`cut -d" " -f2 $keyfile`
    mkdir -p $mnt
    safely mount -o loop,offset=$(expr 2056192 \* 512) $admin_node_disk $mnt
    mkdir -p $mnt/root/.ssh
    if ! grep -q "$pubkey" $mnt/root/.ssh/authorized_keys 2>/dev/null; then
        cat $keyfile >> $mnt/root/.ssh/authorized_keys
    fi
    if [[ -n $user_keyfile ]]; then
        cat $user_keyfile >> $mnt/root/.ssh/authorized_keys
    fi
    safely umount $mnt

    onhost_create_libvirt_adminnode_config /tmp/$cloud-admin.xml

    onhost_create_libvirt_admin_network_config /tmp/$cloud-admin.net.xml

    modprobe kvm-amd
    if [ ! -e /etc/modprobe.d/80-kvm-intel.conf ] ; then
        echo "options kvm-intel nested=1" > /etc/modprobe.d/80-kvm-intel.conf
        rmmod kvm-intel
    fi
    modprobe kvm-intel

    if configure_libvirtd; then # config was changed
        service libvirtd stop
    fi
    safely service libvirtd start
    wait_for 300 1 '[ -S /var/run/libvirt/libvirt-sock ]' 'libvirt startup'

    if ! virsh net-dumpxml $cloud-admin > /dev/null 2>&1; then
        virsh net-define /tmp/$cloud-admin.net.xml
    fi
    virsh net-start $cloud-admin
    if ! virsh define /tmp/$cloud-admin.xml ; then
        echo "=====================================================>>"
        complain 76 "Could not define VM for: $cloud-admin"
    fi
    if ! virsh start $cloud-admin ; then
        echo "=====================================================>>"
        complain 76 "Could not start VM for: $cloud-admin"
    fi

    wait_for 300 1 "ping -q -c 1 -w 1 $adminip >/dev/null" 'crowbar admin VM'

    if [ -z "$NOSETUPPORTFORWARDING" ] ; then
        nodehostips=$(seq -s ' ' 81 $((80 + $nodenumber)))
        cat > /etc/init.d/boot.mkcloud <<EOS
#!/bin/bash

iptables -t nat -F PREROUTING
for i in 22 80 443 3000 4000 4040 ; do
    iptables -I FORWARD -p tcp --dport \$i -j ACCEPT
    for host in 10 $nodehostips ; do
        offset=80
        [ "\$host" = 10 ] && offset=10
        iptables -t nat -I PREROUTING -p tcp --dport \$((\$i + \$host - \$offset + 1100)) -j DNAT --to-destination $net_admin.\$host:\$i
    done
done
iptables -t nat -I PREROUTING -p tcp --dport 6080 -j DNAT --to-destination $net_public.2
echo 0 > /proc/sys/net/ipv4/conf/all/rp_filter
EOS
        if ! grep -q "boot.mkcloud" /etc/init.d/boot.local ; then
            cat >> /etc/init.d/boot.local <<EOS

# --v--v--  Automatically added by mkcloud on `date`
/etc/init.d/boot.mkcloud
# --^--^--  End of automatically added section from mkcloud
EOS
        fi
    fi
    chmod +x /etc/init.d/boot.mkcloud
    /etc/init.d/boot.mkcloud

    wait_for 150 1 "nc -z $adminip 22" 'starting ssh daemon'

    echo "waiting some more for sshd+named to start"
    sleep 25
    ssh_password $adminip "
        mkdir -p ~/.ssh;
        grep -q $pubkey ~/.ssh/authorized_keys 2>/dev/null ||
            echo ssh-dss $pubkey injected-key-from-host >> ~/.ssh/authorized_keys
    "
    echo "you can now proceed with installing crowbar"
}

# Returns success if a change was made
function confset()
{
    file="$1" key="$2" value="$3"
    if grep -q "^$key *= *$value" "$file"; then
        return 1 # already set correctly
    fi

    new_line="$key = $value"
    if grep -q "^$key[ =]" "$file"; then
        # change existing value
        sed -i "s/^$key *=.*/$new_line/" "$file"
    elif grep -q "^# *$key[ =]" "$file"; then
        # uncomment existing setting
        sed -i "s/^# *$key *=.*/$new_line/" "$file"
    else
        # add new setting
        echo "$new_line" >> "$file"
    fi

    return 0
}

# Returns success if the config was changed
function configure_libvirtd()
{
    chkconfig libvirtd on

    changed=

    # needed for HA/STONITH via libvirtd:
    confset /etc/libvirt/libvirtd.conf listen_tcp 1            && changed=y
    confset /etc/libvirt/libvirtd.conf listen_addr '"0.0.0.0"' && changed=y
    confset /etc/libvirt/libvirtd.conf auth_tcp '"none"'       && changed=y

    [ -n "$changed" ]
}

# bring up lonely_node(s) in the admin network
function setuplonelynodes()
{
    local i
    for i in $lonelynodeids; do
        local mac
        local i2=$( printf %02x $i )
        mac="52:54:$i2:88:77:$i2"
        local lonely_node
        lonely_node=$cloud-node$i
        onhost_create_libvirt_computenode_config /tmp/$lonely_node.xml $i $mac
        # change the boot order to not PXE boot the lonely node
        sed -i "s/boot order='1'/boot order='3'/g" /tmp/$lonely_node.xml

        local lonely_disk
        lonely_disk="$vdisk_dir/${cloud}.node$i"

        if [ -n "$want_sles12" ] ; then
            onhost_deploy_image "lonely" "SLE12" $lonely_disk
        else
            onhost_deploy_image "lonely" "SLE11" $lonely_disk
        fi

        virsh destroy $lonely_node
        virsh undefine $lonely_node
        if ! virsh define /tmp/$lonely_node.xml ; then
            echo "=====================================================>>"
            complain 76 "Could not define VM for: $lonely_node"
        fi
        if ! virsh start $lonely_node ; then
            echo "=====================================================>>"
            complain 76 "Could not start VM for: $lonely_node"
        fi
    done
}

# register lonely_node against crowbar
function crowbar_register()
{
    for i in $lonelynodeids; do
        local i2=$( printf %02x $i )
        local mac="52:54:$i2:88:77:$i2"
        sshrun "lonelymac=$mac adminip=$adminip onadmin_crowbar_register"
    done
}

function prepareinstcrowbar()
{
    echo "connecting to crowbar admin server at $adminip"
    # qa_crowbarsetup uses static network which needs static resolv.conf
    # so we use the host's external IPv4 resolvers,
    # which must allow queries from routed IPs
    grep '^nameserver\s*[0-9]*\.' /etc/resolv.conf | sshrun dd of=/etc/resolv.conf
    onadmin prepareinstallcrowbar
    return $?
}

function scp_install_chef_suse_override()
{
    if [ -e "$install_chef_suse_override" ]; then
        $scp -p "$install_chef_suse_override" \
            root@$adminip:/tmp/install-chef-suse.sh
    fi
}

function instcrowbar()
{
    scp_install_chef_suse_override
    onadmin installcrowbar
    local ret=$?
    $scp root@$adminip:screenlog.0 "$artifacts_dir/screenlog.0.install-suse-cloud"
    return $ret
}

function instcrowbarfromgit()
{
    scp_install_chef_suse_override
    onadmin installcrowbarfromgit
    local ret=$?
    $scp root@$adminip:screenlog.0 "$artifacts_dir/screenlog.0.install-suse-cloud"
    return $ret
}

function mkvlan()
{
    local DEFVLAN=$1 ; shift
    local IP=$1 ; shift
    vconfig add $cloudbr $DEFVLAN
    ifconfig $cloudbr.$DEFVLAN $IP/24
    ethtool -K $cloudbr tx off
}

function hypervisor_has_virtio()
{
    local llibvirt_type=${1:-$libvirt_type}
    [[ "$llibvirt_type" = "xen" || "$llibvirt_type" = "hyperv" ]] && return 1
    return 0
}

function onhost_create_libvirt_computenode_config()
{
    onhost_cpuflags_settings
    nodeconfigfile=$1
    nodecounter=$2
    macaddress=$3
    cephvolume="$4"
    drbdvolume="$5"
    nicmodel=virtio
    hypervisor_has_virtio || nicmodel=e1000
    nodememory=$compute_node_memory
    [ "$nodecounter" = "1" ] && nodememory=$controller_node_memory

    cat > $nodeconfigfile <<EOLIBVIRT
<domain type='kvm'>
  <name>$cloud-node$nodecounter</name>
  <memory>$nodememory</memory>
  <currentMemory>$nodememory</currentMemory>
  <vcpu>$vcpus</vcpu>
  <os>
    <type arch='x86_64' machine='pc-0.14'>hvm</type>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  $cpuflags
  <clock offset='utc'/>
  <on_poweroff>preserve</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>$emulator</emulator>
    <disk type='block' device='disk'>
      <driver name='qemu' type='raw' cache='unsafe'/>
      <source dev='$vdisk_dir/$cloud.node$nodecounter'/>
      <target dev='vda' bus='virtio'/>
      <boot order='2'/>
    </disk>
    $cephvolume
    $drbdvolume
    <interface type='network'>
      <mac address='$macaddress'/>
      <source network='$cloud-admin'/>
      <model type='$nicmodel'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
      <boot order='1'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target type='serial' port='0'/>
    </console>
    <input type='mouse' bus='ps2'/>
    <graphics type='vnc' port='-1' autoport='yes'/>
    <video>
      <model type='cirrus' vram='9216' heads='1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
  </devices>
</domain>
EOLIBVIRT

    if ! hypervisor_has_virtio ; then
        sed -i -e "s/<target dev='vd\([^']*\)' bus='virtio'/<target dev='sd\1' bus='ide'/" $nodeconfigfile
    fi
}

function setuppublicnet()
{
    # workaround https://bugzilla.novell.com/show_bug.cgi?id=845496
    echo 0 > /proc/sys/net/bridge/bridge-nf-call-iptables
    # public = $public_vlan
    mkvlan $public_vlan $net_public.1
}

function shutdowncloud()
{
    virsh shutdown $cloud-admin
    for i in $allnodeids ; do
        virsh shutdown $cloud-node$i
    done
}

function restartcloud()
{
    virsh net-start $cloud-admin
    virsh start $cloud-admin
    setuppublicnet
    for i in $allnodeids ; do
        virsh start $cloud-node$i
    done
}

# bring up VMs that will take cloud controller/compute/storage roles
function setupcompute()
{
    setuppublicnet
    alldevices=$(echo {b..z} {a..z}{a..z})
    for i in $allnodeids_without_lonely ; do
        c=1
        i2=$( printf %02x $i )
        macaddress="52:54:$i2:77:77:$i2"

        cephvolume=""
        if [ $cephvolumenumber -gt 0 ] ; then
            for n in $(seq 1 $cephvolumenumber) ; do
                dev=vd$(echo $alldevices | cut -d" " -f$c)
                c=$(($c + 1))
                cephvolume="$cephvolume
    <disk type='block' device='disk'>
        <serial>$cloud-node$i-ceph$n</serial>
        <driver name='qemu' type='raw' cache='unsafe'/>
        <source dev='$vdisk_dir/$cloud.node$i-ceph$n'/>
        <target dev='$dev' bus='virtio'/>
    </disk>"
            done
        fi

        drbdvolume=""
        if [ $drbd_hdd_size != 0 ]; then
            if [ $i -le 2 ] ; then
                drbddev=vd$(echo $alldevices | cut -d" " -f$c)
                c=$(($c + 1))
                local drbd_serial
                drbd_serial="$cloud-node$i-drbd"
                drbdvolume="
    <disk type='block' device='disk'>
        <serial>$drbd_serial</serial>
        <driver name='qemu' type='raw' cache='unsafe'/>
        <source dev='$vdisk_dir/$cloud.node$i-drbd'/>
        <target dev='$drbddev' bus='virtio'/>
    </disk>"
                drbdnode_mac_vol="${drbdnode_mac_vol}+${macaddress}#${drbd_serial}"
                drbdnode_mac_vol="${drbdnode_mac_vol#+}"
            fi
        fi

        onhost_create_libvirt_computenode_config /tmp/$cloud-node$i.xml $i $macaddress "$cephvolume" "$drbdvolume"

        virsh destroy $cloud-node$i 2>/dev/null
        virsh undefine $cloud-node$i 2>/dev/null
        if ! virsh define /tmp/$cloud-node$i.xml ; then
            echo "====>>"
            complain 74 "Could not define VM for: node$i"
        fi
        if ! virsh start $cloud-node$i ; then
            echo "====>>"
            complain 74 "Could not start VM for: node$i"
        fi
    done
    echo "========================================================"
    echo " Note: If you interrupt mkcloud now and want to proceed"
    echo "       later, make sure to run the following command:"
    echo
    echo " export drbdnode_mac_vol=\"${drbdnode_mac_vol}\""
    echo
    echo "========================================================"
    sleep 10
    return 0
}

# allocate cloud nodes with an operating system
# and wait for nodes to reach the ready state
function instcompute()
{
    onadmin allocate ||\
        return $?

    echo "Waiting for the installation of the nodes ..."
    onadmin waitcompute
    return $?
}


function proposal()
{
    onadmin proposal
    return $?
}

function testsetup()
{
    onadmin testsetup
    local ret=$?
    $scp root@$adminip:tempest.log "$artifacts_dir/tempest.log"
    return $ret
}

function rebootcrowbar()
{
    # reboot the crowbar instance
    #  and test if everything is up and running afterwards
    sshrun "reboot"
    wait_for 500 3 "! nc -z $adminip 22" 'crowbar to go down'
    wait_for 500 3   "nc -z $adminip 22" 'crowbar to be back online'
    echo "waiting another 180 seconds for services"
    sleep 180
    sshrun "mount -a -t nfs" # workaround repos not mounted on reboot because NFS needs bind, but bind says it requires NFS
    return $?
}

function rebootcompute()
{
    # reboot compute nodes
    #  and test if everthing is up and running afterwards
    onadmin rebootcompute
    return $?
}

function rebootneutron()
{
    onadmin rebootneutron
    return $?
}

function securitytests()
{
    # install and run security test suite owasp
    onadmin securitytests
    return $?
}

function qa_test()
{
    local ghsc=github.com/SUSE-Cloud
    mkdir -p ~/$ghsc/
    pushd ~/$ghsc/
    if [ -e "qa-openstack-cli" ] ; then
        cd qa-openstack-cli/
        git pull
    else
        git clone https://$ghsc/qa-openstack-cli.git
    fi
    popd
    rsync -av ~/$ghsc/qa-openstack-cli root@$adminip:
    onadmin qa_test
    ret=$?

    mkdir -p .artifacts
    $scp -r root@$adminip:qa_test.logs/ .artifacts/
    return $ret
}

function crowbarbackup()
{
    onadmin crowbarbackup
    local ret=$?
    safely $scp root@$adminip:/tmp/backup-crowbar.tar.gz .
    [ -d "$artifacts_dir" ] && mv backup-crowbar.tar.gz "$artifacts_dir/"
    return $ret
}

function crowbarrestore()
{
    local btarball=backup-crowbar.tar.gz
    [ -e "$artifacts_dir/$btarball" ] && btarball="$artifacts_dir/$btarball"
    if [ ! -e "$btarball" ] ; then
        complain 56 "No crowbar backup tarball found."
    fi
    $scp "$btarball" root@$adminip:/tmp/
    onadmin crowbarpurge && onadmin crowbarrestore
    return $?
}

function prepare_cloudupgrade()
{
    onadmin prepare_cloudupgrade
    return $?
}

function cloudupgrade_1st()
{
    onadmin cloudupgrade_1st
    return $?
}

function cloudupgrade_2nd()
{
    onadmin cloudupgrade_2nd
    return $?
}

function cloudupgrade_clients()
{
    onadmin cloudupgrade_clients
    return $?
}

function cloudupgrade_reboot_and_redeploy_clients()
{
    onadmin cloudupgrade_reboot_and_redeploy_clients
    return $?
}

function usage()
{
    cat <<EOUSAGE
Usage:
$0 <step> [<step>,...]

'step' is one of:
    $allcmds

These steps are expanding to the following steps:
    all
        -> $(expand_steps all)
    all_noreboot
        -> $(expand_steps all_noreboot)
    plain
        -> $(expand_steps plain)
    plain_with_upgrade
        -> $(expand_steps plain_with_upgrade)
    instonly
        -> $(expand_steps instonly)
    alias_new_admin
        -> $(expand_steps alias_new_admin)
    alias_compute
        -> $(expand_steps alias_compute)
    alias_testupdate
        -> $(expand_steps alias_testupdate)
    alias_upgrade
        -> $(expand_steps alias_upgrade)

Steps:
    cleanup:        kill all running VMs, zero out boot sectors of all LVM volumes
    prepare:        create LVM volumes, setup libvirt networks
    setupadmin:     create the admin node and install the cloud product
    prepareinstcrowbar: add repos and install crowbar packages
    instcrowbar:    install crowbar and chef on the admin node
    setupcompute:   create the compute nodes and let crowbar discover them
    instcompute:    allocate and install compute nodes
    proposal:       create and apply proposals for default setup
    setuplonelynodes: boot a number (defined by nodenumberlonelynodes) of non-crowbar registered nodes in the admin network
    crowbar_register: register a number (defined by nodenumberlonelynodes) of non-crowbar nodes with crowbar (setuplonelynodes needs to have run before)
    testsetup:      start a VM in the cloud
    addupdaterepo:  addupdate repos defined in UPDATEREPOS= (URLs separated by '+')
    runupdate:      run zypper up on the crowbar node
                    (compute nodes are automaticallyupdated via chef run)
    rebootcrowbar:  reboot the crowbar instance and wait for it being up
    rebootcompute:  reboot the compute nodes and wait for them being up
    restartcloud:   start a pre-existing cloud again after host reboot
    securitytests:  install and run security test suite
    qa_test:        run the qa test suite
    help:           this usage


Environment variables (need to be exported):

Mandatory
    cloudvg=vg0
        set the volume group name to create lvm volumes in
        Cloud volumes will be prefixed with the cloud name.
        The cleanup function will only cleanup volumes with this prefix.
    cloudpv=/dev/vdx (default /dev/vdb)
        Device where a LVM physical volume will be created, all data lost on this device
        Should be at least 80 GB. The volume group will be called 'cloud'.
    cloudsource=develcloud3 | develcloud4 | develcloud5 | susecloud5 | GM3 | GM4 | GM3+up | GM4+up | M?  (default '')
        defines the installation source of the product
        develcloud3   : product from IBS Devel:Cloud:3
        develcloud4   : product from IBS Devel:Cloud:4
        develcloud5   : product from IBS Devel:Cloud:5
        susecloud5    : product from IBS SUSE:SLE....
        GM3           : SUSE Cloud Goldmaster 3 without updates
        GM4           : SUSE Cloud Goldmaster 4 without updates
        GM3+up        : SUSE Cloud Goldmaster 3 with released maintenance updates
        GM4+up        : SUSE Cloud Goldmaster 4 with released maintenance updates
        M?            : uses official Milestone? ISO image (? is a number)

Optional
    qa_crowbarsetup='path/to/script' (default: same directory as mkcloud is located in)
        set an optional path to qa_crowbarsetup.sh
    hacloud='' | 1  (default='')
        Set up a highly available cloud; requires to configure the clusters via clusterconfig='...'
    clusterconfig
        A string with a cluster configuration. The services for data, network and services cluster can
        be deployed in 1, 2 or 3 clusters. The configuration string looks like this:
        The string is: '<group1>:<group2>' (a ':' separated list of groups).
        A group is:    'clustername1+clustername2=clusternodenumber
        The first clustername of a group defines the name of the cluster.
        Examples:
            3 clusters: clusterconfig='data=2:network=3:services=5'
            2 clusters: clusterconfig='services+data=2:network=3'
            1 cluster:  clusterconfig='data+network+services=2'
    upgrade_cloudsource='' (default='')
        set new cloudsource for upgrade process
    TESTHEAD='' | 1  (default='')
        use Media from Devel:Cloud:Staging and add test update repositories (except for GM* targets)
    cephvolumenumber  (default=0)
        the number of 11GB ceph volumes that will be created per node
        note: proposal step does NOT contain a ceph proposal, do it manually
    nodenumber=2    (default 2)
        set the number of nodes to be created for the cloud (excluding admin node)
        In HA mode (hacloud=1) the nodes needed for clusters are subtracted from nodenumber; the
        remaining nodes are compute nodes.
    nodenumberlonelynode=2    (default 0)
        set the number of non-crowbar registered nodes to be created in the admin network
    nodenumbercompute=1  (default $nodenumber resp. 1 in case of hacloud=1)'
        set the number of compute nodes in a HA setup, the remaining nodes
        of $nodenumber - $nodenumbercompute are used as HA cluster nodes
    vcpus=1         (default 1)
        set the number of CPU cores per compute node
    adminvcpus=1    (default $vcpus)
        set the number of CPU cores for admin node
    admin_node_memory (default 2097152)
        Set the memory in KB assigned to admin node
    controller_node_memory (default 4194304)
        Set the memory in KB assigned to compute nodes
    compute_node_memory (default 2097152)
        Set the memory in KB assigned to compute nodes
    drbd_hdd_size  (default 0, or 50 if hacloud is set)
        Set the size in GB of the DRBD data disks attached to the
        nodes in the cluster hosting the database and rabbitmq.
    networkingplugin
        Set the networking plugin to be used by neutron (e.g. openvswitch),
        if it isn't defined the barclamp-neutron's default is used.
    networkingmode
        Set the networking mode to be used by neutron (e.g. gre)
        if it isn't defined the barclamp-neutron's default is used.
    keep_existing_hostname=1    (default='')
        If this option is enabled crowbar_register keeps the existing
        hostname. When the crowbar_register option is enabled too then
        it generates and sets random hostnames.
    debug_mkcloud=1  (default 0)
        enable debug mode for mkcloud via 'set -x'
    debug_qa_crowbarsetup=1 (default 0)
        enable debug mode for qa_crowbarsetup.sh via 'set -x'
    debug_openstack=1 (default 0)
        enable debug mode for the openstack components
        sets debug true in the openstack proposals
    user_keyfile='path/to/file'
        path to optional user public ssh keyfile to inject into crowbar to
        /root/.ssh/authorized_keys file and to provisioner barclamp
    want_neutronsles12=1 (default 0)
        if there is a SLE12 node, deploy neutron-l3 role into the SLE12 node
    install_chef_suse_override='path/to/script'
        Optional path to an alternate version of install-chef-suse.sh on the mkcloud hostto use
        instead of the one from the packages.  This will be scp'd to the admin node before use.
    public_vlan=<id> (default 300)
        VLAN id for public network

EOUSAGE
    exit 1

# UNDOCUMENTED OPTIONS:
#
# virtualcloud
# cloudfqdn
# forwardmode
# net_fixed
# net_public
# net_admin
# adminnetmask
# adminip
# admingw
# cpuflags
# admin_node_memory
# controller_node_memory
# compute_node_memory
# hyperv_node_memory
# adminnode_hdd_size
# controller_hdd_size
# computenode_hdd_size
# cephvolume_hdd_size
# controller_ceph_hdd_size
# cloudbr
# localreposdir_src
# localreposdir_target

}


function addupdaterepo()
{
    sshrun "UPDATEREPOS=$UPDATEREPOS onadmin_addupdaterepo"
    return $?
}

function runupdate()
{
    onadmin runupdate
    return $?
}

function is_concurrent_run()
{
    [ -e $pidfile ] && kill -0 `cat $pidfile` 2>/dev/null && return 0
    echo $$ > $pidfile
    return 1
}

function sanity_checks()
{
    if test `id -u` != 0 ; then
        complain 1 "This script needs to be run as root" \
            "Please be aware that this script will create a LVM" \
            "and kill all current VMs on this host."
    fi

    if is_concurrent_run; then
        complain 33 "mkcloud was started twice from same working directory: `pwd`" \
            "Please always use a separate working directory for each (parallel) mkcloud run."
    fi

    # test for existence of qa_crowbarsetup.sh
    if [ ! -e $qa_crowbarsetup ] ; then
        complain 87 "$qa_crowbarsetup not found
            Please define the path to it by setting qa_crowbarsetup=/path/to/file
            or call mkcloud from your git clone."
    fi

    if [ -z "$cloudsource" ] ; then
        echo "Please set the env variable:"
        echo "export cloudsource=M?|develcloud3|develcloud4|develcloud5|susecloud5|GM3|GM4|GM3+up|GM4+up"
        exit 1
    fi

    # checking clusterconfig
    if [ -n "$hacloud" -a -z "$clusterconfig" ] ; then
        echo "Examples for clusterconfig:"
        echo '3 clusters: clusterconfig="data=2:network=3:services=3"'
        echo '2 clusters: clusterconfig="services+data=2:network=3"'
        echo '1 cluster:  clusterconfig="data+network+services=2"'
        complain 70 "No cluster config provided for HA setup."
    fi

    vgdisplay "$cloudvg" >/dev/null 2>&1 && needcvol=
    if [ -n "$needcvol" ] ; then
        : ${cloudpv:=/dev/vdb}
        if grep -q $cloudpv /proc/mounts ; then
            complain 92 "The device $cloudpv seems to be used. Exiting."
        fi
        if [ ! -e $cloudpv ] ; then
            complain 93 "$cloudpv does not exist." \
                "Please set the cloud volume group to an existing device: export cloudpv=/dev/sdx" \
                "Running 'partprobe' may help to let the device appear."
        fi
    fi

    if [ -e /etc/init.d/SuSEfirewall2_init ] && rcSuSEfirewall2 status ; then
        complain 91 "SuSEfirewall is running - it will interfere with the iptables rules done by libvirt" \
            "Please stop the SuSEfirewall completely and run mkcloud again" \
            "Run:  rcSuSEfirewall2 stop && insserv -r SuSEfirewall2_setup && insserv -r SuSEfirewall2_init"
    fi

    if grep "devpts.*[^x]mode=.00" /proc/mounts ; then
        complain 13 "/dev/pts is not accessible for libvirt, maybe you use autobuild on your system." \
            "Please remount it using the following command:" \
            " # mount -o remount,mode=620,gid=5 devpts -t devpts /dev/pts"
    fi

    if [[ $wantedcmds =~ "setuplonelynodes" || $wantedcmds =~ "crowbar_register" ]] ; then
        if [[ -z "$nodenumberlonelynode" || $nodenumberlonelynode -lt 1 ]] ; then
            complain 80 "Please set nodenumberlonelynode."
        fi
    fi
}


## MAIN ##
step_aliases="alias_new_admin alias_compute alias_upgrade alias_testupdate"

allcmds="$step_aliases all all_noreboot instonly plain plain_with_upgrade cleanup setuphost prepare setupadmin prepareinstcrowbar instcrowbar instcrowbarfromgit setupcompute instcompute proposal testsetup rebootcrowbar rebootcompute addupdaterepo runupdate testupdate securitytests crowbarbackup crowbarrestore shutdowncloud restartcloud qa_test help rebootneutron prepare_cloudupgrade cloudupgrade_1st cloudupgrade_2nd cloudupgrade_clients cloudupgrade_reboot_and_redeploy_clients setuplonelynodes crowbar_register"
wantedcmds=$@

function expand_steps()
{
    # parse the commands and expand the aliases
    local runcmds=''
    local localwantedcmds=$@
    local cmd
    for cmd in $localwantedcmds ; do

        local found=0
        local onecmd
        for onecmd in $allcmds ; do
            if [ $onecmd = $cmd ] ; then
                found=1
                case "$cmd" in
                    alias_new_admin)
                        runcmds="$runcmds cleanup prepare setupadmin addupdaterepo runupdate prepareinstcrowbar instcrowbar"
                    ;;
                    alias_compute)
                        runcmds="$runcmds setupcompute instcompute proposal"
                    ;;
                    all)
                        runcmds="$runcmds `expand_steps alias_new_admin` rebootcrowbar `expand_steps alias_compute` testsetup rebootcompute"
                    ;;
                    all_noreboot)
                        runcmds="$runcmds`expand_steps alias_new_admin` `expand_steps alias_compute` testsetup"
                    ;;
                    alias_testupdate|testupdate)
                        runcmds="$runcmds addupdaterepo runupdate testsetup"
                    ;;
                    plain)
                        runcmds="$runcmds `expand_steps instonly` proposal"
                    ;;
                    instonly)
                        runcmds="$runcmds cleanup prepare setupadmin prepareinstcrowbar instcrowbar setupcompute instcompute"
                    ;;
                    alias_upgrade)
                        runcmds="$runcmds prepare_cloudupgrade cloudupgrade_1st cloudupgrade_2nd cloudupgrade_clients cloudupgrade_reboot_and_redeploy_clients"
                    ;;
                    plain_with_upgrade)
                        runcmds="$runcmds `expand_steps plain` addupdaterepo runupdate `expand_steps alias_upgrade`"
                    ;;
                    *)
                        runcmds="$runcmds $cmd"
                    ;;
                esac
            fi
        done
        [ $found == 0 ] && complain - "Step $cmd not found." && return

    done
    runcmds=${runcmds## }
    runcmds=${runcmds%% }
    echo "${runcmds//  / }"
}

steplist=`expand_steps $wantedcmds`
[[ ! $steplist ]] || [[ "$steplist" =~ "help" ]] && usage

sanity_checks

echo "You choose to run these mkcloud steps:"
echo "  $steplist"
echo
sleep 2

for cmd in `echo $steplist` ; do
    echo
    echo "============> MKCLOUD STEP START: $cmd <============"
    echo
    sleep 2
    $cmd
    ret=$?
    if [ $ret != 0 ] ; then
        echo
        echo '$h1!!'
        echo "Error detected. Stopping mkcloud."
        echo "The step '$cmd' returned with exit code $ret"
        echo "Please refer to the $cmd function in this script when debugging the issue."
        error_exit $ret ""
    fi >&2
    echo
    echo "^^^^^^^^^^^^= MKCLOUD STEP DONE: $cmd =^^^^^^^^^^^^"
    echo
done

pre_exit_cleanup
show_environment
