#!/bin/bash
#
# mkcloud - Setup a virtual cloud on one system (physical or even virtual)
#
# Authors: J. Daniel Schmidt <jdsn@suse.de>
#          Bernhard M. Wiedemann <bwiedemann@suse.de>
#
# 2012, SUSE LINUX Products GmbH
#

# Quick introduction:
#
# 1. read the usage: mkcloud help
#    or visit https://github.com/SUSE-Cloud/automation/blob/master/docs/mkcloud.md
# 2. This tool relies on the script qa_crowbarsetup.sh
# 3. Please 'export' environment variables according to your needs.


if [[ $debug_mkcloud = 1 ]] ; then
    set -x
fi

if [ -n "$CVOL" ] ; then
    cloudpv=$CVOL
    unset CVOL
    echo "------------------------------------------------------------------------"
    echo "The CVOL variable is deprecated."
    echo "Please either set 'cloudvg' or 'cloudpv'"
    echo " cloudvg will use an existing lvm and not destroy other volumes"
    echo " cloudpv will use an lvm pv device exclusively and destroy all volumes"
    echo
    echo "Continuing in 20 seconds with fallback (cloudpv=\$CVOL):"
    echo " cloudpv=$CVOL"
    echo "Otherwise press Ctrl-C now!"
    echo "------------------------------------------------------------------------"
    sleep 20
fi

# FIXME: separate user-tweakable parameters from script local variables.
# Currently there is no clearly defined interface point.  One of the
# causes of this is violation of the common shell coding standard which
# uses uppercase for environment variables and constants, and lowercase
# for local variables.
: ${mkcloud_dir:=$(dirname $(readlink -e $0))}
: ${qa_crowbarsetup:=${mkcloud_dir}/qa_crowbarsetup.sh}
mkcloud_lib_dir=${mkcloud_dir}/lib
# include separate bash libs
# NOTE that this is a temporary solution during refactoring of mkcloud
mkcloud_temporary_scripts="mkcloud-libvirt.sh"
for script in $mkcloud_temporary_scripts; do
    source ${mkcloud_lib_dir}/$script
done
mkcconf=mkcloud.config
rm -f $mkcconf # we dont want to source old info in the line below
source $qa_crowbarsetup

: ${virtualcloud:=virtual}
: ${net_admin:=192.168.124}
setcloudnetvars $virtualcloud
: ${forwardmode:=nat}
: ${adminnetmask:=255.255.248.0}
# the default nodenumber of compute nodes
nodenumbercomputedefault=2
[ -n "$hacloud" ] && nodenumbercomputedefault=1
: ${nodenumber:=$nodenumbercomputedefault}
: ${nodenumbercompute:=$nodenumbercomputedefault}
# expect to have this many physical machines attached via $mkch_physcloudif
# these replace virtual machines, so we start less VMs
: ${nodenumberphys:=0}
[[ $nodenumberphys = 0 ]] || [[ -n $mkch_physcloudif ]] || complain 100 "need to set mkch_physcloudif for physical nodes"
# configuration of clusters
: ${clusterconfig:=''}
: ${nodenumberlonelynode:=0}
export nodenumber nodenumbercompute nodenumberlonelynode clusterconfig
# '+'-separated list of MAC#serial_of_drbd_volume of the drbd cluster nodes
# (used only internally to transport this information to qa_crowbarsetup):
: ${drbdnode_mac_vol:=''}
: ${cephvolumenumber:=1}
allnodeids_without_lonely=`seq 1 $((nodenumber-nodenumberphys))`
lonelynodeids=`seq $(( nodenumber + 1 )) $(( nodenumber + nodenumberlonelynode ))`
allnodeids="$allnodeids_without_lonely $lonelynodeids"
: ${vcpus:=1}
: ${adminvcpus:=$vcpus}
cpuflags=''
working_dir_orig=`pwd`
: ${artifacts_dir:=$working_dir_orig/.artifacts}
start_time=`date`
: ${cloud:=cloud}
: ${cloudvg:=$cloud}
needcvol=1
: ${vdisk_dir:=/dev/$cloudvg}
: ${admin_node_disk:=$vdisk_dir/$cloud.admin}
: ${admin_node_memory:=2097152}
: ${controller_node_memory:=5242880}
: ${compute_node_memory:=2097152}
: ${hyperv_node_memory:=3000000}
[[ "$libvirt_type" = "hyperv" && $compute_node_memory -lt $hyperv_node_memory ]] && compute_node_memory=$hyperv_node_memory
# hdd size defaults (unless defined otherwise)
: ${adminnode_hdd_size:=15}
: ${controller_hdd_size:=20}
: ${computenode_hdd_size:=15}
: ${cephvolume_hdd_size:=21}
: ${controller_ceph_hdd_size:=25}
if [ -n "$hacloud" ]; then
    : ${drbd_hdd_size:=50}
else
    : ${drbd_hdd_size:=0}
fi
: ${drbd_database_size:=20}
: ${drbd_rabbitmq_size:=20}
# pvlist is filled below
pvlist=
next_pv_device=
pv_cur_device_no=0
: ${cloudbr:=${cloud}br}
sshopts="-oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null"
scp="scp $sshopts"
#if localreposdir_src string is available, the local repositories are used for setup
: ${localreposdir_src:=""}
#localreposdir_target is the 9p target dir and also the mount target dir in the VM
: ${localreposdir_target:="/repositories"}
[ -z "$localreposdir_src" ] && localreposdir_target=""
: ${install_chef_suse_override:=./install-chef-suse.sh}
: ${cct_tests:="features:base"}

emulator=/usr/bin/qemu-system-$(uname -m)
if [ -x /usr/bin/qemu-kvm ] && file /usr/bin/qemu-kvm | grep -q ELF; then
    # on SLE11, qemu-kvm is preferred, since qemu-system-x86_64 is
    # some rotten old stuff without KVM support
    emulator=/usr/bin/qemu-kvm
fi

pidfile=mkcloud.pid

trap 'error_exit $? "error caught by trap"' TERM
exec </dev/null

function is_suse()
{
    grep -qi suse /etc/*release
}

function show_environment()
{
    end_time=`date`
    echo "Environment Details"
    echo "-------------------------------"
    echo "    hostname: `hostname -f`"
    echo "     started: $start_time"
    echo "       ended: $end_time"
    echo "-------------------------------"
    echo " cloudsource: $cloudsource"
    echo "    TESTHEAD: $TESTHEAD"
    echo "  nodenumber: $nodenumber"
    echo "     cloudpv: $cloudpv"
    echo " UPDATEREPOS: $UPDATEREPOS"
    echo "    cephvolumenumber: $cephvolumenumber"
    echo " upgrade_cloudsource: $upgrade_cloudsource"
    echo "-------------------------------"
    env | grep -i "^want_"
    echo "-------------------------------"
}

function pre_exit_cleanup()
{
    rm $pidfile
}

function error_exit()
{
    exitcode=$1
    message=$2
    if [ -z "$SKIPSUPPORTCONFIG" ] ; then
        ssh $sshopts root@$adminip '
            set -x
            for node in $(crowbar machines list | grep ^d) ; do
            (
                echo "Collecting supportconfig from $node"
                timeout 400 ssh $node supportconfig | wc
                timeout 300 scp $node:/var/log/\*tbz /var/log/
            )&
            done
            timeout 500 supportconfig | wc &
            wait
        '
        mkdir -p $artifacts_dir
        $scp root@$adminip:/var/log/*tbz $artifacts_dir/
    fi
    pre_exit_cleanup
    echo $message
    show_environment
    exit $exitcode
} >&2

function sshrun()
{
    cat > $mkcconf <<EOF
        export drbdnode_mac_vol=$drbdnode_mac_vol ;
        export cloud=$virtualcloud ;
        # hostname of NFS server with repos and images:
        export clouddata=$clouddata ;
        export cloudfqdn=$cloudfqdn ;
        export cloudsource=$cloudsource ;
        export upgrade_cloudsource=$upgrade_cloudsource ;
        export adminip=$adminip ;
        export hacloud=$hacloud ;
        export libvirt_type=$libvirt_type ;
        export networkingplugin=$networkingplugin ;
        export networkingmode=$networkingmode ;
        export nosetestparameters=${nosetestparameters} ;
        export tempestoptions='${tempestoptions}' ;
        export cephvolumenumber=$cephvolumenumber ;
        export drbd_database_size=$drbd_database_size ;
        export drbd_rabbitmq_size=$drbd_rabbitmq_size ;
        export shell=$shell ;
        export keep_existing_hostname=$keep_existing_hostname ;
        export cct_tests=$cct_tests ;

        export nova_shared_instance_storage=$nova_shared_instance_storage ;

        export localreposdir_target=$localreposdir_target ;

        export cinder_backend=$cinder_backend;
        export cinder_netapp_storage_protocol=$cinder_netapp_storage_protocol;
        export cinder_netapp_login=$cinder_netapp_login;
        export cinder_netapp_password=$cinder_netapp_password;

        export TESTHEAD=$TESTHEAD ;
        export NOINSTALLCLOUDPATTERN=$NOINSTALLCLOUDPATTERN ;
EOF
    env|grep -e ^debug_ -e ^pre_ -e ^want_ -e ^net_ -e ^nodenumber -e ^clusterconfig | sort >> $mkcconf

    $scp $qa_crowbarsetup $mkcconf root@$adminip:
    ssh $sshopts root@$adminip "echo `hostname` > cloud ; . qa_crowbarsetup.sh ; $@"
    return $?
}

function onadmin()
{
    # functions can have parameters, so pass on all except $1
    local cmd=$1
    shift
    sshrun onadmin_$cmd "$@"
}

function cleanup()
{
    # cleanup leftover from last run
    ${mkcloud_lib_dir}/libvirt/cleanup $cloud $nodenumber $cloudbr $vlan_public

    if ip link show ${cloudbr}.$vlan_public >/dev/null 2>&1; then
        ip link set ${cloudbr}.$vlan_public down
    fi
    if ip link show ${cloudbr} >/dev/null 2>&1; then
        ip link set ${cloudbr} down
        ip link delete ${cloudbr} type bridge
        ip link delete ${cloudbr}-nic
    fi
    # 1. remove leftover partition mappings that are still open for this cloud
    local vol
    dmsetup ls | awk "/^$cloudvg-${cloud}\./ {print \$1}" | while read vol ; do
        kpartx -d /dev/mapper/$vol
    done

    # 2. remove all previous volumes for that cloud; this helps preventing
    # accidental booting and freeing space
    if [ -d $vdisk_dir ]; then
        find -L $vdisk_dir -name "$cloud.*" -type b | \
            safely xargs --no-run-if-empty lvremove --force
    fi

    if [ -n "$wipe" ] ; then
        vgchange -an $cloudvg
        dd if=/dev/zero of=$cloudpv count=1000
    fi
    return 0
}

function onhost_get_next_pv_device()
{
    if [ -z "$pvlist" ] ; then
        pvlist=`pvs --sort -Free | awk '$2~/'$cloudvg'/{print $1}'`
        pv_cur_device_no=0
    fi
    next_pv_device=`perl -e '$i=shift; $i=$i % @ARGV;  print $ARGV[$i]' $pv_cur_device_no $pvlist`
    pv_cur_device_no=$(( $pv_cur_device_no + 1 ))
}

# create lv device wrapper
function _lvcreate()
{
    lv_name=$1
    lv_size=$2
    lv_vg=$3
    lv_pv=$4

    # first: create on the PV device (spread IO)
    # fallback: create in VG (if PVs with different size exist)
    lvcreate -n $lv_name -L ${lv_size}G $lv_vg $lv_pv || \
        safely lvcreate -n $lv_name -L ${lv_size}G $lv_vg
}

# spread block devices over a LVM's PVs so that different VMs
# are likely to use different PVs to optimize concurrent IO throughput
function onhost_create_cloud_lvm()
{
    if [ -n "$needcvol" ] ; then
        safely pvcreate "$cloudpv"
        safely vgcreate "$cloudvg" "$cloudpv"
    fi
    safely vgchange -ay $cloudvg # for later boots

    local hdd_size

    onhost_get_next_pv_device
    _lvcreate $cloud.admin $adminnode_hdd_size $cloudvg $next_pv_device
    for i in $allnodeids ; do
        onhost_get_next_pv_device
        hdd_size=${computenode_hdd_size}
        test "$i" = "1" && hdd_size=${controller_hdd_size}
        _lvcreate $cloud.node$i $hdd_size $cloudvg $next_pv_device
    done

    if [ $cephvolumenumber -gt 0 ] ; then
        for i in $allnodeids ; do
            for n in $(seq 1 $cephvolumenumber) ; do
                onhost_get_next_pv_device
                hdd_size=${cephvolume_hdd_size}
                test "$i" = "1" -a "$n" = "1" && hdd_size=${controller_ceph_hdd_size}
                _lvcreate $cloud.node$i-ceph$n $hdd_size $cloudvg $next_pv_device
            done
        done
    fi

    # create volumes for drbd
    if [ $drbd_hdd_size != 0 ] ; then
        for i in `seq 1 2`; do
            onhost_get_next_pv_device
            _lvcreate $cloud.node$i-drbd $drbd_hdd_size $cloudvg $next_pv_device
        done
    fi

    echo "Checking for LVs treated by LVM as valid PV devices ..."
    if lvmdiskscan | egrep "/dev/($cloudvg/|mapper/$cloudvg-)"; then
        error=$(cat <<EOF
Error: your lvm.conf is not filtering out mkcloud LVs.
Please fix by adding the following regular expressions
to the filter value in the devices { } block within your
/etc/lvm/lvm.conf file:

    "r|/dev/mapper/$cloudvg-|", "r|/dev/$cloudvg/|", "r|/dev/disk/by-id/|"

The filter should also include something like "r|/dev/dm-|" or "r|/dev/dm-1[56]|", but
the exact values depend on your local system setup and could change
over time or have side-effects (on lvm in dm-crypt or lvm in lvm),
so please add/modify it manually.
EOF
)
        complain 94 "$error"
    fi
}

function onhost_deploy_image()
{
    local image
    local role=$1
    local dist=${2:-SLE11}
    local disk=${3:-$admin_node_disk}

    case $dist in
    SLE12)
        image=SLES12.qcow2
        ;;
    SLE11)
        image=SP3-64up.qcow2
        ;;
    *)
        complain 71 "onhost_deploy_image has not enough information about how to deploy. (distribution: $dist)"
        ;;
    esac

    pushd /tmp
    safely wget --progress=dot:mega -N \
        http://clouddata.cloud.suse.de/images/$image

    echo "Cloning $role node vdisk from $image ..."
    safely qemu-img convert -p $image $disk
    popd

    # only resize if we have a 2nd partition with a rootfs
    if fdisk -l $disk | grep -q "2 *\* *.*83 *Linux" ; then
        # make a bigger partition 2
        echo -e "d\n2\nn\np\n2\n\n\na\n2\nw" | fdisk $disk
        part2=$(kpartx -asv $disk|perl -ne 'm/add map (\S+2) / && print $1')
        test -n "$part2" || complain 31 "failed to find partition #2"
        local bdev=/dev/mapper/$part2
        safely fsck -y -f $bdev
        safely resize2fs $bdev
        sleep 1 # time for dev to become unused
        safely kpartx -dv $disk
    fi
}

function onhost_add_etchosts_entries()
{
    grep -q crowbar /etc/hosts || echo "$adminip crowbar.$cloudfqdn crowbar" >> /etc/hosts
}

function onhost_enable_ksm
{
    # enable kernel-samepage-merging to save RAM
    echo 1 > /sys/kernel/mm/ksm/run
    echo 1000 > /sys/kernel/mm/ksm/pages_to_scan
    # huge pages can not be shared or swapped, so do not use them
    echo never > /sys/kernel/mm/transparent_hugepage/enabled
}

function setuphost()
{
    if is_suse ; then
        export ZYPP_LOCK_TIMEOUT=60
        zypper --non-interactive in --no-recommends \
            libvirt kvm lvm2 curl wget bridge-utils \
            dnsmasq netcat-openbsd ebtables libvirt-python
        [ "$?" == 0 -o "$?" == 4 ] || exit 10
    fi
}

function prepare()
{
    local image="SLE11"
    iscloudver 6plus && image="SLE12"

    if ! [ -e ~/.ssh/id_dsa ]; then
        echo "Creating key for controlling our VMs..."
        ssh-keygen -t dsa -f ~/.ssh/id_dsa -N ""
    fi

    onhost_enable_ksm
    onhost_create_cloud_lvm
    onhost_deploy_image "admin" $image "$admin_node_disk"
    onhost_add_etchosts_entries
}

function ssh_password()
{
    SSH_ASKPASS=/root/echolinux
    cat > $SSH_ASKPASS <<EOSSHASK
#!/bin/sh
echo linux
EOSSHASK
    chmod +x $SSH_ASKPASS
    DISPLAY=dummydisplay:0 SSH_ASKPASS=$SSH_ASKPASS setsid ssh $sshopts -oNumberOfPasswordPrompts=1 "$@"
}

function wait_for_crowbar_ssh()
{
    wait_for 150 1 "nc -z $adminip 22" 'admin node to start ssh daemon'
}

# bring up the VM for crowbar
function setupadmin()
{
    libvirt_setupadmin

    wait_for 300 1 "ping -q -c 1 -w 1 $adminip >/dev/null" 'crowbar admin VM'

    if [ -z "$NOSETUPPORTFORWARDING" ] ; then
        nodehostips=$(seq -s ' ' 81 $((80 + $nodenumber)))
        cat > /etc/init.d/boot.mkcloud <<EOS
#!/bin/bash

iptables -t nat -F PREROUTING
for i in 22 80 443 3000 4000 4040 ; do
    iptables -I FORWARD -p tcp --dport \$i -j ACCEPT
    for host in 10 $nodehostips ; do
        offset=80
        [ "\$host" = 10 ] && offset=10
        iptables -t nat -I PREROUTING -p tcp --dport \$((\$i + \$host - \$offset + 1100)) -j DNAT --to-destination $net_admin.\$host:\$i
    done
done
iptables -t nat -I PREROUTING -p tcp --dport 6080 -j DNAT --to-destination $net_public.2
echo 0 > /proc/sys/net/ipv4/conf/all/rp_filter
EOS
        if ! grep -q "boot.mkcloud" /etc/init.d/boot.local ; then
            cat >> /etc/init.d/boot.local <<EOS

# --v--v--  Automatically added by mkcloud on `date`
/etc/init.d/boot.mkcloud
# --^--^--  End of automatically added section from mkcloud
EOS
        fi
    fi
    chmod +x /etc/init.d/boot.mkcloud
    /etc/init.d/boot.mkcloud

    wait_for_crowbar_ssh

    echo "waiting some more for sshd+named to start"
    sleep 25
    echo "Injecting public key into admin node..."
    local keyfile=~/.ssh/id_dsa.pub
    local pubkey=`cut -d" " -f2 $keyfile`
    ssh_password $adminip "
        mkdir -p ~/.ssh;
        grep -q $pubkey ~/.ssh/authorized_keys 2>/dev/null ||
            echo ssh-dss $pubkey injected-key-from-host >> ~/.ssh/authorized_keys
    "
    if [[ -n $user_keyfile ]]; then
        cat $user_keyfile | ssh $adminip "cat >> ~/.ssh/authorized_keys"
    fi
    echo "you can now proceed with installing crowbar"
}

function createadminsnapshot()
{
    virsh shutdown $cloud-admin
    wait_for 150 1 "virsh domstate $cloud-admin|grep shut.off" 'admin node to shut down'
    lvremove -f $admin_node_disk.snap
    safely lvcreate -l100%ORIGIN -s -n $cloud.admin.snap $admin_node_disk
    virsh start $cloud-admin
    wait_for_crowbar_ssh
}

function restoreadminfromsnapshot()
{
    safely test -b $admin_node_disk.snap
    virsh destroy $cloud-admin
    safely lvconvert --merge $admin_node_disk.snap
    createadminsnapshot
}

# Returns success if a change was made
function confset()
{
    file="$1" key="$2" value="$3"
    if grep -q "^$key *= *$value" "$file"; then
        return 1 # already set correctly
    fi

    new_line="$key = $value"
    if grep -q "^$key[ =]" "$file"; then
        # change existing value
        sed -i "s/^$key *=.*/$new_line/" "$file"
    elif grep -q "^# *$key[ =]" "$file"; then
        # uncomment existing setting
        sed -i "s/^# *$key *=.*/$new_line/" "$file"
    else
        # add new setting
        echo "$new_line" >> "$file"
    fi

    return 0
}

# bring up lonely_node(s) in the admin network
function setuplonelynodes()
{
    local i
    for i in $lonelynodeids; do
        local mac
        local i2=$( printf %02x $i )
        mac="52:54:$i2:88:77:$i2"
        local lonely_node
        lonely_node=$cloud-node$i
        ${mkcloud_lib_dir}/libvirt/compute-config $cloud $i $mac "$cephvolume" "$drbdvolume" $compute_node_memory $controller_node_memory $libvirt_type $vcpus $emulator $vdisk_dir "1" > /tmp/$cloud-node$i.xml

        local lonely_disk
        lonely_disk="$vdisk_dir/${cloud}.node$i"

        if [ -n "$want_sles12" ] ; then
            onhost_deploy_image "lonely" "SLE12" $lonely_disk
        else
            onhost_deploy_image "lonely" "SLE11" $lonely_disk
        fi

        ${mkcloud_lib_dir}/libvirt/vm-start /tmp/${lonely_node}.xml
    done
}

# register lonely_node against crowbar
function crowbar_register()
{
    for i in $lonelynodeids; do
        local i2=$( printf %02x $i )
        local mac="52:54:$i2:88:77:$i2"
        sshrun "lonelymac=$mac adminip=$adminip onadmin_crowbar_register"
    done
}

function prepareinstcrowbar()
{
    echo "connecting to crowbar admin server at $adminip"
    onadmin prepareinstallcrowbar
    return $?
}

function scp_install_chef_suse_override()
{
    if [ -e "$install_chef_suse_override" ]; then
        $scp -p "$install_chef_suse_override" \
            root@$adminip:/tmp/install-chef-suse.sh
    fi
}

function instcrowbar()
{
    scp_install_chef_suse_override
    onadmin installcrowbar
    local ret=$?
    $scp root@$adminip:screenlog.0 "$artifacts_dir/screenlog.0.install-suse-cloud"
    return $ret
}

function instcrowbarfromgit()
{
    scp_install_chef_suse_override
    safely rsync -av --exclude ".git" --exclude ".ci-tracking" ./crowbar root@$adminip:/root/
    onadmin installcrowbarfromgit
    local ret=$?
    $scp root@$adminip:screenlog.0 "$artifacts_dir/screenlog.0.install-suse-cloud"
    return $ret
}

function mkvlan()
{
    local DEFVLAN=$1 ; shift
    local IP=$1 ; shift
    vconfig set_name_type DEV_PLUS_VID_NO_PAD
    vconfig add $cloudbr $DEFVLAN
    ifconfig $cloudbr.$DEFVLAN $IP/24
    ethtool -K $cloudbr tx off
}

function hypervisor_has_virtio()
{
    local llibvirt_type=${1:-$libvirt_type}
    [[ "$llibvirt_type" = "xen" || "$llibvirt_type" = "hyperv" ]] && return 1
    return 0
}

function setuppublicnet()
{
    # workaround https://bugzilla.novell.com/show_bug.cgi?id=845496
    echo 0 > /proc/sys/net/bridge/bridge-nf-call-iptables
    mkvlan $vlan_public $net_public.1
    if [[ -n $mkch_physcloudif ]] ; then
        brctl addif $cloudbr $mkch_physcloudif
        ip link set dev $mkch_physcloudif up
    fi
}

function shutdowncloud()
{
    virsh shutdown $cloud-admin
    for i in $allnodeids ; do
        virsh shutdown $cloud-node$i
    done
}

function restartcloud()
{
    virsh net-start $cloud-admin
    virsh start $cloud-admin
    setuppublicnet
    for i in $allnodeids ; do
        virsh start $cloud-node$i
    done
}

# bring up VMs that will take cloud controller/compute/storage roles
function setupnodes()
{
    setuppublicnet
    for i in $allnodeids_without_lonely ; do
        i2=$( printf %02x $i )
        macaddress="52:54:$i2:77:77:$i2"

        # transport drdb volume information to admin node (needed for proposal of data cluster)
        drbd_serial=""
        if [ $drbd_hdd_size != 0 ]; then
            if [ $i -le 2 ] ; then
                drbd_serial="$cloud-node$i-drbd"
                drbdnode_mac_vol="${drbdnode_mac_vol}+${macaddress}#${drbd_serial}"
                drbdnode_mac_vol="${drbdnode_mac_vol#+}"
            fi
        fi

        ${mkcloud_lib_dir}/libvirt/compute-config $cloud $i $macaddress $cephvolumenumber "$drbd_serial" $compute_node_memory $controller_node_memory $libvirt_type $vcpus $emulator $vdisk_dir "3" > /tmp/$cloud-node$i.xml
        ${mkcloud_lib_dir}/libvirt/vm-start /tmp/$cloud-node$i.xml
    done

    echo "========================================================"
    echo " Note: If you interrupt mkcloud now and want to proceed"
    echo "       later, make sure to run the following command:"
    echo
    echo " export drbdnode_mac_vol=\"${drbdnode_mac_vol}\""
    echo
    echo "========================================================"
    sleep 10

    # mkch_physwol can be set by the user to a space-separated list of MAC-addrs
    # of pysical nodes to wake up using WoL
    for i in $mkch_physwol ; do
        ether-wake -i $cloudbr $i
    done
    return 0
}

# allocate cloud nodes with an operating system
# and wait for nodes to reach the ready state
function instnodes()
{
    onadmin allocate ||\
        return $?

    echo "Waiting for the installation of the nodes ..."
    onadmin waitcloud
    return $?
}


function proposal()
{
    onadmin proposal
    return $?
}

function testsetup()
{
    onadmin testsetup
    local ret=$?
    mkdir -p "$artifacts_dir"
    $scp root@$adminip:tempest*.log "$artifacts_dir/"

    # Register the cloud in Rally and import the results
    if [[ $rally_server && -f $artifacts_dir/tempest.subunit.log ]] ; then
        local title=$(testname)
        local type=$(testtype)

        $scp root@$adminip:.openrc "$artifacts_dir/"
        local tmpdir=`ssh $sshopts rally@$rally_server "mktemp -d rally-XXXXXX"`
        $scp "$artifacts_dir/tempest.subunit.log" "$artifacts_dir/.openrc" rally@$rally_server:$tmpdir
        ssh $sshopts rally@$rally_server "
            source rally/bin/activate
            source $tmpdir/.openrc
            deployment=\$(rally deployment create --fromenv --name=\"$title\" | awk '/Using deployment:/{print \$3}')
            rally verify start \"$type\" --tempest-log=$tmpdir/tempest.subunit.log --deployment=\$deployment
            rm -fr $tmpdir
        "
    fi

    return $ret
}

function testname()
{
    local -a var_names=(ha cluster upgrade ceph cinder drbd net
        netmode sles12 dvr docker)
    local -a var_values=("$hacloud" "$clusterconfig"
        "$upgrade_cloudsource" "$cephvolumenumber" "$cinder_backend"
        "$drbd_hdd_size" "$networkingplugin" "$networkingmode"
        "$want_sles12" "$want_dvr" "$want_docker")

    if [[ ${#var_names[@]} != ${#var_values[@]} ]] ; then
        complain 123 "testname error: arrays of different length, please check the code"
    fi

    local name="$cloudsource"
    local i=0
    for n in ${var_names[@]} ; do
        if [[ ${var_values[$i]} ]] ; then
            name="$name/$n:${var_values[$i]}"
        fi
        i=$(($i+1))
    done

    # Check if this instance of mkcloud is running inside Jenkins
    if [[ $JENKINS_URL ]] ; then
        name="$BUILD_NUMBER - $name - $BUILD_URL"
    fi

    echo $name
}

function testtype()
{
    # If `tempestoptions` contains '-s' or '--smoke' parameter, is a
    # smoke test.  If there is '--load-list 2015.03.required.txt' is a
    # defcore test.  A full test is supossed by default.
    local type="full"
    if [[ $tempestoptions ]] ; then
        if [[ $tempestoptions =~ -s ]] ; then
            type="smoke"
        elif [[ $tempestoptions =~ --load-list.*.required.txt ]] ; then
            type="defcore"
        elif [[ $tempestoptions =~ --load-list ]] ; then
            type="ad-hoc"
        fi
    fi

    echo $type
}

function rebootcrowbar()
{
    # reboot the crowbar instance
    #  and test if everything is up and running afterwards
    sshrun "reboot"
    wait_for 50 3 "! nc -z $adminip 22" 'crowbar to go down'
    wait_for_crowbar_ssh
    echo "waiting another 180 seconds for services"
    sleep 180
    sshrun "mount -a -t nfs" # workaround repos not mounted on reboot because NFS needs bind, but bind says it requires NFS
    return $?
}

function rebootcloud()
{
    # reboot compute nodes
    #  and test if everthing is up and running afterwards
    onadmin rebootcloud
    return $?
}

function rebootcompute()
{
    echo "WARNING: called deprecated rebootcompute step" >&2
    rebootcloud
    return $?
}

function rebootneutron()
{
    onadmin rebootneutron
    return $?
}

function qa_test()
{
    local ghsc=github.com/SUSE-Cloud
    mkdir -p ~/$ghsc/
    pushd ~/$ghsc/
    if [ -e "qa-openstack-cli" ] ; then
        cd qa-openstack-cli/
        git pull
    else
        git clone https://$ghsc/qa-openstack-cli.git
    fi
    popd
    rsync -av ~/$ghsc/qa-openstack-cli root@$adminip:
    onadmin qa_test
    ret=$?

    mkdir -p .artifacts
    $scp -r root@$adminip:qa_test.logs/ .artifacts/
    return $ret
}

function crowbarbackup()
{
    onadmin crowbarbackup
    local ret=$?
    safely $scp root@$adminip:/tmp/backup-crowbar.tar.gz .
    [ -d "$artifacts_dir" ] && mv backup-crowbar.tar.gz "$artifacts_dir/"
    return $ret
}

function crowbarrestore()
{
    local btarball=backup-crowbar.tar.gz
    [ -e "$artifacts_dir/$btarball" ] && btarball="$artifacts_dir/$btarball"
    if [ ! -e "$btarball" ] ; then
        complain 56 "No crowbar backup tarball found."
    fi
    $scp "$btarball" root@$adminip:/tmp/
    onadmin crowbarpurge && onadmin crowbarrestore
    return $?
}

function prepare_cloudupgrade()
{
    onadmin prepare_cloudupgrade
    return $?
}

function cloudupgrade_1st()
{
    onadmin cloudupgrade_1st
    return $?
}

function cloudupgrade_2nd()
{
    onadmin cloudupgrade_2nd
    return $?
}

function cloudupgrade_clients()
{
    onadmin cloudupgrade_clients
    return $?
}

function cloudupgrade_reboot_and_redeploy_clients()
{
    onadmin cloudupgrade_reboot_and_redeploy_clients
    return $?
}

function cct()
{
    onadmin run_cct "$@"
    return $?
}

function show_steps()
{
    cat <<EOSTEPS
Usage:
$0 <step> [<step>,...]

'step' is one of:
    $allcmds

These steps are expanding to the following steps:
    all
        -> $(expand_steps all)
    all_noreboot
        -> $(expand_steps all_noreboot)
    plain
        -> $(expand_steps plain)
    plain_with_upgrade
        -> $(expand_steps plain_with_upgrade)
    instonly
        -> $(expand_steps instonly)

Steps:
    cleanup:        kill all running VMs, zero out boot sectors of all LVM volumes
    prepare:        create LVM volumes, setup libvirt networks
    setupadmin:     create the admin node and install the cloud product
    prepareinstcrowbar: add repos and install crowbar packages
    instcrowbar:    install crowbar and chef on the admin node
    setupnodes:     create the nodes and let crowbar discover them
    instnodes:      allocate and install compute nodes
    proposal:       create and apply proposals for default setup
    setuplonelynodes: boot a number (defined by nodenumberlonelynodes) of non-crowbar registered nodes in the admin network
    crowbar_register: register a number (defined by nodenumberlonelynodes) of non-crowbar nodes with crowbar (setuplonelynodes needs to have run before)
    testsetup:      start a VM in the cloud
    addupdaterepo:  addupdate repos defined in UPDATEREPOS= (URLs separated by '+')
    runupdate:      run zypper up on the crowbar node
                    (compute nodes are automaticallyupdated via chef run)
    rebootcrowbar:  reboot the crowbar instance and wait for it being up
    rebootcloud:    reboot the cloud nodes and wait for them being up
    restartcloud:   start a pre-existing cloud again after host reboot
    createadminsnapshot: create snapshot of admin node disk
    restoreadminfromsnapshot: restore admin node disk from snapshot
    qa_test:        run the qa test suite
    help:           usage of steps and environment variables
    steps:          usage of steps only

EOSTEPS
}

function steps() {
    show_steps
    exit 1
}

function usage()
{
    show_steps
    cat <<EOUSAGE


Environment variables (need to be exported):

Mandatory
    cloudvg=vg0
        set the volume group name to create lvm volumes in
        Cloud volumes will be prefixed with the cloud name.
        The cleanup function will only cleanup volumes with this prefix.
    cloudpv=/dev/vdx (default /dev/vdb)
        Device where a LVM physical volume will be created, all data lost on this device
        Should be at least 80 GB. The volume group will be called 'cloud'.
    cloudsource=develcloud4 | develcloud5 | develcloud6 | susecloud6 | GM4 | GM4+up | GM5 | GM5+up | GM6 | GM6+up | M?  (default '')
        NOTE: The latest version always is in development. So do NOT expect it to work out of the box.
        NOTE: If you need a stable/working version use <latest-version>-1.
        defines the installation source of the product
        develcloud4   : product from IBS Devel:Cloud:4
        develcloud5   : product from IBS Devel:Cloud:5
        develcloud6   : product from IBS Devel:Cloud:6
        susecloud6    : product from IBS SUSE:SLE....
        GM4           : SUSE Cloud Goldmaster 4 without updates
        GM5           : SUSE Cloud Goldmaster 5 without updates
        GM6           : SUSE Cloud Goldmaster 6 without updates
        GM4+up        : SUSE Cloud Goldmaster 4 with released maintenance updates
        GM5+up        : SUSE Cloud Goldmaster 5 with released maintenance updates
        GM6+up        : SUSE Cloud Goldmaster 6 with released maintenance updates
        M?            : uses official Milestone? ISO image (? is a number)

Optional
    qa_crowbarsetup='path/to/script' (default: same directory as mkcloud is located in)
        set an optional path to qa_crowbarsetup.sh
    hacloud='' | 1  (default='')
        Set up a highly available cloud; requires to configure the clusters via clusterconfig='...'
    clusterconfig
        A string with a cluster configuration. The services for data, network and services cluster can
        be deployed in 1, 2 or 3 clusters. The configuration string looks like this:
        The string is: '<group1>:<group2>' (a ':' separated list of groups).
        A group is:    'clustername1+clustername2=clusternodenumber
        The first clustername of a group defines the name of the cluster.
        Examples:
            3 clusters: clusterconfig='data=2:network=3:services=5'
            2 clusters: clusterconfig='services+data=2:network=3'
            1 cluster:  clusterconfig='data+network+services=2'
    upgrade_cloudsource='' (default='')
        set new cloudsource for upgrade process
    TESTHEAD='' | 1  (default='')
        use Media from Devel:Cloud:Staging and add test update repositories (except for GM* targets)
    cephvolumenumber  (default=1)
        the number of ceph volumes that will be created per node
        note: proposal step does contain a ceph proposal only in SUSE Cloud 4, in
        SUSE OpenStack Cloud 5 do it manually
    cephvolume_hdd_size (default 21)
        Set the size in GB of data disk attached in compute nodes with enabled 'cephvolumenumber'
    controller_ceph_hdd_size (default 25)
        Set the size in GB of data disk attached in controller node  with enabled 'cephvolumenumber'
    nodenumber=2    (default 2)
        set the number of nodes to be created for the cloud (excluding admin node)
        In HA mode (hacloud=1) the nodes needed for clusters are subtracted from nodenumber; the
        remaining nodes are compute nodes.
    nodenumberlonelynode=2    (default 0)
        set the number of non-crowbar registered nodes to be created in the admin network
    nodenumbercompute=1  (default $nodenumber resp. 1 in case of hacloud=1)'
        set the number of compute nodes in a HA setup, the remaining nodes
        of $nodenumber - $nodenumbercompute are used as HA cluster nodes
    vcpus=1         (default 1)
        set the number of CPU cores per compute node
    adminvcpus=1    (default $vcpus)
        set the number of CPU cores for admin node
    admin_node_memory (default 2097152)
        Set the memory in KB assigned to admin node
    controller_node_memory (default 5242880)
        Set the memory in KB assigned to compute nodes
    compute_node_memory (default 2097152)
        Set the memory in KB assigned to compute nodes
    drbd_hdd_size  (default 0, or 50 if hacloud is set)
        Set the size in GB of the DRBD data disks attached to the
        nodes in the cluster hosting the database and rabbitmq.
    drbd_database_size (default 20)
        Set the size in GB of the DRBD LV to request the database barclamp
        to set up within the DRBD data disks attached to the nodes in the
        cluster hosting the database.
    drbd_rabbitmq_size (default 20)
        Set the size in GB of the DRBD LV to request the rabbitmq barclamp
        to set up within the DRBD data disks attached to the nodes in the
        cluster hosting RabbitMQ.
    networkingplugin
        Set the networking plugin to be used by neutron (e.g. openvswitch),
        if it isn't defined the barclamp-neutron's default is used.
    networkingmode
        Set the networking mode to be used by neutron (e.g. gre)
        if it isn't defined the barclamp-neutron's default is used.
    keep_existing_hostname=1    (default='')
        If this option is enabled crowbar_register keeps the existing
        hostname. When the crowbar_register option is enabled too then
        it generates and sets random hostnames.
    debug_mkcloud=1  (default 0)
        enable debug mode for mkcloud via 'set -x'
    debug_qa_crowbarsetup=1 (default 0)
        enable debug mode for qa_crowbarsetup.sh via 'set -x'
    debug_openstack=1 (default 0)
        enable debug mode for the openstack components
        sets debug true in the openstack proposals
    user_keyfile='path/to/file'
        path to optional user public ssh keyfile to inject into crowbar to
        /root/.ssh/authorized_keys file and to provisioner barclamp
    want_sles12=1 (default 0)
        setup SLE12 compute nodes
    want_dvr=1 (default='')
        if DVR support should be turned on by neutron barclamp. Only works with openvswitch and vxlan.
    want_docker=1 (default='')
        Deploy docker image (upload image to glance, create instance based on it etc.).
        Only possible with SLE12 node.
    install_chef_suse_override='path/to/script'
        Optional path to an alternate version of install-chef-suse.sh on the mkcloud hostto use
        instead of the one from the packages.  This will be scp'd to the admin node before use.
    vlan_public=<id> (default 300)
        VLAN id for public network
    tempestoptions (default='-t -s')
        parameters passed to run_tempest.sh script
    rally_server (default='')
        rally server address
    cct_tests='test1+test2' (default='features')
        Defines which cct_tests should be run while the cct step.
EOUSAGE
    onadmin_help
    exit 1

# UNDOCUMENTED OPTIONS:
#
# virtualcloud
# cloudfqdn
# forwardmode
# net_public
# net_admin
# adminnetmask
# adminip
# admingw
# cpuflags
# admin_node_memory
# controller_node_memory
# compute_node_memory
# hyperv_node_memory
# adminnode_hdd_size
# controller_hdd_size
# computenode_hdd_size
# cloudbr
# libvirt_type
# localreposdir_src
# localreposdir_target

}


function addupdaterepo()
{
    sshrun "UPDATEREPOS=$UPDATEREPOS onadmin_addupdaterepo"
    return $?
}

function runupdate()
{
    onadmin runupdate
    return $?
}

function is_concurrent_run()
{
    [ -e $pidfile ] && kill -0 `cat $pidfile` 2>/dev/null && return 0
    echo $$ > $pidfile
    return 1
}

function sanity_checks()
{
    if test `id -u` != 0 ; then
        complain 1 "This script needs to be run as root" \
            "Please be aware that this script will create a LVM" \
            "and kill all current VMs on this host."
    fi

    if is_concurrent_run; then
        complain 33 "mkcloud was started twice from same working directory: `pwd`" \
            "Please always use a separate working directory for each (parallel) mkcloud run."
    fi

    # test for existence of qa_crowbarsetup.sh
    if [ ! -e $qa_crowbarsetup ] ; then
        complain 87 "$qa_crowbarsetup not found
            Please define the path to it by setting qa_crowbarsetup=/path/to/file
            or call mkcloud from your git clone."
    fi

    if [ -z "$cloudsource" ] ; then
        echo "Please set the env variable:"
        echo "export cloudsource=M?|develcloud4|develcloud5|develcloud6|susecloud6|GM4|GM4+up|GM5|GM5+up"
        exit 1
    fi

    # checking clusterconfig
    if [ -n "$hacloud" -a -z "$clusterconfig" ] ; then
        echo "Examples for clusterconfig:"
        echo '3 clusters: clusterconfig="data=2:network=3:services=3"'
        echo '2 clusters: clusterconfig="services+data=2:network=3"'
        echo '1 cluster:  clusterconfig="data+network+services=2"'
        complain 70 "No cluster config provided for HA setup."
    fi

    vgdisplay "$cloudvg" >/dev/null 2>&1 && needcvol=
    if [ -n "$needcvol" ] ; then
        : ${cloudpv:=/dev/vdb}
        if grep -q $cloudpv /proc/mounts ; then
            complain 92 "The device $cloudpv seems to be used. Exiting."
        fi
        if [ ! -e $cloudpv ] ; then
            complain 93 "$cloudpv does not exist." \
                "Please set the cloud volume group to an existing device: export cloudpv=/dev/sdx" \
                "Running 'partprobe' may help to let the device appear."
        fi
    fi

    if [ -e /etc/init.d/SuSEfirewall2_init ] && rcSuSEfirewall2 status ; then
        complain 91 "SuSEfirewall is running - it will interfere with the iptables rules done by libvirt" \
            "Please stop the SuSEfirewall completely and run mkcloud again" \
            "Run:  rcSuSEfirewall2 stop && insserv -r SuSEfirewall2_setup && insserv -r SuSEfirewall2_init"
    fi

    if grep "devpts.*[^x]mode=.00" /proc/mounts ; then
        complain 13 "/dev/pts is not accessible for libvirt, maybe you use autobuild on your system." \
            "Please remount it using the following command:" \
            " # mount -o remount,mode=620,gid=5 devpts -t devpts /dev/pts"
    fi

    if [[ $wantedcmds =~ "setuplonelynodes" || $wantedcmds =~ "crowbar_register" ]] ; then
        if [[ -z "$nodenumberlonelynode" || $nodenumberlonelynode -lt 1 ]] ; then
            complain 80 "Please set nodenumberlonelynode."
        fi
    fi

    case $cinder_backend in
        ''|netapp|local|raw|rbd)
            ;;
        *)
            complain 101 "$cinder_backend as the cinder backend is currently not supported."
            ;;
    esac

    # allow cloud instances to get responses from dnsmasq
    # by preventing libvirt to tell it to bind only to the bridge interface
    # see also PR #290
    grep -q -- --bind-dynamic /usr/lib*/libvirt.so.0 \
        && complain 111 "please do: sed -i.orig -e 's/--bind-dynamic/--bindnotthere/g' /usr/lib*/libvirt.so.0 ; rclibvirtd restart
        This is needed for VMs to have DNS, but might have a security impact
        if the machine has a global public IP
        see bnc#928384 and CVE-2012-3411"

    if [[ $want_sles12 = 1 ]] && [[ $want_ceph = 1 ]] && [[ $nodenumber -lt 3 ]] ; then
        complain 113 "Please increase number of nodes for this setup, minimal nodenumber=3"
    fi
}


## MAIN ##
step_aliases="_new_admin _compute _upgrade _testupdate"

allcmds="$step_aliases all all_noreboot instonly plain \
    plain_with_upgrade cleanup setuphost prepare setupadmin \
    prepareinstcrowbar instcrowbar instcrowbarfromgit setupnodes \
    setupcompute instnodes instcompute proposal testsetup rebootcrowbar \
    rebootcloud addupdaterepo runupdate testupdate \
    crowbarbackup crowbarrestore shutdowncloud restartcloud qa_test help \
    rebootneutron prepare_cloudupgrade cloudupgrade_1st cloudupgrade_2nd \
    cloudupgrade_clients cloudupgrade_reboot_and_redeploy_clients \
    setuplonelynodes crowbar_register createadminsnapshot \
    restoreadminfromsnapshot cct steps"
wantedcmds=$@

function expand_steps()
{
    # parse the commands and expand the aliases
    local runcmds=''
    local localwantedcmds=$@
    local cmd
    for cmd in $localwantedcmds ; do

        local found=0
        local onecmd
        for onecmd in $allcmds ; do
            if [[ $cmd =~ ^$onecmd(\+.+)?$ ]] ; then
                found=1
                case "$cmd" in
                    setupcompute|instcompute)
                        corrected=${cmd/compute/nodes}
                        echo "WARNING: the '$cmd' step is deprecated; use '$corrected' instead." >&2
                        cmd=$corrected
                        ;;
                esac

                case "$cmd" in
                    _new_admin)
                        runcmds="$runcmds cleanup prepare setupadmin addupdaterepo runupdate prepareinstcrowbar instcrowbar"
                    ;;
                    _compute)
                        runcmds="$runcmds setupnodes instnodes proposal"
                    ;;
                    all)
                        runcmds="$runcmds `expand_steps _new_admin` rebootcrowbar `expand_steps _compute` cct testsetup rebootcloud"
                    ;;
                    all_noreboot)
                        runcmds="$runcmds `expand_steps _new_admin` `expand_steps _compute` cct testsetup"
                    ;;
                    _testupdate|testupdate)
                        runcmds="$runcmds addupdaterepo runupdate cct testsetup"
                    ;;
                    plain)
                        runcmds="$runcmds `expand_steps instonly` proposal"
                    ;;
                    instonly)
                        runcmds="$runcmds cleanup prepare setupadmin prepareinstcrowbar instcrowbar setupnodes instnodes"
                    ;;
                    _upgrade)
                        runcmds="$runcmds prepare_cloudupgrade cloudupgrade_1st cloudupgrade_2nd cloudupgrade_clients cloudupgrade_reboot_and_redeploy_clients"
                    ;;
                    plain_with_upgrade)
                        runcmds="$runcmds `expand_steps plain` addupdaterepo runupdate `expand_steps _upgrade`"
                    ;;
                    *)
                        runcmds="$runcmds $cmd"
                    ;;
                esac
            fi
        done
        [ $found == 0 ] && complain - "Step $cmd not found." && return

    done
    runcmds=${runcmds## }
    runcmds=${runcmds%% }
    echo "${runcmds//  / }"
}

steplist=`expand_steps $wantedcmds`
[[ ! $steplist ]] || [[ "$steplist" =~ "help" ]] && usage
[[ "$steplist" =~ "steps" ]] && steps

sanity_checks

echo "You choose to run these mkcloud steps:"
echo "  $steplist"
echo
sleep 2

for cmd in `echo $steplist` ; do
    echo
    echo "============> MKCLOUD STEP START: $cmd <============"
    echo
    sleep 2
    echo $cmd >> mkcloud.steps.log
    # support calls to functions with parameters
    # this is currently used for the cct function
    cmd_parameters="${cmd#*+}"
    cmd=${cmd%%+*}
    $cmd "${cmd_parameters//+/ }"
    ret=$?
    if [ $ret != 0 ] ; then
        set +x
        echo
        echo '$h1!!'
        echo "Error detected. Stopping mkcloud."
        echo "The step '$cmd' returned with exit code $ret"
        echo "Please refer to the $cmd function in this script when debugging the issue."
        error_exit $ret ""
    fi >&2
    echo
    echo "^^^^^^^^^^^^= MKCLOUD STEP DONE: $cmd =^^^^^^^^^^^^"
    echo
done

pre_exit_cleanup
show_environment
