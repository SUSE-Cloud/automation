#!/bin/bash
#
# mkcloud - Setup a virtual cloud on one system (physical or even virtual)
#
# Authors: J. Daniel Schmidt <jdsn@suse.de>
#          Bernhard M. Wiedemann <bwiedemann@suse.de>
#
# 2012, SUSE LINUX Products GmbH
#

# Quick introduction:
#
# This tool relies on the script qa_crowbarsetup.sh (in the same directory)
# Please 'export' environment variables according to your needs:
#
# CVOL=/dev/vdx  (default=/dev/vdb)
#       device where a LVM physical volume can be created
#       should be at least 80 GB
#       the volume group will be called "cloud"
#
# cloudsource=develcloud|susecloud|Beta?   (required, no default)
#       defines the source for the installation of the cloud product
#       develcloud    : product from IBS Devel:Cloud:1.0
#       develcloud2.0 : product from IBS Devel:Cloud:2.0
#       develcloud3   : product from IBS Devel:Cloud:3
#       susecloud     : product from IBS SUSE:SLE....
#       Beta?         : uses official Beta? ISO image (? is a number)
#
# TESTHEAD=''|1   (default='')
#                will use latest published packages from Devel:Cloud
#                even if there is no new ISO created yet
#
# cephvolumenumber (default 0)
#                sets the number of 5GB ceph volumes that will be created per node
#                for ceph testing
#
# nodenumber     (default 2)
#                sets the number of nodes to be created
#
# vcpus          (default 1)
#                sets the number of CPU cores assigned to each node (admin and compute)

adminip=192.168.124.10
nodenumber=${nodenumber:-2}
cephvolumenumber=${cephvolumenumber:-0}
allnodeids=`seq 1 $nodenumber`
vcpus=${vcpus:-1}
cpuflags=''
working_dir_orig=`pwd`
artifacts_dir=${artifacts_dir:-$working_dir_orig/.artifacts}
start_time=`date`
cloudbr=cloudbr1
mnt=/tmp/cloudmnt
debug=${debug:-0}

trap 'error_exit $? "error caught by trap"' TERM

function show_environment()
{
  end_time=`date`
  echo "Environment Details"
  echo "-------------------------------"
  echo "    hostname: `hostname -f`"
  echo "     started: $start_time"
  echo "       ended: $end_time"
  echo "-------------------------------"
  echo " cloudsource: $cloudsource"
  echo "    TESTHEAD: $TESTHEAD"
  echo "  nodenumber: $nodenumber"
  echo "        CVOL: $CVOL"
  echo " UPDATEREPOS: $UPDATEREPOS"
  echo " cephvolumenumber: $cephvolumenumber"
  echo "-------------------------------"
  env | grep -i "_with_ssl"
  echo "-------------------------------"
}

function error_exit()
{
	exitcode=$1
	message=$2
	ssh root@crowbar '
	  for node in $(crowbar machines list | grep ^d) ; do
	    # exclude /proc, bnc#829927    
	    ssh $node supportconfig -x PROC | wc
	    scp $node:/var/log/\*tbz /var/log/
	  done
	  supportconfig | wc
	'
    mkdir -p $artifacts_dir
	scp root@crowbar:/var/log/*tbz $artifacts_dir/
	echo $message
	show_environment
	exit $exitcode
}

function wait_for()
{
  timecount=${1:-300}
  timesleep=${2:-1}
  condition=${3:-'/bin/true'}
  waitfor=${4:-'unknown process'}

  echo "Waiting for: $waitfor"
  n=$timecount
  while test $n -gt 0 && ! eval $condition
  do
    echo -n .
    sleep $timesleep
    n=$(($n - 1))
  done
  echo

  if [ $n = 0 ] ; then
    echo "Error: Waiting for '$waitfor' timed out."
    echo "This check was used: $condition"
    exit 11
  fi
}


function sshrun()
{
  ssh root@$adminip "export cephvolumenumber=$cephvolumenumber ; export shell=$shell ; export nodenumber=$nodenumber ; export cloudsource=$cloudsource ; export TESTHEAD=$TESTHEAD ; export all_with_ssl=$all_with_ssl ; export keystone_with_ssl=$keystone_with_ssl ; export glance_with_ssl=$glance_with_ssl ; export nova_with_ssl=$nova_with_ssl ; export novadashboard_with_ssl=$novadashboard_with_ssl ; export networkingplugin=$networkingplugin ; export debug=$debug ; export nosetestparameters=${nosetestparameters} ; $@"
  return $?
}


function cleanup()
{
  # cleanup leftover from last run
  allnodenames=`for i in $allnodeids ; do echo -n "node\$i " ; done`
  for n in admin $allnodenames ; do
      virsh destroy cloud-$n
      virsh undefine cloud-$n
  done
  virsh net-destroy cloud-admin
  ifdown $cloudbr
  brctl delbr $cloudbr
  tunctl -d ${cloudbr}-nic
  # zero node volumes to prevent accidental booting
  for node in $allnodeids ; do
    dd if=/dev/zero of=/dev/cloud/node$node count=1 bs=8192
  done
  umount $mnt
  for i in /dev/loop* ; do
    if losetup $i | grep -q cloud-admin ; then
      losetup -d $i
    fi
  done
  rm -f /var/run/libvirt/qemu/cloud*.xml /var/lib/libvirt/network/cloud*.xml
  return 0
}

function h_create_cloud_lvm()
{
  pvcreate $CVOL
  vgcreate cloud $CVOL
  vgchange -ay cloud # for later boots

  # hdd size defaults (unless defined otherwise)
  adminnode_hdd_size=${adminnode_hdd_size:-15}
  computenode_hdd_size=${computenode_hdd_size:-20}
  cephvolume_hdd_size=${cephvolume_hdd_size:-5}

  lvcreate -n admin -L ${adminnode_hdd_size}G cloud
  for i in $allnodeids ; do
    lvcreate -n node$i -L ${computenode_hdd_size}G cloud
  done

  if [ $cephvolumenumber -gt 0 ] ; then
    for i in $allnodeids ; do
      for n in $(echo `seq 1 $cephvolumenumber`) ; do
        lvcreate -n node$i-ceph$n -L ${cephvolume_hdd_size}G cloud
      done
    done
  fi
}

function h_deploy_admin_image()
{
  pushd /tmp
  wget --progress=dot:mega -nc http://clouddata.cloud.suse.de/images/$adminimage-64up.qcow2
  qemu-img convert -p $adminimage-64up.qcow2 /dev/cloud/admin
  popd

  if fdisk -l /dev/cloud/admin | grep -q 2056192 ; then
    # make a bigger partition 2
    echo -e "d\n2\nn\np\n2\n\n\na\n2\nw" | fdisk /dev/cloud/admin
    loopdevice=`losetup --find`
    losetup -o $(expr 2056192 \* 512) $loopdevice /dev/cloud/admin
    fsck -y -f $loopdevice
    resize2fs $loopdevice
    sync
    losetup -d $loopdevice
  fi
}

function h_add_etchosts_entries()
{
  grep -q crowbar /etc/hosts || echo "$adminip crowbar.virtual.cloud.suse.de crowbar" >> /etc/hosts
}

function prepare()
{
  zypper --non-interactive in --no-recommends libvirt kvm lvm2 curl wget bridge-utils dnsmasq netcat-openbsd ebtables

  echo "Creating key for controlling our VMs..."
  [ -e ~/.ssh/id_dsa ] || ssh-keygen -t dsa -f ~/.ssh/id_dsa -N ""

  grep -q NumberOfPasswordPrompts ~/.ssh/config 2>/dev/null || cat > ~/.ssh/config <<EOSSH
Host crowbar $adminip
NumberOfPasswordPrompts 0
UserKnownHostsFile /dev/null
StrictHostKeyChecking no
EOSSH

  h_create_cloud_lvm
  h_deploy_admin_image
  h_add_etchosts_entries
}

function ssh_password()
{
  SSH_ASKPASS=/root/echolinux
  cat > $SSH_ASKPASS <<EOSSHASK
#!/bin/sh
echo linux
EOSSHASK
  chmod +x $SSH_ASKPASS
  DISPLAY=dummydisplay:0 SSH_ASKPASS=$SSH_ASKPASS setsid ssh -oNumberOfPasswordPrompts=1 "$@"
}

function h_cpuflags_settings()
{ # used for admin and compute nodes
  cpuflags="<cpu match='minimum'>
      <model>qemu64</model>
      <feature policy='require' name='fxsr_opt'/>
      <feature policy='require' name='mmxext'/>
      <feature policy='require' name='lahf_lm'/>
      <feature policy='require' name='sse4a'/>
      <feature policy='require' name='abm'/>
      <feature policy='require' name='cr8legacy'/>
      <feature policy='require' name='misalignsse'/>
      <feature policy='require' name='popcnt'/>
      <feature policy='require' name='pdpe1gb'/>
      <feature policy='require' name='cx16'/>
      <feature policy='require' name='3dnowprefetch'/>
      <feature policy='require' name='cmp_legacy'/>
      <feature policy='require' name='monitor'/>
    </cpu>"
  grep -q "flags.* npt" /proc/cpuinfo || cpuflags=""

  if grep -q "vendor_id.*GenuineIntel" /proc/cpuinfo; then
      cpuflags="<cpu mode='custom' match='exact'>
      <model fallback='allow'>core2duo</model>
      <feature policy='require' name='vmx'/>
    </cpu>"
  fi
}


function h_create_libvirt_adminnode_config()
{
  h_cpuflags_settings

  cat > $1 <<EOLIBVIRT
  <domain type='kvm'>
    <name>cloud-admin</name>
    <uuid>169738f2-63a1-3fef-2ee8-1fef03498574</uuid>
    <memory>2097152</memory>
    <currentMemory>2097152</currentMemory>
    <vcpu>$vcpus</vcpu>
    <os>
      <type arch='x86_64' machine='pc-0.14'>hvm</type>
      <boot dev='hd'/>
    </os>
    <features>
      <acpi/>
      <apic/>
      <pae/>
    </features>
    $cpuflags
    <clock offset='utc'/>
    <on_poweroff>preserve</on_poweroff>
    <on_reboot>restart</on_reboot>
    <on_crash>restart</on_crash>
    <devices>
      <emulator>/usr/bin/qemu-kvm</emulator>
      <disk type='block' device='disk'>
        <driver name='qemu' type='raw' cache='unsafe'/>
        <source dev='/dev/cloud/admin'/>
        <target dev='vda' bus='virtio'/>
        <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
      </disk>
      <interface type='network'>
        <mac address='52:54:00:77:77:70'/>
        <source network='cloud-admin'/>
        <model type='virtio'/>
        <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
      </interface>
      <serial type='pty'>
        <target port='0'/>
      </serial>
      <console type='pty'>
        <target type='serial' port='0'/>
      </console>
      <input type='mouse' bus='ps2'/>
      <graphics type='vnc' port='-1' autoport='yes'/>
      <video>
        <model type='cirrus' vram='9216' heads='1'/>
        <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
      </video>
      <memballoon model='virtio'>
        <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
      </memballoon>
    </devices>
  </domain>
EOLIBVIRT
}

function h_create_libvirt_admin_network_config()
{
  # dont specify range
  # this allows to use the same network for cloud-nodes that get DHCP from crowbar
  # doc: http://libvirt.org/formatnetwork.html
  cat > $1 <<EOLIBVIRTNET
  <network>
    <name>cloud-admin</name>
    <uuid>76b08f53-4fe0-3bb6-8220-d4cfc7b23423</uuid>
    <bridge name='$cloudbr' stp='off' delay='0' />
    <mac address='52:54:00:AB:B1:77'/>
    <ip address='192.168.124.1' netmask='255.255.248.0'>
      <dhcp>
        <host mac="52:54:00:77:77:70" name="crowbar.example.com" ip="$adminip"/>
      </dhcp>
    </ip>
    <forward mode='nat'>
    </forward>
  </network>
EOLIBVIRTNET
}


function setupadmin()
{
  echo "Injecting public key into image..."
  pubkey=`cut -d" " -f2 ~/.ssh/id_dsa.pub`
  mkdir -p $mnt
  mount -o loop,offset=$(expr 2056192 \* 512) /dev/cloud/admin $mnt
  mkdir -p $mnt/root/.ssh
  grep -q $pubkey $mnt/root/.ssh/authorized_keys 2>/dev/null || cat ~/.ssh/id_dsa.pub >> $mnt/root/.ssh/authorized_keys
  umount $mnt
  sync

  h_create_libvirt_adminnode_config /tmp/cloud-admin.xml

  h_create_libvirt_admin_network_config /tmp/cloud-admin.net.xml

  modprobe kvm-amd
  if [ ! -e /etc/modprobe.d/80-kvm-intel.conf ] ; then
    echo "options kvm-intel nested=1" > /etc/modprobe.d/80-kvm-intel.conf
    rmmod kvm-intel
  fi
  modprobe kvm-intel
  insserv libvirtd
  rclibvirtd start
  wait_for 300 1 '[ -S /var/run/libvirt/libvirt-sock ]' 'libvirt startup'

  [ -e /etc/libvirt/qemu/networks/cloud-admin.xml ] || virsh net-create /tmp/cloud-admin.net.xml
  virsh net-start cloud-admin
  if ! virsh create /tmp/cloud-admin.xml ; then
    echo "=====================================================>>"
    echo "Error: Could not create VM for: cloud-admin"
    exit 76
  fi

  wait_for 300 1 "ping -q -c 1 -w 1 $adminip >/dev/null" 'crowbar admin VM'

  if ! grep -q "iptables -t nat -F PREROUTING" /etc/init.d/boot.local; then
      nodehostips=$(echo `seq 81 $((80 + $nodenumber))`)
      cat >> /etc/init.d/boot.local <<EOS
iptables -t nat -F PREROUTING
for i in 22 80 443 3000 4000 4040 ; do
    iptables -I FORWARD -p tcp --dport \$i -j ACCEPT
    for host in 10 $nodehostips ; do
EOS
      cat >> /etc/init.d/boot.local <<'EOS'
        iptables -t nat -I PREROUTING -p tcp --dport $(($i + $host % 10 + 1100)) -j DNAT --to-destination 192.168.124.$host:$i
    done
done
iptables -t nat -I PREROUTING -p tcp --dport 6080 -j DNAT --to-destination 192.168.122.2
echo 0 > /proc/sys/net/ipv4/conf/all/rp_filter
EOS
  fi
  /etc/init.d/boot.local

  wait_for 150 1 "nc -z $adminip 22" 'starting ssh daemon'

  echo "waiting some more for sshd+named to start"
  sleep 25
  ssh_password $adminip "mkdir ~/.ssh ; grep -q $pubkey ~/.ssh/authorized_keys 2>/dev/null || echo ssh-dss $pubkey injected-key-from-host >> ~/.ssh/authorized_keys"
  echo "you can now proceed with installing crowbar"
}

function instcrowbar()
{
  echo "connecting to crowbar admin server at $adminip"
  # qa_crowbarsetup uses static network which needs static resolv.conf
  grep '^nameserver\s*[0-9]*\.' /etc/resolv.conf | sshrun dd of=/etc/resolv.conf
  scp -p install-chef-suse.sh root@$adminip:/tmp/
  scp qa_crowbarsetup.sh root@$adminip:
  sshrun "echo `hostname` > cloud ; installcrowbar=1 bash -x qa_crowbarsetup.sh virtual"
  return $?
}

function mkvlan()
{
  DEFVLAN=$1 ; shift
  IP=$1 ; shift
  cat > /etc/sysconfig/network/ifcfg-$cloudbr.$DEFVLAN <<EONET
# VLAN Interface for the xxx network
USERCONTROL='no'
STARTMODE='auto'
BOOTPROTO='static'
ETHERDEVICE='$cloudbr'
IPADDR='$IP/24'
VLAN_ID=$DEFVLAN
EONET

  ifup $cloudbr.$DEFVLAN
}


function h_create_libvirt_computenode_config()
{
  h_cpuflags_settings
  nodeconfigfile=$1
  nodecounter=$2
  cephvolume="$3"
  i2=$(printf %02x $nodecounter)

  cat > $nodeconfigfile <<EOLIBVIRT
<domain type='kvm'>
  <name>cloud-node$nodecounter</name>
  <uuid>169738f2-63$i2-3fef-2ee8-1fef03498574</uuid>
  <memory>2097152</memory>
  <currentMemory>2097152</currentMemory>
  <vcpu>$vcpus</vcpu>
  <os>
    <type arch='x86_64' machine='pc-0.14'>hvm</type>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  $cpuflags
  <clock offset='utc'/>
  <on_poweroff>preserve</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/bin/qemu-kvm</emulator>
    <disk type='block' device='disk'>
      <driver name='qemu' type='raw' cache='unsafe'/>
      <source dev='/dev/cloud/node$nodecounter'/>
      <target dev='vda' bus='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
      <boot order='2'/>
    </disk>
    $cephvolume
    <interface type='network'>
      <mac address='52:54:$i2:77:77:$i2'/>
      <source network='cloud-admin'/>
      <model type='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
      <boot order='1'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target type='serial' port='0'/>
    </console>
    <input type='mouse' bus='ps2'/>
    <graphics type='vnc' port='-1' autoport='yes'/>
    <video>
      <model type='cirrus' vram='9216' heads='1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
    <memballoon model='virtio'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </memballoon>
  </devices>
</domain>
EOLIBVIRT

}


function setupcompute()
{
  # public = 300
  mkvlan 300 192.168.122.1
  # nova-fixed = 500
  mkvlan 500 192.168.123.254

  alldevices=$(echo {b..z} {a..z}{a..z})
  for i in $allnodeids ; do
    c=1
    cephvolume=""
    if [ $cephvolumenumber -gt 0 ] ; then
      for n in $(echo `seq 1 $cephvolumenumber`) ; do
        dev=vd$(echo $alldevices | cut -d" " -f$c)
        c=$(($c + 1))
        cephvolume="$cephvolume
    <disk type='block' device='disk'>
      <serial>cloud-node$i-ceph$n</serial>
      <driver name='qemu' type='raw' cache='unsafe'/>
      <source dev='/dev/cloud/node$i-ceph$n'/>
      <target dev='$dev' bus='virtio'/>
    </disk>"
      done
    fi

    h_create_libvirt_computenode_config /tmp/cloud-node$i.xml $i "$cephvolume"

    virsh destroy cloud-node$i 2>/dev/null
    if ! virsh create /tmp/cloud-node$i.xml ; then
      echo "====>>"
      echo "Error: Could not create VM for: node$i"
      exit 74
    fi
  done
  return 0
}

function instcompute()
{
  scp qa_crowbarsetup.sh root@$adminip:
  sshrun "allocate=1 bash -x qa_crowbarsetup.sh virtual"
  ret=$?
  [ $ret != 0 ] && return $ret

  echo "Waiting for the installation of the nodes ..."
  sshrun 'waitcompute=1 bash -x qa_crowbarsetup.sh virtual'
  return $?
}


function proposal()
{
  scp qa_crowbarsetup.sh root@$adminip:
  sshrun "proposal=1 bash -x qa_crowbarsetup.sh virtual"
  return $?
}

function testsetup()
{
  scp qa_crowbarsetup.sh root@$adminip:
  sshrun "testsetup=1 bash -x qa_crowbarsetup.sh virtual"
  return $?
}

function rebootcrowbar()
{
  # reboot the crowbar instance
  #  and test if everything is up and running afterwards
  sshrun "reboot"
  wait_for 500 3 "! nc -z $adminip 22" 'crowbar to go down'
  wait_for 500 3   "nc -z $adminip 22" 'crowbar to be back online'
  echo "waiting another 180 seconds for services"
  sleep 180
  sshrun "mount -a -t nfs" # FIXME workaround repos not mounted on reboot
  return $?
}

function rebootcompute()
{
  # reboot compute nodes
  #  and test if everthing is up and running afterwards
  sshrun "rebootcompute=1 bash -x qa_crowbarsetup.sh virtual"
  return $?
}


function securitytests()
{
  scp qa_crowbarsetup.sh root@$adminip:
  # install and run security test suite owasp
  sshrun "securitytests=1 bash -x qa_crowbarsetup.sh virtual"
  return $?
}

function tempest()
{
  mkdir -p .artifacts
  scp run_tempest.sh qa_crowbarsetup.sh root@$adminip:
  sshrun "tempestconfigure=1 bash -x qa_crowbarsetup.sh virtual"
  echo "Tempest configuration returned with exit code: $?"
  echo "Now archiving artifact files from configuration step"
  scp root@$adminip:tempestlog/* .artifacts/
  sshrun "tempestrun=1 bash -x qa_crowbarsetup.sh virtual"
  ret=$?
  scp root@$adminip:tempestlog/* .artifacts/
  return $ret
}


function usage()
{
  echo "Usage:"
  echo "$0 <command> [<command>,...]"
  echo
  echo "  'command' is one of:"
  echo "   all instonly plain cleanup prepare setupadmin instcrowbar setupcompute"
  echo "   instcompute proposal addupdaterepo runupdate testsetup rebootcrowbar "
  echo "   rebootcompute help"
  echo
  echo "   all      -> expands to: cleanup prepare setupadmin instcrowbar rebootcrowbar"
  echo "                           setupcompute instcompute proposal testsetup"
  echo "                           rebootcompute"
  echo "   all_noreboot -> exp to: cleanup prepare setupadmin instcrowbar setupcompute"
  echo "                           instcompute proposal testsetup"
  echo "   plain    -> expands to: cleanup prepare setupadmin instcrowbar setupcompute"
  echo "                           instcompute proposal"
  echo "   instonly -> expands to: cleanup prepare setupadmin instcrowbar setupcompute"
  echo "                           instcompute"
  echo
  echo "   cleanup:       kill all running VMs, zero out boot sectors of all LVM volumes"
  echo "   prepare:       create LVM volumes, setup libvirt networks"
  echo "   setupadmin:    create the admin node and install the cloud product"
  echo "   instcrowbar:   install crowbar and chef on the admin node"
  echo "   setupcompute:  create the compute nodes and let crowbar discover them"
  echo "   instcompute:   allocate and install compute nodes"
  echo "   proposal:      create and apply proposals for default setup"
  echo "   testsetup:     start a VM in the cloud"
  echo "   addupdaterepo: addupdate repos defined in UPDATEREPOS= (URLs separated by '+')"
  echo "   runupdate:     run zypper up on the crowbar node"
  echo "                  (compute nodes are automaticallyupdated via chef run)"
  echo "   rebootcrowbar: reboot the crowbar instance and wait for it being up"
  echo "   rebootcompute: reboot the compute nodes and wait for them being up"
  echo "   securitytests: install and run security test suite"
  echo "   tempest:       setup and run tempest test suite"
  echo "   help:          this usage"
  echo
  echo " Environment variables (need to be exported):"
  echo
  echo " Mandatory"
  echo "   CVOL=/dev/vdx (default /dev/vdb)"
  echo "       :  LVM will be created on this device (at least 80GB)"
  echo "   cloudsource=develcloud | develcloud2.0 | develcloud3 | susecloud | Beta?  (default '')"
  echo "       : defines the installation source"
  echo
  echo " Optional"
  echo "   TESTHEAD='' | 1  (default='')"
  echo "       : use latest published packages from Devel:Cloud"
  echo "   cephvolumenumber  (default=0)"
  echo "       : the number of 5GB ceph volumes that will be created per node"
  echo "         note: proposal step does NOT contain a ceph proposal, do it manually"
  echo "   nodenumber=2    (default 2)"
  echo "       : set the number of nodes to be created (excl. admin node)"
  echo "   vcpus=1         (default 1)"
  echo "       : set the number of CPU cores per node (admin and compute)"
  echo
  exit 1
}


function addupdaterepo()
{
  scp qa_crowbarsetup.sh root@$adminip:
  sshrun "addupdaterepo=1 UPDATEREPOS=$UPDATEREPOS  bash -x qa_crowbarsetup.sh virtual"
  return $?
}

function runupdate()
{
  scp qa_crowbarsetup.sh root@$adminip:
  sshrun "runupdate=1 bash -x qa_crowbarsetup.sh virtual"
  return $?
}

function sanity_checks()
{
  if test `id -u` != 0 ; then
    echo "Error: This script needs to be run as root"
    echo "  Please be aware that this script will create a LVM"
    echo "  and kill all current VMs on this host."
    exit 1
  fi

  SCRIPT=$(basename $0)
  PSFILE=`mktemp /tmp/mkcloud.XXXXXX`
  ps ax > $PSFILE
  if [ `grep -v -e grep -e SCREEN $PSFILE | grep $SCRIPT | wc -l` -gt 1 ]  ; then
    echo "Error: mkcloud was started twice."
    echo "This is not supported ... exiting."
    echo
    echo 'Maybe you just have a "$EDITOR mkcloud" process running. Please close it.'
    rm $PSFILE
    exit 33
  fi
  rm $PSFILE

  # always fetch the latest version
  if [ -z "$NOQACROWBARDOWNLOAD" ] ; then
    #wget -O qa_crowbarsetup.sh "https://raw.github.com/SUSE-Cloud/automation/master/scripts/qa_crowbarsetup.sh"
    # Switch to curl as wget has an issue with github: https://github.com/netz98/n98-magerun/issues/75
    curl -s -o qa_crowbarsetup.sh "https://raw.github.com/SUSE-Cloud/automation/master/scripts/qa_crowbarsetup.sh"
  fi

  # fetch the latest version of tempest script
  if [ -z "$NOTEMPESTSCRIPTDOWNLOAD" ] ; then
    curl -s -o run_tempest.sh "https://raw.github.com/SUSE-Cloud/automation/master/scripts/jenkins/run_tempest.sh"
  fi

  if [ ! -e qa_crowbarsetup.sh ] ; then
    echo "Error: qa_crowbarsetup.sh not found in same directory"
    echo "Please put the latest version of this script in the same directory."
    exit 87
  fi

  if [ -z "$cloudsource" ] ; then
    echo "Please set the env variable:"
    echo "export cloudsource=Beta?|develcloud|develcloud2.0|develcloud3|susecloud"
    exit 1
  fi

  case $cloudsource in
    develcloud2.0|develcloud3|susecloud2.0|Beta*|RC*|GMC*|GM2.0)
      adminimage=SP3
    ;;
    *)
      adminimage=SP2
    ;;
  esac

  CVOL=${CVOL:-/dev/vdb}
  if grep -q $CVOL /proc/mounts ; then
    echo "The device $CVOL seems to be used. Exiting."
    exit 92
  fi
  if [ ! -e $CVOL ] ; then
    echo "Error: $CVOL does not exist."
    echo "Please set the cloud volume group to an existing device: export CVOL=/dev/sdx"
    echo "Running 'partprobe' may help to let the device appear."
    exit 93
  fi

  if [ -e /etc/init.d/SuSEfirewall2_init ] && rcSuSEfirewall2 status ; then
    echo "Error: SuSEfirewall is running - it will interfere with the iptables rules done by libvirt"
    echo "Please stop the SuSEfirewall completely and run mkcloud again"
    echo "Run:  rcSuSEfirewall2 stop && insserv -r SuSEfirewall2_setup && insserv -r SuSEfirewall2_init"
    exit 91
  fi

  if grep "devpts.*[^x]mode=.00" /proc/mounts ; then
    echo "Error: /dev/pts is not accessible for libvirt, maybe you use autobuild on your system."
    echo "Please remount it using the following command:"
    echo " # mount -o remount,mode=620,gid=5 devpts -t devpts /dev/pts"
    exit 13
  fi
}


## MAIN ##

allcmds="all all_noreboot instonly plain cleanup prepare setupadmin instcrowbar setupcompute instcompute proposal testsetup rebootcrowbar rebootcompute addupdaterepo runupdate testupdate securitytests tempest help"
wantedcmds=$@
runcmds=''

if [ -z "$wantedcmds" ] ; then
  usage
fi

# parse the commands and expand the aliases
for cmd in $wantedcmds ; do
  if [ "$cmd" = "help" -o "$cmd" = "--help" -o "$cmd" = "usage" ] ; then
    usage
  fi

  ok=0
  for onecmd in $allcmds ; do
    if [ $onecmd = $cmd ] ; then
      ok=1
      case $cmd in
        all)
          runcmds="$runcmds cleanup prepare setupadmin addupdaterepo runupdate instcrowbar rebootcrowbar setupcompute instcompute proposal testsetup rebootcompute"
        ;;
        all_noreboot)
          runcmds="$runcmds cleanup prepare setupadmin addupdaterepo runupdate instcrowbar setupcompute instcompute proposal testsetup"
        ;;
        testupdate)
          runcmds="$runcmds addupdaterepo runupdate testsetup"
        ;;
        plain)
          runcmds="$runcmds cleanup prepare setupadmin instcrowbar setupcompute instcompute proposal"
        ;;
        instonly)
          runcmds="$runcmds cleanup prepare setupadmin instcrowbar setupcompute instcompute"
        ;;
        *)
          runcmds="$runcmds $cmd"
        ;;
      esac
    fi
  done

  if [ $ok = 0 ] ; then
    echo "Error: Command $cmd unknown."
    usage
  fi
done

sanity_checks

echo "You choose to run these mkcloud steps:"
echo "  $runcmds"
echo
sleep 2

for cmd in `echo $runcmds` ; do
  echo
  echo "============> MKCLOUD STEP START: $cmd <============"
  echo
  sleep 2
  $cmd
  ret=$?
  if [ $ret != 0 ] ; then
    echo
    echo '$h1!!'
    echo "Error detected. Stopping mkcloud."
    echo "The step '$cmd' returned with exit code $ret"
    echo "Please refer to the $cmd function in this script when debugging the issue."
    error_exit $ret ""
  fi
  echo
  echo "^^^^^^^^^^^^= MKCLOUD STEP DONE: $cmd =^^^^^^^^^^^^"
  echo
done

show_environment
