#!/bin/bash
#
# mkcloud - Setup a virtual cloud on one system (physical or even virtual)
#
# Authors: J. Daniel Schmidt <jdsn@suse.de>
#          Bernhard M. Wiedemann <bwiedemann@suse.de>
#
# 2012, SUSE LINUX Products GmbH
#

# Quick introduction:
#
# This tool relies on the script qa_crowbarsetup.sh (in the same directory)
# Please 'export' environment variables according to your needs:
#
# Either 'cloudvg' or 'cloudpv' must be set.
# Another required parameter is 'cloudsource'.
# All other parameters have defaults unless defined otherwise.
#
# cloudvg=vg0
#       volume group name to use to create lvm volumes
#
# cloudpv=/dev/vdx  (default=/dev/vdb)
#       device where a LVM physical volume can be created
#       should be at least 80 GB
#       the volume group will be called "cloud"
#
# cloudsource=develcloud3|develcloud4|develcloud5|susecloud5|GM3|GM4|GM3+up|GM4+up|M?   (required, no default)
#       defines the source for the installation of the cloud product
#       develcloud3   : product from IBS Devel:Cloud:3
#       develcloud4   : product from IBS Devel:Cloud:4
#       develcloud5   : product from IBS Devel:Cloud:5
#       susecloud5    : product from IBS SUSE:SLE....
#       GM3           : SUSE Cloud Goldmaster 3 without updates
#       GM4           : SUSE Cloud Goldmaster 4 without updates
#       GM3+up        : SUSE Cloud Goldmaster 3 with released maintenance updates
#       GM4+up        : SUSE Cloud Goldmaster 4 with released maintenance updates
#       M?            : uses official Milestone? ISO image (? is a number)
#
# TESTHEAD=''|1   (default='')
#                will use latest published packages from Devel:Cloud
#                even if there is no new ISO created yet
#
# cephvolumenumber (default 0)
#                sets the number of 5GB ceph volumes that will be created per node
#                for ceph testing
#
# nodenumber     (default 2)
#                sets the overall number of nodes to be created
#
# nodenumbercompute (default $nodenumber resp. 1 in case of hacloud=1)
#                set the number of compute nodes in a HA setup, the remaining nodes
#                of $nodenumber - $nodenumercompute is used as HA cluster nodes
#
# vcpus          (default 1)
#                sets the number of CPU cores assigned to each compute node
# adminvcpus     (default $vcpus)
#                sets the number of CPU cores assigned to admin node
# public_vlan    VLAN id for public network (default 300)
#
# UNDOCUMENTED OPTIONS:
#
# virtualcloud
# cloudfqdn
# forwardmode
# net_fixed
# net_public
# net_admin
# adminnetmask
# adminip
# admingw
# nodenumbercomputedefault
# nodenumberdatacluster
# nodenumberdataclusterdefault
# nodenumbernetworkcluster   (why no default for this?)
# nodenumberservicescluster  (why no default for this?)
# drbdnode_mac_vol
# cpuflags
# admin_node_memory
# controller_node_memory
# compute_node_memory
# hyperv_node_memory
# adminnode_hdd_size
# controller_hdd_size
# computenode_hdd_size
# cephvolume_hdd_size
# controller_ceph_hdd_size
# drbd_hdd_size
# cloudbr
# debug
# localreposdir_src
# localreposdir_target

if [ -n "$CVOL" ] ; then
    cloudpv=$CVOL
    unset CVOL
    echo "------------------------------------------------------------------------"
    echo "The CVOL variable is deprecated."
    echo "Please either set 'cloudvg' or 'cloudpv'"
    echo " cloudvg will use an existing lvm and not destroy other volumes"
    echo " cloudpv will use an lvm pv device exclusively and destroy all volumes"
    echo
    echo "Continuing in 20 seconds with fallback (cloudpv=\$CVOL):"
    echo " cloudpv=$CVOL"
    echo "Otherwise press Ctrl-C now!"
    echo "------------------------------------------------------------------------"
    sleep 20
fi

# FIXME: separate user-tweakable parameters from script local variables.
# Currently there is no clearly defined interface point.  One of the
# causes of this is violation of the common shell coding standard which
# uses uppercase for environment variables and constants, and lowercase
# for local variables.
: ${virtualcloud:=virtual}
: ${cloudfqdn:=$virtualcloud.cloud.suse.de}
: ${forwardmode:=nat}
: ${net_fixed:=192.168.123}
: ${net_public:=192.168.122}
: ${net_admin:=192.168.124}
: ${public_vlan:=300}
: ${adminnetmask:=255.255.248.0}
adminip=$net_admin.10
admingw=$net_admin.1
nodenumbercomputedefault=2
[ -n "$hacloud" ] && nodenumbercomputedefault=1
: ${nodenumber:=$nodenumbercomputedefault}
: ${nodenumbercompute:=$nodenumbercomputedefault}
[ -n "$hacloud" ] && nodenumberdataclusterdefault=2 # currently only drbd supported with 2 nodes
: ${nodenumberdatacluster:=$nodenumberdataclusterdefault}
: ${nodenumbernetworkcluster:=''}
: ${nodenumberservicescluster:=''}
# '+'-separated list of MAC#serial_of_drbd_volume of the drbd cluster nodes
: ${drbdnode_mac_vol:=''}
: ${cephvolumenumber:=0}
allnodeids=`seq 1 $nodenumber`
: ${vcpus:=1}
: ${adminvcpus:=$vcpus}
cpuflags=''
working_dir_orig=`pwd`
: ${artifacts_dir:=$working_dir_orig/.artifacts}
start_time=`date`
: ${cloud:=cloud}
: ${cloudvg:=$cloud}
needcvol=1
: ${admin_node_memory:=2097152}
: ${controller_node_memory:=4194304}
: ${compute_node_memory:=2097152}
: ${hyperv_node_memory:=3000000}
[[ "$libvirt_type" = "hyperv" && $compute_node_memory < $hyperv_node_memory ]] && compute_node_memory=$hyperv_node_memory
# hdd size defaults (unless defined otherwise)
: ${adminnode_hdd_size:=15}
: ${controller_hdd_size:=20}
: ${computenode_hdd_size:=15}
: ${cephvolume_hdd_size:=11}
: ${controller_ceph_hdd_size:=25}
: ${drbd_hdd_size:=50}
# pvlist is filled below
pvlist=
next_pv_device=
pv_cur_device_no=0
: ${cloudbr:=${cloud}br}
mnt=/tmp/cloudmnt/$$
sshopts="-oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null"
scp="scp $sshopts"
: ${debug:=0}
#if localreposdir_src string is available, the local repositories are used for setup
: ${localreposdir_src:=""}
#localreposdir_target is the 9p target dir and also the mount target dir in the VM
: ${localreposdir_target:="/repositories"}
[ -z "$localreposdir_src" ] && localreposdir_target=""

emulator=/usr/bin/qemu-system-$(uname -m)
if [ -x /usr/bin/qemu-kvm ] && file /usr/bin/qemu-kvm | grep -q ELF; then
    # on SLE11, qemu-kvm is preferred, since qemu-system-x86_64 is
    # some rotten old stuff without KVM support
    emulator=/usr/bin/qemu-kvm
fi

pidfile=mkcloud.pid

trap 'error_exit $? "error caught by trap"' TERM
exec </dev/null

function complain() # {{{
{
    local ex=$1; shift
    printf "Error: %s\n" "$@" >&2
    [[ $ex != - ]] && exit $ex
} # }}}

function show_environment()
{
    end_time=`date`
    echo "Environment Details"
    echo "-------------------------------"
    echo "    hostname: `hostname -f`"
    echo "     started: $start_time"
    echo "       ended: $end_time"
    echo "-------------------------------"
    echo " cloudsource: $cloudsource"
    echo "    TESTHEAD: $TESTHEAD"
    echo "  nodenumber: $nodenumber"
    echo "     cloudpv: $cloudpv"
    echo " UPDATEREPOS: $UPDATEREPOS"
    echo "    cephvolumenumber: $cephvolumenumber"
    echo " upgrade_cloudsource: $upgrade_cloudsource"
    echo "-------------------------------"
    env | grep -i "_with_ssl"
    echo "-------------------------------"
}

function pre_exit_cleanup()
{
    rm $pidfile
}

function error_exit()
{
    exitcode=$1
    message=$2
    ssh $sshopts root@$adminip '
        set -x
        for node in $(crowbar machines list | grep ^d) ; do
            echo "Collecting supportconfig from $node"
            timeout 300 ssh $node supportconfig | wc
            timeout 300 scp $node:/var/log/\*tbz /var/log/
        done
        timeout 300 supportconfig | wc
    '
    mkdir -p $artifacts_dir
    $scp root@$adminip:/var/log/*tbz $artifacts_dir/
    pre_exit_cleanup
    echo $message
    show_environment
    exit $exitcode
} >&2

function wait_for()
{
    timecount=${1:-300}
    timesleep=${2:-1}
    condition=${3:-'/bin/true'}
    waitfor=${4:-'unknown process'}

    echo "Waiting for: $waitfor"
    n=$timecount
    while test $n -gt 0 && ! eval $condition
    do
        echo -n .
        sleep $timesleep
        n=$(($n - 1))
    done
    echo

    if [ $n = 0 ] ; then
        complain 11 "Waiting for '$waitfor' timed out." \
            "This check was used: $condition"
    fi
}

function sshrun()
{
    mkcconf=mkcloud.config
    cat > $mkcconf <<EOF
        export debug=$debug ;
        export drbdnode_mac_vol=$drbdnode_mac_vol ;
        export cloudfqdn=$cloudfqdn ;
        export cloudsource=$cloudsource ;
        export upgrade_cloudsource=$upgrade_cloudsource ;
        export hacloud=$hacloud ;
        export libvirt_type=$libvirt_type ;
        export net_admin=$net_admin ;
        export net_public=$net_public ;
        export net_fixed=$net_fixed ;
        export networkingplugin=$networkingplugin ;
        export networkingmode=$networkingmode ;
        export nodenumber=$nodenumber ;
        export nodenumbercompute=$nodenumbercompute ;
        export nodenumbernetworkcluster=$nodenumbernetworkcluster ;
        export nodenumberservicescluster=$nodenumberservicescluster ;
        export nodenumberdatacluster=$nodenumberdatacluster ;
        export nosetestparameters=${nosetestparameters} ;
        export tempestoptions='${tempestoptions}' ;
        export cephvolumenumber=$cephvolumenumber ;
        export shell=$shell ;

        export all_with_ssl=$all_with_ssl ;
        export glance_with_ssl=$glance_with_ssl ;
        export keystone_with_ssl=$keystone_with_ssl ;
        export nova_with_ssl=$nova_with_ssl ;
        export novadashboard_with_ssl=$novadashboard_with_ssl ;

        export localreposdir_target=$localreposdir_target ;

        export cinder_conf_volume_type=$cinder_conf_volume_type;
        export cinder_conf_volume_params=$cinder_conf_volume_params;

        export TESTHEAD=$TESTHEAD ;
        export NOINSTALLCLOUDPATTERN=$NOINSTALLCLOUDPATTERN ;
EOF
    env|grep -e ^pre_ -e ^want_ >> $mkcconf

    $scp qa_crowbarsetup.sh $mkcconf root@$adminip:
    ssh $sshopts root@$adminip "$@"
    return $?
}


function cleanup()
{
    # cleanup leftover from last run
    allnodenames=$(seq --format="node%.0f" 1 $(($nodenumber + 20)))
    for name in admin $allnodenames ; do
        vm=$cloud-$name
        if ! virsh domid $vm >/dev/null 2>&1; then
            continue
        fi
        virsh destroy $vm
        machinectl terminate qemu-$cloud-$n 2>/dev/null ||: # workaround bnc#916518
        virsh undefine $vm
    done

    local net=$cloud-admin
    if virsh net-uuid $net >/dev/null 2>&1; then
        virsh net-destroy $net
        virsh net-undefine $net
    fi

    if ip link show ${cloudbr}.$public_vlan >/dev/null 2>&1; then
        ip link set ${cloudbr}.$public_vlan down
    fi
    if ip link show ${cloudbr} >/dev/null 2>&1; then
        ip link set ${cloudbr} down
        ip link delete ${cloudbr} type bridge
        ip link delete ${cloudbr}-nic
    fi

    # remove all previous volumes for that cloud; this helps preventing
    # accidental booting and freeing space
    if [ -d /dev/$cloudvg ]; then
        find -L /dev/$cloudvg -name "$cloud.*" -type b | \
            xargs -r lvremove --force
    fi

    losetup | awk '$6 ~ /'"$cloud-admin"'/ {print $1}' | \
        xargs -r losetup -d

    rm -f /var/run/libvirt/qemu/$cloud-*.xml /var/lib/libvirt/network/$cloud-*.xml \
        /etc/sysconfig/network/ifcfg-$cloudbr.$public_vlan
    if [ -n "$wipe" ] ; then
        vgchange -an $cloudvg
        dd if=/dev/zero of=$cloudpv count=1000
    fi
    return 0
}

function onhost_get_next_pv_device()
{
    if [ -z "$pvlist" ] ; then
        pvlist=`pvs --sort -Free | awk '$2~/'$cloudvg'/{print $1}'`
        pv_cur_device_no=0
    fi
    next_pv_device=`perl -e '$i=shift; $i=$i % @ARGV;  print $ARGV[$i]' $pv_cur_device_no $pvlist`
    pv_cur_device_no=$(( $pv_cur_device_no + 1 ))
}

# spread block devices over a LVM's PVs so that different VMs
# are likely to use different PVs to optimize concurrent IO throughput
function onhost_create_cloud_lvm()
{
    if [ -n "$needcvol" ] ; then
        pvcreate "$cloudpv"
        vgcreate "$cloudvg" "$cloudpv"
    fi
    vgchange -ay $cloudvg # for later boots

    local hdd_size

    lvrename /dev/$cloudvg/admin /dev/$cloudvg/$cloud.admin # transition until 2015
    onhost_get_next_pv_device
    lvcreate -n $cloud.admin -L ${adminnode_hdd_size}G $cloudvg $next_pv_device
    for i in $allnodeids ; do
        lvrename /dev/$cloudvg/node$i /dev/$cloudvg/$cloud.node$i # transition until 2015
        onhost_get_next_pv_device
        hdd_size=${computenode_hdd_size}
        test "$i" = "1" && hdd_size=${controller_hdd_size}
        lvcreate -n $cloud.node$i -L ${hdd_size}G $cloudvg $next_pv_device
    done

    if [ $cephvolumenumber -gt 0 ] ; then
        for i in $allnodeids ; do
            for n in $(seq 1 $cephvolumenumber) ; do
                lvrename /dev/$cloudvg/node$i-ceph$n /dev/$cloudvg/$cloud.node$i-ceph$n # transition until 2015
                onhost_get_next_pv_device
                hdd_size=${cephvolume_hdd_size}
                test "$i" = "1" -a "$n" = "1" && hdd_size=${controller_ceph_hdd_size}
                lvcreate -n $cloud.node$i-ceph$n -L ${hdd_size}G $cloudvg $next_pv_device
            done
        done
    fi

    # create volumes for drbd
    if [ -n "$hacloud" ] ; then
        for i in `seq 1 $nodenumberdatacluster`; do
            onhost_get_next_pv_device
            lvcreate -n $cloud.node$i-drbd -L ${drbd_hdd_size=}G $cloudvg $next_pv_device
        done
    fi
}

function onhost_deploy_admin_image()
{
    pushd /tmp
    wget --progress=dot:mega -nc http://clouddata.cloud.suse.de/images/SP3-64up.qcow2
    qemu-img convert -p SP3-64up.qcow2 /dev/$cloudvg/$cloud.admin
    popd

    if fdisk -l /dev/$cloudvg/$cloud.admin | grep -q 2056192 ; then
        # make a bigger partition 2
        echo -e "d\n2\nn\np\n2\n\n\na\n2\nw" | fdisk /dev/$cloudvg/$cloud.admin
        loopdevice=`losetup --find`
        losetup -o $(expr 2056192 \* 512) $loopdevice /dev/$cloudvg/$cloud.admin
        fsck -y -f $loopdevice
        resize2fs $loopdevice
        sync
        losetup -d $loopdevice
    fi
}

function onhost_add_etchosts_entries()
{
    grep -q crowbar /etc/hosts || echo "$adminip crowbar.$cloudfqdn crowbar" >> /etc/hosts
}

function onhost_enable_ksm
{
    # enable kernel-samepage-merging to save RAM
    echo 1 > /sys/kernel/mm/ksm/run
    echo 1000 > /sys/kernel/mm/ksm/pages_to_scan
}

function prepare()
{
    zypper --non-interactive in --no-recommends libvirt kvm lvm2 curl wget bridge-utils dnsmasq netcat-openbsd ebtables

    if ! [ -e ~/.ssh/id_dsa ]; then
        echo "Creating key for controlling our VMs..."
        ssh-keygen -t dsa -f ~/.ssh/id_dsa -N ""
    fi

    onhost_enable_ksm
    onhost_create_cloud_lvm
    onhost_deploy_admin_image
    onhost_add_etchosts_entries
}

function ssh_password()
{
    SSH_ASKPASS=/root/echolinux
    cat > $SSH_ASKPASS <<EOSSHASK
#!/bin/sh
echo linux
EOSSHASK
    chmod +x $SSH_ASKPASS
    DISPLAY=dummydisplay:0 SSH_ASKPASS=$SSH_ASKPASS setsid ssh $sshopts -oNumberOfPasswordPrompts=1 "$@"
}

function onhost_local_repository_mount()
{
    #add xml snippet to be able to mount a local dir via 9p in a VM
    if [ -n "${localreposdir_src}" ]; then
        local_repository_mount="<filesystem type='mount' accessmode='squash'>
            <source dir='$localreposdir_src'/>
            <target dir='$localreposdir_target'/>
            <readonly/>
            <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
            </filesystem>"
    else
        local_repository_mount=""
    fi
}

function onhost_cpuflags_settings()
{ # used for admin and compute nodes
    cpuflags="<cpu match='minimum'>
            <model>qemu64</model>
            <feature policy='require' name='fxsr_opt'/>
            <feature policy='require' name='mmxext'/>
            <feature policy='require' name='lahf_lm'/>
            <feature policy='require' name='sse4a'/>
            <feature policy='require' name='abm'/>
            <feature policy='require' name='cr8legacy'/>
            <feature policy='require' name='misalignsse'/>
            <feature policy='require' name='popcnt'/>
            <feature policy='require' name='pdpe1gb'/>
            <feature policy='require' name='cx16'/>
            <feature policy='require' name='3dnowprefetch'/>
            <feature policy='require' name='cmp_legacy'/>
            <feature policy='require' name='monitor'/>
        </cpu>"
    grep -q "flags.* npt" /proc/cpuinfo || cpuflags=""

    if grep -q "vendor_id.*GenuineIntel" /proc/cpuinfo; then
        cpuflags="<cpu mode='custom' match='exact'>
            <model fallback='allow'>core2duo</model>
            <feature policy='require' name='vmx'/>
        </cpu>"
    fi
}


function onhost_create_libvirt_adminnode_config()
{
    onhost_cpuflags_settings
    onhost_local_repository_mount

    cat > $1 <<EOLIBVIRT
  <domain type='kvm'>
    <name>$cloud-admin</name>
    <memory>$admin_node_memory</memory>
    <currentMemory>$admin_node_memory</currentMemory>
    <vcpu>$adminvcpus</vcpu>
    <os>
      <type arch='x86_64' machine='pc-0.14'>hvm</type>
      <boot dev='hd'/>
    </os>
    <features>
      <acpi/>
      <apic/>
      <pae/>
    </features>
    $cpuflags
    <clock offset='utc'/>
    <on_poweroff>preserve</on_poweroff>
    <on_reboot>restart</on_reboot>
    <on_crash>restart</on_crash>
    <devices>
      <emulator>$emulator</emulator>
      <disk type='block' device='disk'>
        <driver name='qemu' type='raw' cache='unsafe'/>
        <source dev='/dev/$cloudvg/$cloud.admin'/>
        <target dev='vda' bus='virtio'/>
        <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
      </disk>
      <interface type='network'>
        <mac address='52:54:00:77:77:70'/>
        <source network='$cloud-admin'/>
        <model type='virtio'/>
        <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
      </interface>
      <serial type='pty'>
        <target port='0'/>
      </serial>
      <console type='pty'>
        <target type='serial' port='0'/>
      </console>
      <input type='mouse' bus='ps2'/>
      <graphics type='vnc' port='-1' autoport='yes'/>
      <video>
        <model type='cirrus' vram='9216' heads='1'/>
        <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
      </video>
      <memballoon model='virtio'>
        <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
      </memballoon>
      $local_repository_mount
    </devices>
  </domain>
EOLIBVIRT
}

function onhost_create_libvirt_admin_network_config()
{
    # dont specify range
    # this allows to use the same network for cloud-nodes that get DHCP from crowbar
    # doc: http://libvirt.org/formatnetwork.html
    cat > $1 <<EOLIBVIRTNET
  <network>
    <name>$cloud-admin</name>
    <bridge name='$cloudbr' stp='off' delay='0' />
    <mac address='52:54:00:AB:B1:77'/>
    <ip address='$admingw' netmask='$adminnetmask'>
      <dhcp>
        <host mac="52:54:00:77:77:70" name="crowbar.$cloudfqdn" ip="$adminip"/>
      </dhcp>
    </ip>
    <forward mode='$forwardmode'>
    </forward>
  </network>
EOLIBVIRTNET
}


# bring up the VM for crowbar
function setupadmin()
{
    echo "Injecting public key into image..."
    keyfile=~/.ssh/id_dsa.pub
    pubkey=`cut -d" " -f2 $keyfile`
    mkdir -p $mnt
    mount -o loop,offset=$(expr 2056192 \* 512) /dev/$cloudvg/$cloud.admin $mnt
    mkdir -p $mnt/root/.ssh
    grep -q "$pubkey" $mnt/root/.ssh/authorized_keys 2>/dev/null || cat $keyfile >> $mnt/root/.ssh/authorized_keys
    umount $mnt
    sync

    onhost_create_libvirt_adminnode_config /tmp/$cloud-admin.xml

    onhost_create_libvirt_admin_network_config /tmp/$cloud-admin.net.xml

    modprobe kvm-amd
    if [ ! -e /etc/modprobe.d/80-kvm-intel.conf ] ; then
        echo "options kvm-intel nested=1" > /etc/modprobe.d/80-kvm-intel.conf
        rmmod kvm-intel
    fi
    modprobe kvm-intel
    # needed for HA/STONITH via libvirtd:
    if ! grep -q ^listen_tcp /etc/libvirt/libvirtd.conf ; then
        service libvirtd stop
        echo 'auth_tcp = "none"
listen_tcp = 1
listen_addr = "0.0.0.0"' >> /etc/libvirt/libvirtd.conf
    fi
    chkconfig libvirtd on
    service libvirtd start
    wait_for 300 1 '[ -S /var/run/libvirt/libvirt-sock ]' 'libvirt startup'

    if ! virsh net-dumpxml $cloud-admin > /dev/null 2>&1; then
        virsh net-define /tmp/$cloud-admin.net.xml
    fi
    virsh net-start $cloud-admin
    if ! virsh define /tmp/$cloud-admin.xml ; then
        echo "=====================================================>>"
        complain 76 "Could not define VM for: $cloud-admin"
    fi
    if ! virsh start $cloud-admin ; then
        echo "=====================================================>>"
        complain 76 "Could not start VM for: $cloud-admin"
    fi

    wait_for 300 1 "ping -q -c 1 -w 1 $adminip >/dev/null" 'crowbar admin VM'

    if ( ! grep -q "iptables -t nat -F PREROUTING" /etc/init.d/boot.local ) && [ -z "$NOSETUPPORTFORWARDING" ] ; then
        nodehostips=$(seq -s ' ' 81 $((80 + $nodenumber)))
        cat >> /etc/init.d/boot.local <<EOS
iptables -t nat -F PREROUTING
for i in 22 80 443 3000 4000 4040 ; do
    iptables -I FORWARD -p tcp --dport \$i -j ACCEPT
    for host in 10 $nodehostips ; do
        offset=80
        [ "\$host" = 10 ] && offset=10
        iptables -t nat -I PREROUTING -p tcp --dport \$((\$i + \$host - \$offset + 1100)) -j DNAT --to-destination $net_admin.\$host:\$i
    done
done
iptables -t nat -I PREROUTING -p tcp --dport 6080 -j DNAT --to-destination $net_public.2
echo 0 > /proc/sys/net/ipv4/conf/all/rp_filter
EOS
    fi
    /etc/init.d/boot.local

    wait_for 150 1 "nc -z $adminip 22" 'starting ssh daemon'

    echo "waiting some more for sshd+named to start"
    sleep 25
    ssh_password $adminip "mkdir ~/.ssh ; grep -q $pubkey ~/.ssh/authorized_keys 2>/dev/null || echo ssh-dss $pubkey injected-key-from-host >> ~/.ssh/authorized_keys"
    echo "you can now proceed with installing crowbar"
}

function prepareinstcrowbar()
{
    echo "connecting to crowbar admin server at $adminip"
    # qa_crowbarsetup uses static network which needs static resolv.conf
    # so we use the host's external IPv4 resolvers,
    # which must allow queries from routed IPs
    grep '^nameserver\s*[0-9]*\.' /etc/resolv.conf | sshrun dd of=/etc/resolv.conf
    sshrun "echo `hostname` > cloud ; prepareinstallcrowbar=1 bash -x qa_crowbarsetup.sh $virtualcloud"
    return $?
}

function instcrowbar()
{
    # copy local install-chef-suse.sh, if it exist it will override the original one:
    $scp -p install-chef-suse.sh root@$adminip:/tmp/
    sshrun "echo `hostname` > cloud ; installcrowbar=1 bash -x qa_crowbarsetup.sh $virtualcloud"
    local ret=$?
    $scp root@$adminip:screenlog.0 "$artifacts_dir/screenlog.0.install-suse-cloud"
    return $ret
}

function instcrowbarfromgit()
{
    # copy local install-chef-suse.sh, if it exist it will override the original one:
    $scp -p install-chef-suse.sh root@$adminip:/tmp/
    sshrun "echo `hostname` > cloud ; installcrowbarfromgit=1 bash -x qa_crowbarsetup.sh $virtualcloud"
    local ret=$?
    $scp root@$adminip:screenlog.0 "$artifacts_dir/screenlog.0.install-suse-cloud"
    return $ret
}

function mkvlan()
{
    local DEFVLAN=$1 ; shift
    local IP=$1 ; shift
    vconfig add $cloudbr $DEFVLAN
    ifconfig $cloudbr.$DEFVLAN $IP/24
    ethtool -K $cloudbr tx off
}

function hypervisor_has_virtio()
{
    local llibvirt_type=${1:-$libvirt_type}
    [[ "$llibvirt_type" = "xen" || "$llibvirt_type" = "hyperv" ]] && return 1
    return 0
}

function onhost_create_libvirt_computenode_config()
{
    onhost_cpuflags_settings
    nodeconfigfile=$1
    nodecounter=$2
    macaddress=$3
    cephvolume="$4"
    drbdvolume="$5"
    nicmodel=virtio
    hypervisor_has_virtio || nicmodel=e1000
    nodememory=$compute_node_memory
    [ "$nodecounter" = "1" ] && nodememory=$controller_node_memory

    cat > $nodeconfigfile <<EOLIBVIRT
<domain type='kvm'>
  <name>$cloud-node$nodecounter</name>
  <memory>$nodememory</memory>
  <currentMemory>$nodememory</currentMemory>
  <vcpu>$vcpus</vcpu>
  <os>
    <type arch='x86_64' machine='pc-0.14'>hvm</type>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  $cpuflags
  <clock offset='utc'/>
  <on_poweroff>preserve</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>$emulator</emulator>
    <disk type='block' device='disk'>
      <driver name='qemu' type='raw' cache='unsafe'/>
      <source dev='/dev/$cloudvg/$cloud.node$nodecounter'/>
      <target dev='vda' bus='virtio'/>
      <boot order='2'/>
    </disk>
    $cephvolume
    $drbdvolume
    <interface type='network'>
      <mac address='$macaddress'/>
      <source network='$cloud-admin'/>
      <model type='$nicmodel'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
      <boot order='1'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target type='serial' port='0'/>
    </console>
    <input type='mouse' bus='ps2'/>
    <graphics type='vnc' port='-1' autoport='yes'/>
    <video>
      <model type='cirrus' vram='9216' heads='1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
  </devices>
</domain>
EOLIBVIRT

    if ! hypervisor_has_virtio ; then
        sed -i -e "s/<target dev='vd\([^']*\)' bus='virtio'/<target dev='sd\1' bus='ide'/" $nodeconfigfile
    fi
}

function setuppublicnet()
{
    # workaround https://bugzilla.novell.com/show_bug.cgi?id=845496
    echo 0 > /proc/sys/net/bridge/bridge-nf-call-iptables
    # public = $public_vlan
    mkvlan $public_vlan $net_public.1
}

function shutdowncloud()
{
    virsh shutdown $cloud-admin
    for i in $allnodeids ; do
        virsh shutdown $cloud-node$i
    done
}

function restartcloud()
{
    virsh net-start $cloud-admin
    virsh start $cloud-admin
    setuppublicnet
    for i in $allnodeids ; do
        virsh start $cloud-node$i
    done
}

# bring up VMs that will take cloud controller/compute/storage roles
function setupcompute()
{
    setuppublicnet
    alldevices=$(echo {b..z} {a..z}{a..z})
    for i in $allnodeids ; do
        c=1
        i2=$( printf %02x $i )
        macaddress="52:54:$i2:77:77:$i2"

        cephvolume=""
        if [ $cephvolumenumber -gt 0 ] ; then
            for n in $(seq 1 $cephvolumenumber) ; do
                dev=vd$(echo $alldevices | cut -d" " -f$c)
                c=$(($c + 1))
                cephvolume="$cephvolume
    <disk type='block' device='disk'>
        <serial>$cloud-node$i-ceph$n</serial>
        <driver name='qemu' type='raw' cache='unsafe'/>
        <source dev='/dev/$cloudvg/$cloud.node$i-ceph$n'/>
        <target dev='$dev' bus='virtio'/>
    </disk>"
            done
        fi

        drbdvolume=""
        if [ -n "$hacloud" ] ; then
            if [ $i -le $nodenumberdatacluster ] ; then
                drbddev=vd$(echo $alldevices | cut -d" " -f$c)
                c=$(($c + 1))
                local drbd_serial
                drbd_serial="$cloud-node$i-drbd"
                drbdvolume="
    <disk type='block' device='disk'>
        <serial>$drbd_serial</serial>
        <driver name='qemu' type='raw' cache='unsafe'/>
        <source dev='/dev/$cloudvg/$cloud.node$i-drbd'/>
        <target dev='$drbddev' bus='virtio'/>
    </disk>"
                drbdnode_mac_vol="${drbdnode_mac_vol}+${macaddress}#${drbd_serial}"
                drbdnode_mac_vol="${drbdnode_mac_vol#+}"
            fi
        fi

        onhost_create_libvirt_computenode_config /tmp/$cloud-node$i.xml $i $macaddress "$cephvolume" "$drbdvolume"

        virsh destroy $cloud-node$i 2>/dev/null
        virsh undefine $cloud-node$i 2>/dev/null
        if ! virsh define /tmp/$cloud-node$i.xml ; then
            echo "====>>"
            complain 74 "Could not define VM for: node$i"
        fi
        if ! virsh start $cloud-node$i ; then
            echo "====>>"
            complain 74 "Could not start VM for: node$i"
        fi
    done
    echo "========================================================"
    echo " Note: If you interrupt mkcloud now and want to proceed"
    echo "       later, make sure to run the following command:"
    echo
    echo " export drbdnode_mac_vol=\"${drbdnode_mac_vol}\""
    echo
    echo "========================================================"
    sleep 10
    return 0
}

# allocate cloud nodes with an operating system
# and wait for nodes to reach the ready state
function instcompute()
{
    sshrun "allocate=1 bash -x qa_crowbarsetup.sh $virtualcloud"
    ret=$?
    [ $ret != 0 ] && return $ret

    echo "Waiting for the installation of the nodes ..."
    sshrun "waitcompute=1 bash -x qa_crowbarsetup.sh $virtualcloud"
    return $?
}


function proposal()
{
    sshrun "proposal=1 bash -x qa_crowbarsetup.sh $virtualcloud"
    return $?
}

function testsetup()
{
    sshrun "testsetup=1 bash -x qa_crowbarsetup.sh $virtualcloud"
    local ret=$?
    $scp root@$adminip:tempest.log "$artifacts_dir/tempest.log"
    return $ret
}

function rebootcrowbar()
{
    # reboot the crowbar instance
    #  and test if everything is up and running afterwards
    sshrun "reboot"
    wait_for 500 3 "! nc -z $adminip 22" 'crowbar to go down'
    wait_for 500 3   "nc -z $adminip 22" 'crowbar to be back online'
    echo "waiting another 180 seconds for services"
    sleep 180
    sshrun "mount -a -t nfs" # FIXME workaround repos not mounted on reboot
    return $?
}

function rebootcompute()
{
    # reboot compute nodes
    #  and test if everthing is up and running afterwards
    sshrun "rebootcompute=1 bash -x qa_crowbarsetup.sh $virtualcloud"
    return $?
}

function rebootneutron()
{
    sshrun "rebootneutron=1 bash qa_crowbarsetup.sh $virtualcloud"
    return $?
}

function securitytests()
{
    # install and run security test suite owasp
    sshrun "securitytests=1 bash -x qa_crowbarsetup.sh $virtualcloud"
    return $?
}

function qa_test()
{
    local ghsc=github.com/SUSE-Cloud
    mkdir -p ~/$ghsc/
    pushd ~/$ghsc/
    if [ -e "qa-openstack-cli" ] ; then
        cd qa-openstack-cli/
        git pull
    else
        git clone https://$ghsc/qa-openstack-cli.git
    fi
    popd
    rsync -av ~/$ghsc/qa-openstack-cli root@$adminip:
    sshrun "qa_test=1 bash -x qa_crowbarsetup.sh $virtualcloud"
    ret=$?

    mkdir -p .artifacts
    $scp -r root@$adminip:qa_test.logs/ .artifacts/
    return $ret
}

function crowbarbackup()
{
    sshrun "AGREEUNSUPPORTED=1 CB_BACKUP_IGNOREWARNING=1 bash -x /usr/sbin/crowbar-backup backup /tmp/backup-crowbar.tar.gz"
    ret=$?
    $scp root@$adminip:/tmp/backup-crowbar.tar.gz .
    [ -d "$artifacts_dir" ] && mv backup-crowbar.tar.gz "$artifacts_dir/"
    return $ret
}

function crowbarrestore()
{
    btarball=backup-crowbar.tar.gz
    [ -e "$artifacts_dir/$btarball" ] && btarball="$artifacts_dir/$btarball"
    if [ ! -e "$btarball" ] ; then
        echo "No crowbar backup tarball found."
        return 56
    fi
    $scp "$btarball" root@$adminip:/tmp/
    sshrun "AGREEUNSUPPORTED=1 CB_BACKUP_IGNOREWARNING=1 bash -x /usr/sbin/crowbar-backup purge"
    # Need to install crowbar-backup again as purge deletes it
    sshrun "zypper --non-interactive in --no-recommends crowbar"
    sshrun "AGREEUNSUPPORTED=1 CB_BACKUP_IGNOREWARNING=1 bash -x /usr/sbin/crowbar-backup restore /tmp/backup-crowbar.tar.gz"
    ret=$?
    return $ret
}

function prepare_cloudupgrade()
{
    sshrun "prepare_cloudupgrade=1 bash -x qa_crowbarsetup.sh $virtualcloud"
    return $?
}

function cloudupgrade_1st()
{
    sshrun "cloudupgrade_1st=1 bash -x qa_crowbarsetup.sh $virtualcloud"
    return $?
}

function cloudupgrade_2nd()
{
    sshrun "cloudupgrade_2nd=1 bash -x qa_crowbarsetup.sh $virtualcloud"
    return $?
}

function cloudupgrade_clients()
{
    sshrun "cloudupgrade_clients=1 bash -x qa_crowbarsetup.sh $virtualcloud"
    return $?
}

function cloudupgrade_reboot_and_redeploy_clients()
{
    sshrun "cloudupgrade_reboot_and_redeploy_clients=1 bash -x qa_crowbarsetup.sh $virtualcloud"
    return $?
}

function usage()
{
    echo "Usage:"
    echo "$0 <command> [<command>,...]"
    echo
    echo "  'command' is one of:"
    echo "   all instonly plain cleanup prepare setupadmin prepareinstcrowbar instcrowbar setupcompute"
    echo "   instcompute proposal addupdaterepo runupdate testsetup rebootcrowbar "
    echo "   rebootcompute help"
    echo
    echo "   all      -> expands to: cleanup prepare setupadmin addupdaterepo runupdate prepareinstcrowbar instcrowbar rebootcrowbar setupcompute instcompute proposal testsetup rebootcompute"
    echo "   all_noreboot -> exp to: cleanup prepare setupadmin addupdaterepo runupdate prepareinstcrowbar instcrowbar setupcompute instcompute proposal testsetup"
    echo "   plain    -> expands to: cleanup prepare setupadmin prepareinstcrowbar instcrowbar setupcompute instcompute proposal"
    echo "   plain_with_upgrade  -> expands to: cleanup prepare setupadmin prepareinstcrowbar instcrowbar setupcompute instcompute proposal prepare_cloudupgrade cloudupgrade_1st cloudupgrade_2nd cloudupgrade_clients cloudupgrade_reboot_and_redeploy_clients"
    echo "   instonly -> expands to: cleanup prepare setupadmin prepareinstcrowbar instcrowbar setupcompute instcompute"
    echo
    echo "   cleanup:       kill all running VMs, zero out boot sectors of all LVM volumes"
    echo "   prepare:       create LVM volumes, setup libvirt networks"
    echo "   setupadmin:    create the admin node and install the cloud product"
    echo "   prepareinstcrowbar: add repos and install crowbar packages"
    echo "   instcrowbar:   install crowbar and chef on the admin node"
    echo "   setupcompute:  create the compute nodes and let crowbar discover them"
    echo "   instcompute:   allocate and install compute nodes"
    echo "   proposal:      create and apply proposals for default setup"
    echo "   testsetup:     start a VM in the cloud"
    echo "   addupdaterepo: addupdate repos defined in UPDATEREPOS= (URLs separated by '+')"
    echo "   runupdate:     run zypper up on the crowbar node"
    echo "                  (compute nodes are automaticallyupdated via chef run)"
    echo "   rebootcrowbar: reboot the crowbar instance and wait for it being up"
    echo "   rebootcompute: reboot the compute nodes and wait for them being up"
    echo "   restartcloud:  start a pre-existing cloud again after host reboot"
    echo "   securitytests: install and run security test suite"
    echo "   qa_test:       run the qa test suite"
    echo "   help:          this usage"
    echo
    echo " Environment variables (need to be exported):"
    echo
    echo " Mandatory"
    echo "   cloudpv=/dev/vdx (default /dev/vdb)"
    echo "       :  LVM will be created on this device (at least 80GB)"
    echo "   cloudsource=develcloud3 | develcloud4 | develcloud5 | susecloud5 | GM3 | GM4 | GM3+up | GM4+up | M?  (default '')"
    echo "       : defines the installation source"
    echo
    echo " Optional"
    echo "   hacloud='' | 1  (default='')"
    echo "       : setup a high availability cloud, requires at least 7 (better 9) nodes"
    echo "   upgrade_cloudsource='' (default='')"
    echo "       : set new cloudsource for upgrade process"
    echo "   TESTHEAD='' | 1  (default='')"
    echo "       : use Media from Devel:Cloud:Staging and add test update repositories (except for GM* targets)"
    echo "   cephvolumenumber  (default=0)"
    echo "       : the number of 11GB ceph volumes that will be created per node"
    echo "         note: proposal step does NOT contain a ceph proposal, do it manually"
    echo "   nodenumber=2    (default 2)"
    echo "       : set the number of nodes to be created (excl. admin node)"
    echo '   nodenumbercompute=1  (default $nodenumber resp. 1 in case of hacloud=1)'
    echo "       :  set the number of compute nodes in a HA setup, the remaining nodes"
    echo '          of $nodenumber - $nodenumercompute are used as HA cluster nodes'
    echo "   vcpus=1         (default 1)"
    echo "       : set the number of CPU cores per compute node"
    echo "   adminvcpus=1    (default $vcpus)"
    echo "       : set the number of CPU cores for admin node"
    echo "   admin_node_memory (default 2097152)"
    echo "       : Set the memory in KB assigned to admin node"
    echo "   controller_node_memory (default 4194304)"
    echo "       : Set the memory in KB assigned to compute nodes"
    echo "   compute_node_memory (default 2097152)"
    echo "       : Set the memory in KB assigned to compute nodes"
    echo "   networkingplugin"
    echo "       : Set the networking plugin to be used by neutron (e.g. openvswitch),"
    echo "         if it isn't defined the barclamp-neutron's default is used."
    echo "   networkingmode"
    echo "       : Set the networking mode to be used by neutron (e.g. gre)"
    echo "         if it isn't defined the barclamp-neutron's default is used."
    echo
    exit 1
}


function addupdaterepo()
{
    sshrun "addupdaterepo=1 UPDATEREPOS=$UPDATEREPOS  bash -x qa_crowbarsetup.sh $virtualcloud"
    return $?
}

function runupdate()
{
    sshrun "runupdate=1 bash -x qa_crowbarsetup.sh $virtualcloud"
    return $?
}

function is_concurrent_run()
{
    [ -e $pidfile ] && kill -0 `cat $pidfile` 2>/dev/null && return 0
    echo $$ > $pidfile
    return 1
}

function sanity_checks()
{
    if test `id -u` != 0 ; then
        complain 1 "This script needs to be run as root" \
            "Please be aware that this script will create a LVM" \
            "and kill all current VMs on this host."
    fi

    if is_concurrent_run; then
        complain 33 "mkcloud was started twice from same working directory: `pwd`" \
            "Please always use a separate working directory for each (parallel) mkcloud run."
    fi

    # always fetch the latest version
    if [ -z "$NOQACROWBARDOWNLOAD" ] ; then
        #wget -O qa_crowbarsetup.sh "https://raw.githubusercontent.com/SUSE-Cloud/automation/master/scripts/qa_crowbarsetup.sh"
        # Switch to curl as wget has an issue with github: https://github.com/netz98/n98-magerun/issues/75
        curl -s -o qa_crowbarsetup.sh "https://raw.githubusercontent.com/SUSE-Cloud/automation/master/scripts/qa_crowbarsetup.sh"
    fi

    if [ ! -e qa_crowbarsetup.sh ] ; then
        complain 87 "qa_crowbarsetup.sh not found in same directory" \
            "Please put the latest version of this script in the same directory."
    fi

    if [ -z "$cloudsource" ] ; then
        echo "Please set the env variable:"
        echo "export cloudsource=M?|develcloud3|develcloud4|develcloud5|susecloud5|GM3|GM4|GM3+up|GM4+up"
        exit 1
    fi

    if [ -n "$hacloud" ] ; then
        local nodenumbercomputeoffset=0
        if [ -z "$nodenumbernetworkcluster" ] || [ -z "$nodenumberservicescluster" ] || [ -z "$nodenumberdatacluster" ] ; then
            # currently only drbd with 2 nodes supported
            nodenumberdatacluster=2
            local maxnodes=$(( nodenumber - nodenumbercompute - nodenumberdatacluster ))
            nodenumbercomputeoffset=$(( maxnodes % 6 ))
            local mn=$(( maxnodes - nodenumbercomputeoffset ))
            nodenumbernetworkcluster=$(( mn / 2 ))
            nodenumberservicescluster=$(( mn / 2 ))
        fi

        I="    "
        echo "== HA node number overview =="
        echo  "configured nodes (min. nodes) : use case"
        echo                "$I$nodenumber (9) : nodenumber"
        echo         "$I$nodenumbercompute (1) : nodenumber compute"
        echo   "$I$nodenumbercomputeoffset (0) : unused nodes added as compute nodes"
        echo "Resulting HA node numbers:"
        echo     "$I$nodenumberdatacluster (2) : nodenumber data cluster"
        echo  "$I$nodenumbernetworkcluster (3) : nodenumber network cluster"
        echo "$I$nodenumberservicescluster (3) : nodenumber services cluster"

        # sanity check
        if  [[ $(( $nodenumbercompute + $nodenumbercomputeoffset )) < 1  ]] || \
            [[ $(( $nodenumberservicescluster )) < 3 ]] || \
            [[ $(( $nodenumbernetworkcluster  )) < 3 ]] || \
            [[ $nodenumberdatacluster < $nodenumberdataclusterdefault ]] ; then
            echo
            echo "Error: too few nodes for a HA cloud setup"
            echo "Please redefine the number of nodes:"
            echo " export nodenumber=<x>         and/or"
            echo " export nodenumbercompute=<y>"
            echo
            echo "Note:"
            echo " For a quorum a cluster needs to have an odd number of nodes, at least 3."
            echo " For drbd cluster only 2 nodes are supported currently."
            echo " So your nodenumber shoud be:"
            echo "     compute_node_number + 2 (drbd cluster) + 2 * odd_number (cluster)"
            complain 88 "Please raise the number of nodes via e.g. export nodenumber=10"
        fi

        nodenumbercompute=$(( $nodenumbercompute + $nodenumbercomputeoffset ))
        echo
        echo "Waiting 20 seconds review time. Press Ctrl-C to cancel this run."
        sleep 20
    fi

    vgdisplay "$cloudvg" >/dev/null 2>&1 && needcvol=
    if [ -n "$needcvol" ] ; then
        : ${cloudpv:=/dev/vdb}
        if grep -q $cloudpv /proc/mounts ; then
            complain 92 "The device $cloudpv seems to be used. Exiting."
        fi
        if [ ! -e $cloudpv ] ; then
            complain 93 "$cloudpv does not exist." \
                "Please set the cloud volume group to an existing device: export cloudpv=/dev/sdx" \
                "Running 'partprobe' may help to let the device appear."
        fi
    fi

    if [ -e /etc/init.d/SuSEfirewall2_init ] && rcSuSEfirewall2 status ; then
        complain 91 "SuSEfirewall is running - it will interfere with the iptables rules done by libvirt" \
            "Please stop the SuSEfirewall completely and run mkcloud again" \
            "Run:  rcSuSEfirewall2 stop && insserv -r SuSEfirewall2_setup && insserv -r SuSEfirewall2_init"
    fi

    if grep "devpts.*[^x]mode=.00" /proc/mounts ; then
        complain 13 "/dev/pts is not accessible for libvirt, maybe you use autobuild on your system." \
            "Please remount it using the following command:" \
            " # mount -o remount,mode=620,gid=5 devpts -t devpts /dev/pts"
    fi
}


## MAIN ##
step_aliases="alias_new_admin alias_compute alias_upgrade alias_testupdate"

allcmds="$step_aliases all all_noreboot instonly plain plain_with_upgrade cleanup prepare setupadmin prepareinstcrowbar instcrowbar instcrowbarfromgit setupcompute instcompute proposal testsetup rebootcrowbar rebootcompute addupdaterepo runupdate testupdate securitytests crowbarbackup crowbarrestore shutdowncloud restartcloud qa_test help rebootneutron prepare_cloudupgrade cloudupgrade_1st cloudupgrade_2nd cloudupgrade_clients cloudupgrade_reboot_and_redeploy_clients"
wantedcmds=$@

if [ -z "$wantedcmds" ] ; then
    usage
fi

function expand_steps()
{
    # parse the commands and expand the aliases
    local runcmds=''
    local localwantedcmds=$@
    local cmd
    for cmd in $localwantedcmds ; do

        local found=0
        local onecmd
        for onecmd in $allcmds ; do
            if [ $onecmd = $cmd ] ; then
                found=1
                case "$cmd" in
                    alias_new_admin)
                        runcmds="$runcmds cleanup prepare setupadmin addupdaterepo runupdate prepareinstcrowbar instcrowbar"
                    ;;
                    alias_compute)
                        runcmds="$runcmds setupcompute instcompute proposal"
                    ;;
                    all)
                        runcmds="$runcmds `expand_steps alias_new_admin` rebootcrowbar `expand_steps alias_compute` testsetup rebootcompute"
                    ;;
                    all_noreboot)
                        runcmds="$runcmds`expand_steps alias_new_admin` `expand_steps alias_compute` testsetup"
                    ;;
                    alias_testupdate|testupdate)
                        runcmds="$runcmds addupdaterepo runupdate testsetup"
                    ;;
                    plain)
                        runcmds="$runcmds `expand_steps instonly` proposal"
                    ;;
                    instonly)
                        runcmds="$runcmds cleanup prepare setupadmin prepareinstcrowbar instcrowbar setupcompute instcompute"
                    ;;
                    alias_upgrade)
                        runcmds="$runcmds prepare_cloudupgrade cloudupgrade_1st cloudupgrade_2nd cloudupgrade_clients cloudupgrade_reboot_and_redeploy_clients"
                    ;;
                    plain_with_upgrade)
                        runcmds="$runcmds `expand_steps plain` addupdaterepo runupdate `expand_steps alias_upgrade`"
                    ;;
                    *)
                        runcmds="$runcmds $cmd"
                    ;;
                esac
            fi
        done
        [ $found == 0 ] && complain - "Step $cmd not found." && return

    done
    echo "$runcmds"
}

steplist=`expand_steps $wantedcmds`
[ -z "$steplist" ] && usage

sanity_checks

echo "You choose to run these mkcloud steps:"
echo "  $steplist"
echo
sleep 2

for cmd in `echo $steplist` ; do
    echo
    echo "============> MKCLOUD STEP START: $cmd <============"
    echo
    sleep 2
    $cmd
    ret=$?
    if [ $ret != 0 ] ; then
        echo
        echo '$h1!!'
        echo "Error detected. Stopping mkcloud."
        echo "The step '$cmd' returned with exit code $ret"
        echo "Please refer to the $cmd function in this script when debugging the issue."
        error_exit $ret ""
    fi >&2
    echo
    echo "^^^^^^^^^^^^= MKCLOUD STEP DONE: $cmd =^^^^^^^^^^^^"
    echo
done

pre_exit_cleanup
show_environment
