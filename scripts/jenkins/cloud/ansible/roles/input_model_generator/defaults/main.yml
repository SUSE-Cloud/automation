#
# (c) Copyright 2018 SUSE LLC
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
---

qe_ardana_bm_info_git_url: "https://gitlab.suse.de/cloud-qe/ardana-bm-info.git"
qe_ardana_bm_info_dir: "{{ workspace_path }}/ardana-bm-info"

template_file_name: "{{ scenario[item ~ '_template'] }}.yml"

clm_model: standalone
availability_zones: 3

rhel_distro_id: "rhel{{ rhel_os_version }}-x86_64"

# Backend for designate: powerdns or bind
designate_backend: bind

# Regular expression matching disabled service components or component groups
# (i.e. values in service_component_groups)
disabled_services: ""

# Comma separated values list of Crowbar barclamps to be deployed. If empty,
# all applicable barclamps will be deployed.
enabled_services: ""

# Disable extra volume groups in the generated disk models when LVM is
# disabled in the base image, since these have no effect anyway as long as
# `skip_disk_config` is also used to skip LVM disk configuration
# in deployed clouds
enable_extra_volume_groups: "{{ want_lvm }}"

# Disable cinder LVM device group in Crowbar
cinder_disk_enabled: "{{ cinder_lvm_enabled and cloud_product == 'ardana' }}"

# Disable swiftpac device group in Crowbar
swiftpac_disk_enabled: "{{ cloud_product == 'ardana' }}"

swiftobj_disk_enabled: True

enable_external_network_bridge: "{{ versioned_features.external_network_bridge.enabled }}"

enable_designate_worker_producer: "{{ versioned_features.designate_worker_producer.enabled }}"

cp_prefix: cp1
max_host_prefix_len: "{{ 33-(cp_prefix|length) }}"
host_prefix: "{{ ('ardana-' ~ cloud_env)[:max_host_prefix_len|int-1] if cloud_env is defined else 'ardana' }}"

# First octets of the generated subnet prefixes for Ardana networks
subnet_prefix:
  ipv4: 192.168
  ipv6: fd00:0:0

# First octets of the generated subnet prefixes for Ardana neutron networks
neutron_subnet_prefix:
  ipv4: 172
  ipv6: fd00:1:0

# Start value for the third octet for the generated subnet prefixes and also
# start of generated VLAN IDs for Ardana networks
gen_subnet_start: 100
# Start value for the fourth octet for the generated server IP addresses
gen_ip_start: 100
# Gap to be left between the generated IP for the admin node and the rest of the nodes
gen_ip_gap: 1

service_component_groups:
  COMMON:
    - lifecycle-manager-target
    - stunnel
    - freezer-agent
    - monasca-agent
    - logging-rotate
    - logging-producer
  CLM:
    - lifecycle-manager
    - tempest
    # Required for testing in (run-test.sh)
    - openstack-client
    - barbican-client
    - ceilometer-client
    - cinder-client
    - designate-client
    - glance-client
    - heat-client
    - ironic-client
    - keystone-client
    - magnum-client
    - monasca-client
    - neutron-client
    - nova-client
    - swift-client
    - manila-client
    - octavia-client
  CORE:
    - ntp-server
    - ip-cluster
    - "{{ designate_backend }}"
    - memcached
    - barbican-api
    - barbican-worker
    - cinder-api
    - cinder-scheduler
    - cinder-volume
    - cinder-backup
    - designate-api
    - designate-central
    - "{{ enable_designate_worker_producer | ternary('designate-worker', 'designate-pool-manager') }}"
    - "{{ enable_designate_worker_producer | ternary('designate-producer', 'designate-zone-manager') }}"
    - designate-mdns
    - glance-api
    #- glance-api:
    #    ha_mode: false
    #    glance_stores: 'file'
    #    glance_default_store: 'file'
    - glance-registry
    - horizon
    - heat-api
    - heat-api-cfn
    - heat-api-cloudwatch
    - heat-engine
    - keystone-api
    - keystone-client
    - magnum-api
    - magnum-conductor
    - nova-api
    - nova-placement-api
    - nova-scheduler
    - nova-conductor
    - nova-console-auth
    - nova-novncproxy
    - neutron-server
    - neutron-ml2-plugin
    - octavia-api
    - octavia-health-manager
    - ops-console-web
    - manila-api
    - manila-share
  DBMQ:
    - mysql
    - rabbitmq
  NEUTRON:
    - neutron-vpn-agent
    - neutron-dhcp-agent
    - neutron-metadata-agent
    - neutron-openvswitch-agent
  LMM:
    - ceilometer-api
    - ceilometer-polling
    - ceilometer-agent-notification
    - ceilometer-common
    #- ceilometer-client
    - cassandra
    - kafka
    - spark

    # Freezer-api need to be installed on same node as logging-server
    # as they share elasticsearch
    - freezer-api
    - logging-server

    - storm
    - monasca-api
    - monasca-persister
    - monasca-notifier
    - monasca-threshold
    #- monasca-client
    - monasca-transform
    - zookeeper
  SWPAC:
    - swift-proxy
    - swift-account
    - swift-container
    - swift-ring-builder
    #- swift-client
  SWOBJ:
    - swift-object
  COMPUTE:
    - nova-compute-kvm
    - nova-compute
    - neutron-l3-agent
    - neutron-metadata-agent
    - neutron-openvswitch-agent
    - neutron-lbaasv2-agent
  RHEL_COMPUTE:
    - nova-compute-kvm
    - nova-compute
    - neutron-l3-agent
    - neutron-metadata-agent
    - neutron-openvswitch-agent
    - neutron-lbaasv2-agent

disk_component_groups:
  OS:
    logical-volumes:
      # The policy is not to consume 100% of the space of each volume group.
      # At least 5% should be left free for snapshots.

      - name: root
        size: 65
        mount: /

      # Reserved space for kernel crash dumps
      # Should evaluate to a value that is slightly larger than
      # the memory size of your server
      - name: crash
        size: 15
        mount: /var/crash
        fstype: ext4
        mkfs-opts: -O large_file

      # Local Log files.
      - name: log
        size: 15%
        mount: /var/log
        fstype: ext4
        mkfs-opts: -O large_file

    consumer:
       name: os

  CINDER-IMAGE:
    logical-volumes:
      - name: cinder_image
        size: 95%
        mount: /var/lib/cinder
        fstype: ext4

  GLANCE-CACHE:
    logical-volumes:
      - name: glance-cache
        size: 95%
        mount: /var/lib/glance/cache
        fstype: ext4
        mkfs-opts: -O large_file
        consumer:
          name: glance-api
          usage: glance-cache
  DBMQ:
    logical-volumes:

      # Mysql Database.  All persistent state from OpenStack services
      # is saved here.  Although the individual objects are small the
      # accumulated data can grow over time
      - name: mysql
        size: 45%
        mount: /var/lib/mysql
        fstype: ext4
        mkfs-opts: -O large_file
        consumer:
          name: mysql

      # Rabbitmq works mostly in memory, but needs to be able to persist
      # messages to disc under high load. This area should evaluate to a value
      # that is slightly larger than the memory size of your server
      - name: rabbitmq
        size: 45%
        mount: /var/lib/rabbitmq
        fstype: ext4
        mkfs-opts: -O large_file
        consumer:
          name: rabbitmq
          rabbitmq_env: home

  LMM:
    logical-volumes:

      # Database storage for event monitoring (Monasca).  Events are generally
      # small data objects.
      - name: cassandra_db
        size: 25%
        mount: /var/cassandra/data
        fstype: ext4
        mkfs-opts: -O large_file
        consumer:
          name: cassandra

      - name: cassandra_log
        size: 5%
        mount: /var/cassandra/commitlog
        fstype: ext4
        mkfs-opts: -O large_file
        consumer:
          name: cassandra

      # Messaging system for monitoring.
      - name: kafka
        size: 20%
        mount: /var/kafka
        fstype: ext4
        mkfs-opts: -O large_file
        consumer:
          name: kafka

      # Data storage for centralized logging. This holds log entries from all
      # servers in the cloud and hence can require a lot of disk space.
      - name: elasticsearch
        size: 40%
        mount: /var/lib/elasticsearch
        fstype: ext4

      # Zookeeper is used to provide cluster co-ordination in the monitoring
      # system.  Although not a high user of disc space we have seen issues
      # with zookeeper snapshots filling up filesystems so we keep it in its
      # own space for stability
      - name: zookeeper
        size: 5%
        mount: /var/lib/zookeeper
        fstype: ext4

  SWPAC:
      consumer:
        name: swift
        attrs:
          rings:
            - account
            - container
  SWOBJ:
      consumer:
        name: swift
        attrs:
          rings:
            - object-0


firewall_rule_groups:
  PING:
    # open ICMP echo request (ping)
    - type: allow
      remote-ip-prefix:  0.0.0.0/0
      # icmp type
      port-range-min: 8
      # icmp code
      port-range-max: 0
      protocol: icmp

  NETCAT:
    - type: allow
      remote-ip-prefix:  0.0.0.0/0
      port-range-min: 11382
      port-range-max: 11382
      protocol: tcp

barclamps:
  CORE:
    aodh:
      - aodh-server
    barbican:
      - barbican-controller
    ceilometer:
      - ceilometer-server
      - ceilometer-central
    cinder:
      - cinder-controller
      - cinder-volume
    designate:
      - designate-server
      - designate-worker
    glance:
      - glance-server
    heat:
      - heat-server
    horizon:
      - horizon-server
    ironic:
      - ironic-server
    keystone:
      - keystone-server
    magnum:
      - magnum-server
    manila:
      - manila-server
      - manila-share
    monasca:
      - monasca-agent
      - monasca-log-agent
    neutron:
      - neutron-server
    nova:
      - nova-controller
      - ec2-api
    octavia:
      - octavia-api
    sahara:
      - sahara-server
    tempest:
      - tempest
    trove:
      - trove-server

  DBMQ:
    database:
      - database-server
      - mysql-server
    rabbitmq:
      - rabbitmq-server
  NEUTRON:
    monasca:
      - monasca-agent
      - monasca-log-agent
    neutron:
      - neutron-network
  LMM:
    monasca:
      - monasca-server
      - monasca-agent
  SWPAC:
    ceilometer:
      - ceilometer-swift-proxy-middleware
    monasca:
      - monasca-agent
      - monasca-log-agent
    swift:
      - swift-proxy
      - swift-ring-compute
      - swift-dispersion
  SWOBJ:
    ceilometer:
      - ceilometer-agent
    monasca:
      - monasca-agent
      - monasca-log-agent
    swift:
      - swift-storage
  COMPUTE:
    ceilometer:
      - ceilometer-agent
    monasca:
      - monasca-agent
      - monasca-log-agent
    nova:
      - nova-compute-kvm

# barclamp roles that cannot be clustered, even when associated with a cluster
# service group
non_clustered_roles:
  ceilometer-agent:
  ceilometer-swift-proxy-middleware:
  cinder-volume:
  designate-worker:
    max: 1
  designate-server:
    max: 1
  manila-share:
  monasca-server:
    max: 1
  monasca-master:
    max: 1
  monasca-agent:
  monasca-log-agent:
  nova-compute-kvm:
  swift-storage:
  swift-dispersion:
    max: 1
  swift-ring-compute:
    max: 1
  tempest:
    max: 1

# list of available Crowbar barclamps, in the order they must appear in a
# Crowbar batch scenario file.
# note: pacemaker and nfs_client are not in this list because they are handled
# separately
ordered_barclamps:
  - database
  - rabbitmq
  - keystone
  - swift
#  - ceph
  - monasca
  - glance
  - cinder
  - neutron
#  - ironic
  - nova
  - horizon
  - ceilometer
  - heat
  - manila
  - trove
  - designate
  - barbican
  - octavia
  - magnum
  - sahara
  - aodh
  - tempest

neutron_networkingplugin: openvswitch
neutron_networkingmode: vxlan
neutron_use_dvr: false
neutron_use_l3_ha: false
